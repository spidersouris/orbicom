{"step": 0, "token": "Ġ___", "logprob": -2.25862193107605, "top_k": [{"token": "<|end_of_text|>", "logprob": -0.3289344608783722}, {"token": "___", "logprob": -2.25862193107605}, {"token": "ĊĊĊĊ", "logprob": -2.78987193107605}, {"token": "<", "logprob": -2.98518443107605}, {"token": "ĊĊĊĊĊ", "logprob": -3.58674693107605}, {"token": "__", "logprob": -4.274247169494629}, {"token": "_____", "logprob": -4.367997169494629}, {"token": "______", "logprob": -4.649247169494629}], "context": ""}
{"step": 1, "token": "ĊĊ", "logprob": -0.5827663540840149, "top_k": [{"token": "ĊĊ", "logprob": -0.5827663540840149}, {"token": "Ċ", "logprob": -1.0983912944793701}, {"token": "ĊĊĊ", "logprob": -2.22339129447937}], "context": " ___"}
{"step": 2, "token": "ĠĠ", "logprob": -0.332568496465683, "top_k": [{"token": "", "logprob": -0.332568496465683}, {"token": "", "logprob": -1.4263185262680054}, {"token": "", "logprob": -3.152880907058716}], "context": " ___\n\n"}
{"step": 3, "token": "Ġ#", "logprob": -4.650464057922363, "top_k": [{"token": "Text", "logprob": -0.18171393871307373}, {"token": "<", "logprob": -2.5645265579223633}, {"token": "Vo", "logprob": -3.2754640579223633}, {"token": "Il", "logprob": -4.447339057922363}, {"token": "Le", "logprob": -4.462964057922363}, {"token": "__", "logprob": -4.642651557922363}, {"token": "#", "logprob": -4.650464057922363}, {"token": "___", "logprob": -4.712964057922363}], "context": " ___\n\n  "}
{"step": 4, "token": "ĠJ", "logprob": -4.313398361206055, "top_k": [{"token": "Text", "logprob": -1.5712106227874756}, {"token": "Ce", "logprob": -3.0712106227874756}, {"token": "", "logprob": -3.1571481227874756}, {"token": "Vo", "logprob": -3.2508981227874756}, {"token": "L", "logprob": -3.2821481227874756}, {"token": "Les", "logprob": -3.3133981227874756}, {"token": "Ex", "logprob": -3.3446481227874756}, {"token": "Test", "logprob": -3.3915231227874756}, {"token": "G", "logprob": -3.4305856227874756}, {"token": "Le", "logprob": -3.4383981227874756}, {"token": "text", "logprob": -3.5790231227874756}, {"token": "A", "logprob": -3.5868356227874756}, {"token": "Code", "logprob": -3.6649606227874756}, {"token": "1", "logprob": -3.6805856227874756}, {"token": "La", "logprob": -3.7430856227874756}, {"token": "R", "logprob": -3.8368356227874756}, {"token": "-*-", "logprob": -3.9462106227874756}, {"token": "C", "logprob": -4.016523361206055}, {"token": "Ãī", "logprob": -4.079023361206055}, {"token": "Pour", "logprob": -4.125898361206055}, {"token": "En", "logprob": -4.141523361206055}, {"token": "Je", "logprob": -4.188398361206055}, {"token": "Ce", "logprob": -4.227460861206055}, {"token": "Text", "logprob": -4.235273361206055}, {"token": "Comment", "logprob": -4.250898361206055}, {"token": "V", "logprob": -4.250898361206055}, {"token": "Question", "logprob": -4.266523361206055}, {"token": "DÃ©", "logprob": -4.282148361206055}, {"token": "J", "logprob": -4.313398361206055}, {"token": "Pro", "logprob": -4.352460861206055}], "context": " ___\n\n   #"}
{"step": 5, "token": "'util", "logprob": -3.5997984409332275, "top_k": [{"token": "'ai", "logprob": -0.5685484409332275}, {"token": "'", "logprob": -1.9591734409332275}, {"token": "'a", "logprob": -3.1857359409332275}, {"token": "our", "logprob": -3.3419859409332275}, {"token": "'util", "logprob": -3.5997984409332275}, {"token": "'av", "logprob": -3.8966734409332275}, {"token": "'en", "logprob": -3.9747984409332275}, {"token": "ardin", "logprob": -3.9904234409332275}, {"token": "us", "logprob": -4.052923202514648}, {"token": "'y", "logprob": -4.318548202514648}, {"token": "EU", "logprob": -4.326360702514648}, {"token": "oueur", "logprob": -4.341985702514648}, {"token": "'Ã©", "logprob": -4.357610702514648}, {"token": "'app", "logprob": -4.638860702514648}, {"token": "ours", "logprob": -4.646673202514648}, {"token": "'acc", "logprob": -4.771673202514648}, {"token": "Ã©s", "logprob": -4.873235702514648}, {"token": "ournÃ©e", "logprob": -4.927923202514648}, {"token": "âĢĻai", "logprob": -4.966985702514648}, {"token": "oc", "logprob": -5.091985702514648}, {"token": "acc", "logprob": -5.185735702514648}], "context": " ___\n\n   # J"}
{"step": 6, "token": "ise", "logprob": 0.0, "top_k": [{"token": "ise", "logprob": 0.0}], "context": " ___\n\n   # J'util"}
{"step": 7, "token": "Ġla", "logprob": -1.9456456899642944, "top_k": [{"token": "le", "logprob": -1.4456456899642944}, {"token": "la", "logprob": -1.9456456899642944}, {"token": "les", "logprob": -2.078458309173584}, {"token": "un", "logprob": -2.555020809173584}, {"token": "des", "logprob": -2.680020809173584}, {"token": "ce", "logprob": -2.758145809173584}, {"token": "l", "logprob": -2.851895809173584}, {"token": "une", "logprob": -3.109708309173584}, {"token": "cette", "logprob": -3.351895809173584}, {"token": "pour", "logprob": -3.930020809173584}, {"token": "Python", "logprob": -3.992520809173584}, {"token": "ici", "logprob": -4.195645809173584}, {"token": "[", "logprob": -4.351895809173584}, {"token": "ces", "logprob": -4.359708309173584}, {"token": "`", "logprob": -4.414395809173584}, {"token": "\"", "logprob": -4.859708309173584}, {"token": "J", "logprob": -4.898770809173584}, {"token": "de", "logprob": -4.961270809173584}, {"token": "plusieurs", "logprob": -4.984708309173584}, {"token": "du", "logprob": -5.062833309173584}, {"token": "python", "logprob": -5.109708309173584}, {"token": "votre", "logprob": -5.164395809173584}, {"token": "G", "logprob": -5.172208309173584}, {"token": "Open", "logprob": -5.180020809173584}], "context": " ___\n\n   # J'utilise"}
{"step": 8, "token": "Ġfonction", "logprob": -1.5240122079849243, "top_k": [{"token": "fonction", "logprob": -1.5240122079849243}, {"token": "mÃ©thode", "logprob": -2.3599495887756348}, {"token": "bibli", "logprob": -2.4224495887756348}, {"token": "classe", "logprob": -2.7115120887756348}, {"token": "commande", "logprob": -2.8755745887756348}, {"token": "variable", "logprob": -2.9536995887756348}, {"token": "syntax", "logprob": -2.9771370887756348}, {"token": "lib", "logprob": -3.1333870887756348}, {"token": "version", "logprob": -3.3599495887756348}, {"token": "langue", "logprob": -3.6880745887756348}, {"token": "bal", "logprob": -3.9380745887756348}, {"token": "structure", "logprob": -3.9458870887756348}, {"token": "licence", "logprob": -3.9458870887756348}, {"token": "techn", "logprob": -3.9693245887756348}, {"token": "gÃ©nÃ©", "logprob": -3.9771370887756348}, {"token": "rÃ¨", "logprob": -4.117762088775635}, {"token": "mÃªme", "logprob": -4.133387088775635}, {"token": "technique", "logprob": -4.219324588775635}, {"token": "phrase", "logprob": -4.344324588775635}, {"token": "mÃ©", "logprob": -4.367762088775635}, {"token": "requ", "logprob": -4.469324588775635}, {"token": "propri", "logprob": -4.594324588775635}, {"token": "cha", "logprob": -4.633387088775635}, {"token": "vari", "logprob": -4.649012088775635}, {"token": "base", "logprob": -4.664637088775635}, {"token": "macro", "logprob": -4.664637088775635}, {"token": "norm", "logprob": -4.680262088775635}, {"token": "gr", "logprob": -4.805262088775635}, {"token": "suite", "logprob": -4.805262088775635}, {"token": "form", "logprob": -4.828699588775635}], "context": " ___\n\n   # J'utilise la"}
{"step": 9, "token": "Ġgenerate", "logprob": -3.5660934448242188, "top_k": [{"token": "`", "logprob": -1.3395310640335083}, {"token": "de", "logprob": -1.9957810640335083}, {"token": "\"", "logprob": -2.4879684448242188}, {"token": "suiv", "logprob": -2.7301559448242188}, {"token": "random", "logprob": -2.7457809448242188}, {"token": "<", "logprob": -3.5270309448242188}, {"token": "generate", "logprob": -3.5660934448242188}, {"token": "d", "logprob": -3.6910934448242188}, {"token": "pour", "logprob": -3.8004684448242188}, {"token": "get", "logprob": -3.9801559448242188}, {"token": "markdown", "logprob": -4.027030944824219}, {"token": "__", "logprob": -4.027030944824219}, {"token": "[", "logprob": -4.066093444824219}, {"token": "text", "logprob": -4.183280944824219}, {"token": "'", "logprob": -4.237968444824219}, {"token": "qui", "logprob": -4.316093444824219}, {"token": "ĊĊ", "logprob": -4.316093444824219}, {"token": "```", "logprob": -4.331718444824219}, {"token": "_", "logprob": -4.362968444824219}, {"token": "Ċ", "logprob": -4.394218444824219}, {"token": "rand", "logprob": -4.409843444824219}, {"token": "du", "logprob": -4.441093444824219}, {"token": "word", "logprob": -4.441093444824219}, {"token": ":", "logprob": -4.456718444824219}, {"token": "nal", "logprob": -4.628593444824219}, {"token": "\\", "logprob": -4.644218444824219}, {"token": "Âł", "logprob": -4.644218444824219}, {"token": "", "logprob": -4.714530944824219}, {"token": "Python", "logprob": -4.714530944824219}, {"token": "python", "logprob": -4.761405944824219}], "context": " ___\n\n   # J'utilise la fonction"}
{"step": 10, "token": "_text", "logprob": -0.5720898509025574, "top_k": [{"token": "_text", "logprob": -0.5720898509025574}, {"token": "Text", "logprob": -2.017402410507202}, {"token": "()", "logprob": -2.267402410507202}, {"token": "pour", "logprob": -2.447089910507202}, {"token": "\\_", "logprob": -3.618964910507202}, {"token": "avec", "logprob": -4.447089672088623}, {"token": "qui", "logprob": -4.618964672088623}, {"token": "_paragraph", "logprob": -4.634589672088623}, {"token": "_story", "logprob": -4.704902172088623}, {"token": "de", "logprob": -4.798652172088623}, {"token": "_article", "logprob": -4.853339672088623}, {"token": "_random", "logprob": -4.876777172088623}, {"token": "ĊĊ", "logprob": -5.040839672088623}, {"token": "Ċ", "logprob": -5.220527172088623}, {"token": "Markdown", "logprob": -5.290839672088623}, {"token": "()Ċ", "logprob": -5.337714672088623}], "context": " ___\n\n   # J'utilise la fonction generate"}
{"step": 11, "token": "Ġavec", "logprob": -3.199218273162842, "top_k": [{"token": "()", "logprob": -0.9179682731628418}, {"token": "pour", "logprob": -2.019530773162842}, {"token": "de", "logprob": -3.089843273162842}, {"token": "_to", "logprob": -3.152343273162842}, {"token": "avec", "logprob": -3.199218273162842}, {"token": "qui", "logprob": -3.339843273162842}, {"token": "_from", "logprob": -3.386718273162842}, {"token": "ĊĊ", "logprob": -3.503905773162842}, {"token": "()Ċ", "logprob": -3.519530773162842}, {"token": "()ĊĊ", "logprob": -3.628905773162842}, {"token": "Ċ", "logprob": -3.730468273162842}, {"token": "afin", "logprob": -4.207030773162842}, {"token": "(),", "logprob": -4.261718273162842}, {"token": "dans", "logprob": -4.285155773162842}, {"token": "Ãł", "logprob": -4.347655773162842}, {"token": "().", "logprob": -4.394530773162842}, {"token": ",", "logprob": -4.394530773162842}, {"token": "en", "logprob": -4.410155773162842}, {"token": "du", "logprob": -4.417968273162842}, {"token": "_fr", "logprob": -4.441405773162842}, {"token": ".", "logprob": -4.449218273162842}, {"token": "_with", "logprob": -4.464843273162842}, {"token": "et", "logprob": -4.589843273162842}, {"token": ".ĊĊ", "logprob": -4.636718273162842}], "context": " ___\n\n   # J'utilise la fonction generate_text"}
{"step": 12, "token": "Ġles", "logprob": -1.8565094470977783, "top_k": [{"token": "le", "logprob": -1.5518219470977783}, {"token": "les", "logprob": -1.8565094470977783}, {"token": "un", "logprob": -1.8643219470977783}, {"token": "la", "logprob": -2.1065094470977783}, {"token": "l", "logprob": -2.5205719470977783}, {"token": "des", "logprob": -2.7705719470977783}, {"token": "une", "logprob": -2.8018219470977783}, {"token": "ce", "logprob": -3.1065094470977783}, {"token": "comme", "logprob": -3.8096344470977783}, {"token": "ces", "logprob": -3.8799469470977783}, {"token": "cette", "logprob": -4.106509208679199}, {"token": "pour", "logprob": -4.223696708679199}, {"token": "plusieurs", "logprob": -4.317446708679199}, {"token": "", "logprob": -4.629946708679199}, {"token": "generate", "logprob": -4.629946708679199}], "context": " ___\n\n   # J'utilise la fonction generate_text avec"}
{"step": 13, "token": "Ġarguments", "logprob": -1.6088404655456543, "top_k": [{"token": "param", "logprob": -0.4525904357433319}, {"token": "arguments", "logprob": -1.6088404655456543}, {"token": "options", "logprob": -2.8588404655456543}, {"token": "paramet", "logprob": -4.046340465545654}, {"token": "donnÃ©es", "logprob": -4.335402965545654}, {"token": "deux", "logprob": -4.468215465545654}, {"token": "valeurs", "logprob": -4.577590465545654}, {"token": "suiv", "logprob": -4.796340465545654}, {"token": "argument", "logprob": -4.796340465545654}, {"token": "mots", "logprob": -4.804152965545654}, {"token": "", "logprob": -4.804152965545654}, {"token": "par", "logprob": -4.835402965545654}, {"token": "text", "logprob": -4.983840465545654}, {"token": "variables", "logprob": -5.030715465545654}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les"}
{"step": 14, "token": "Ġsuiv", "logprob": -0.30874866247177124, "top_k": [{"token": "suiv", "logprob": -0.30874866247177124}, {"token": "text", "logprob": -3.113436222076416}, {"token": ":", "logprob": -3.629061222076416}, {"token": "\"", "logprob": -3.683748722076416}, {"token": "de", "logprob": -3.871248722076416}, {"token": "Ċ", "logprob": -3.972811222076416}, {"token": "texte", "logprob": -4.043123722076416}, {"token": "ĊĊ", "logprob": -4.160311222076416}, {"token": "ci", "logprob": -4.175936222076416}, {"token": ":Ċ", "logprob": -4.207186222076416}, {"token": "Âł", "logprob": -4.707186222076416}, {"token": "(", "logprob": -4.714998722076416}, {"token": ":Ċ", "logprob": -4.730623722076416}, {"token": "Ċ", "logprob": -4.761873722076416}, {"token": ":", "logprob": -4.808748722076416}, {"token": "qui", "logprob": -4.824373722076416}, {"token": ":ĊĊ", "logprob": -4.871248722076416}, {"token": "du", "logprob": -4.925936222076416}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments"}
{"step": 15, "token": "ants", "logprob": 0.0, "top_k": [{"token": "ants", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suiv"}
{"step": 16, "token": "Âł", "logprob": -1.4739116430282593, "top_k": [{"token": "Âł", "logprob": -1.4739116430282593}, {"token": ":Ċ", "logprob": -1.5989116430282593}, {"token": ":ĊĊ", "logprob": -1.8020366430282593}, {"token": ":", "logprob": -2.067661762237549}, {"token": ":Ċ", "logprob": -2.848911762237549}, {"token": ":ĊĊ", "logprob": -3.067661762237549}, {"token": "Ċ", "logprob": -3.083286762237549}, {"token": "ĊĊ", "logprob": -3.083286762237549}, {"token": "pour", "logprob": -3.739536762237549}, {"token": ":", "logprob": -3.942661762237549}, {"token": ".Ċ", "logprob": -3.942661762237549}, {"token": ".ĊĊ", "logprob": -3.973911762237549}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants"}
{"step": 17, "token": ":", "logprob": -1.3134843111038208, "top_k": [{"token": ":Ċ", "logprob": -0.9384843111038208}, {"token": ":ĊĊ", "logprob": -1.0791093111038208}, {"token": ":", "logprob": -1.3134843111038208}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants "}
{"step": 18, "token": "ĠĠĊ", "logprob": -3.2626378536224365, "top_k": [{"token": "Ċ", "logprob": -0.9345128536224365}, {"token": "ĊĊ", "logprob": -1.6845128536224365}, {"token": "ĊĊ", "logprob": -2.5829503536224365}, {"token": "Âł", "logprob": -2.8876378536224365}, {"token": "Ċ", "logprob": -3.2626378536224365}, {"token": "Ċ", "logprob": -3.3251378536224365}, {"token": "ĊĊ", "logprob": -3.5360753536224365}, {"token": "#", "logprob": -3.5985753536224365}, {"token": "", "logprob": -3.7079503536224365}, {"token": "\"", "logprob": -4.004825592041016}, {"token": "`", "logprob": -4.028263092041016}, {"token": "ĊĊ", "logprob": -4.207950592041016}, {"token": "text", "logprob": -4.395450592041016}, {"token": "(", "logprob": -4.543888092041016}, {"token": "{", "logprob": -4.543888092041016}, {"token": "ĊĊ", "logprob": -4.645450592041016}, {"token": "<|end_of_text|>", "logprob": -4.747013092041016}, {"token": "", "logprob": -4.786075592041016}, {"token": "'", "logprob": -4.801700592041016}, {"token": "<", "logprob": -4.817325592041016}, {"token": "ÂłĊ", "logprob": -4.864200592041016}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :"}
{"step": 19, "token": "ĠĠ", "logprob": -0.4112575352191925, "top_k": [{"token": "", "logprob": -0.4112575352191925}, {"token": "Ċ", "logprob": -2.16125750541687}, {"token": "", "logprob": -2.55188250541687}, {"token": "Ċ", "logprob": -2.78625750541687}, {"token": "", "logprob": -3.59875750541687}, {"token": "", "logprob": -3.66125750541687}, {"token": "Ċ", "logprob": -4.153445243835449}, {"token": "", "logprob": -4.294070243835449}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n"}
{"step": 20, "token": "Ġ```", "logprob": -2.8906168937683105, "top_k": [{"token": "#", "logprob": -0.8671793937683105}, {"token": "-", "logprob": -1.6953043937683105}, {"token": "*", "logprob": -2.3515543937683105}, {"token": "`", "logprob": -2.8828043937683105}, {"token": "```", "logprob": -2.8906168937683105}, {"token": "##", "logprob": -3.3515543937683105}, {"token": "", "logprob": -3.5624918937683105}, {"token": "generate", "logprob": -3.7187418937683105}, {"token": "text", "logprob": -3.8593668937683105}, {"token": "language", "logprob": -4.1562418937683105}, {"token": "<", "logprob": -4.2812418937683105}, {"token": "```Ċ", "logprob": -4.2968668937683105}, {"token": "**", "logprob": -4.4687418937683105}, {"token": "(", "logprob": -4.4843668937683105}, {"token": "+", "logprob": -4.8515543937683105}, {"token": "__", "logprob": -4.9374918937683105}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n  "}
{"step": 21, "token": "python", "logprob": -0.4198358356952667, "top_k": [{"token": "python", "logprob": -0.4198358356952667}, {"token": "Ċ", "logprob": -1.7792108058929443}, {"token": "py", "logprob": -3.6073358058929443}, {"token": "text", "logprob": -3.6463983058929443}, {"token": "{", "logprob": -3.9979608058929443}, {"token": "Ċ", "logprob": -4.013586044311523}, {"token": "j", "logprob": -4.052648544311523}, {"token": "Ċ", "logprob": -4.091711044311523}, {"token": "javascript", "logprob": -4.162023544311523}, {"token": "js", "logprob": -4.404211044311523}, {"token": "ruby", "logprob": -4.435461044311523}, {"token": "bash", "logprob": -4.537023544311523}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```"}
{"step": 22, "token": "ĠĠĊ", "logprob": -0.721308171749115, "top_k": [{"token": "Ċ", "logprob": -0.721308171749115}, {"token": "Ċ", "logprob": -0.986933171749115}, {"token": "Ċ", "logprob": -2.6431832313537598}, {"token": "Ċ", "logprob": -2.6588082313537598}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python"}
{"step": 23, "token": "ĠĠ", "logprob": -0.10790188610553741, "top_k": [{"token": "", "logprob": -0.10790188610553741}, {"token": "", "logprob": -3.1860268115997314}, {"token": "Ċ", "logprob": -3.3500893115997314}, {"token": "", "logprob": -3.6547768115997314}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n"}
{"step": 24, "token": "Ġtext", "logprob": -2.3576436042785645, "top_k": [{"token": "generate", "logprob": -1.295143485069275}, {"token": "#", "logprob": -1.943580985069275}, {"token": "text", "logprob": -2.3576436042785645}, {"token": "from", "logprob": -2.3732686042785645}, {"token": "model", "logprob": -3.2326436042785645}, {"token": "import", "logprob": -3.2560811042785645}, {"token": "def", "logprob": -3.3342061042785645}, {"token": "language", "logprob": -3.3576436042785645}, {"token": "args", "logprob": -3.7013936042785645}, {"token": "-", "logprob": -3.7248311042785645}, {"token": "lang", "logprob": -3.8654561042785645}, {"token": "function", "logprob": -3.8967061042785645}, {"token": "{", "logprob": -3.9592061042785645}, {"token": "(", "logprob": -3.9748311042785645}, {"token": "input", "logprob": -4.0607686042785645}, {"token": "{Ċ", "logprob": -4.1857686042785645}, {"token": "{'", "logprob": -4.3888936042785645}, {"token": "```", "logprob": -4.5060811042785645}, {"token": "{\"", "logprob": -4.5373311042785645}, {"token": "params", "logprob": -4.6076436042785645}, {"token": "arguments", "logprob": -4.8029561042785645}, {"token": "n", "logprob": -4.8888936042785645}, {"token": "num", "logprob": -4.8888936042785645}, {"token": "generator", "logprob": -4.9904561042785645}, {"token": "Generate", "logprob": -5.0842061042785645}, {"token": "", "logprob": -5.1310811042785645}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n  "}
{"step": 25, "token": "_model", "logprob": -2.733234405517578, "top_k": [{"token": "=", "logprob": -0.7801094055175781}, {"token": "_generator", "logprob": -1.8972969055175781}, {"token": "_type", "logprob": -2.358234405517578}, {"token": "_model", "logprob": -2.733234405517578}, {"token": "_to", "logprob": -3.561359405517578}, {"token": "_gen", "logprob": -3.733234405517578}, {"token": "_format", "logprob": -3.780109405517578}, {"token": "Âł", "logprob": -3.944171905517578}, {"token": "_length", "logprob": -3.991046905517578}, {"token": "_generation", "logprob": -4.084796905517578}, {"token": "gen", "logprob": -4.178546905517578}, {"token": "=", "logprob": -4.303546905517578}, {"token": "_input", "logprob": -4.350421905517578}, {"token": ":", "logprob": -4.670734405517578}, {"token": "_config", "logprob": -4.748859405517578}, {"token": "_file", "logprob": -4.819171905517578}, {"token": "_template", "logprob": -4.842609405517578}, {"token": "_generate", "logprob": -4.928546905517578}, {"token": "=\"", "logprob": -4.959796905517578}, {"token": "_style", "logprob": -5.084796905517578}, {"token": "_engine", "logprob": -5.100421905517578}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text"}
{"step": 26, "token": "Ġ=", "logprob": -0.3507142663002014, "top_k": [{"token": "=", "logprob": -0.3507142663002014}, {"token": "_name", "logprob": -2.1475892066955566}, {"token": "=\"", "logprob": -3.0147767066955566}, {"token": "Âł", "logprob": -3.2960267066955566}, {"token": "='", "logprob": -3.3194642066955566}, {"token": ":", "logprob": -3.5460267066955566}, {"token": "_path", "logprob": -3.5772767066955566}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model"}
{"step": 27, "token": "Ġ\"", "logprob": -0.6508421301841736, "top_k": [{"token": "\"", "logprob": -0.6508421301841736}, {"token": "'", "logprob": -1.2055296897888184}, {"token": "Text", "logprob": -3.6899046897888184}, {"token": "text", "logprob": -3.7836546897888184}, {"token": "Auto", "logprob": -3.8617796897888184}, {"token": "model", "logprob": -3.9867796897888184}, {"token": "models", "logprob": -4.221154689788818}, {"token": "bert", "logprob": -4.369592189788818}, {"token": "[\"", "logprob": -4.588342189788818}, {"token": "load", "logprob": -4.603967189788818}, {"token": "Bert", "logprob": -4.611779689788818}, {"token": "transformers", "logprob": -4.650842189788818}, {"token": "None", "logprob": -4.658654689788818}, {"token": "language", "logprob": -4.846154689788818}, {"token": "generate", "logprob": -4.932092189788818}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model ="}
{"step": 28, "token": "XL", "logprob": -4.902002811431885, "top_k": [{"token": "text", "logprob": -1.0191901922225952}, {"token": "g", "logprob": -1.3785651922225952}, {"token": "fr", "logprob": -2.6832528114318848}, {"token": "t", "logprob": -3.2770028114318848}, {"token": "en", "logprob": -3.2926278114318848}, {"token": "bert", "logprob": -3.3082528114318848}, {"token": "ro", "logprob": -3.9801278114318848}, {"token": "x", "logprob": -4.191065311431885}, {"token": "dist", "logprob": -4.308252811431885}, {"token": "m", "logprob": -4.347315311431885}, {"token": "alice", "logprob": -4.355127811431885}, {"token": "G", "logprob": -4.362940311431885}, {"token": "Text", "logprob": -4.566065311431885}, {"token": "open", "logprob": -4.573877811431885}, {"token": "T", "logprob": -4.573877811431885}, {"token": "large", "logprob": -4.589502811431885}, {"token": "small", "logprob": -4.589502811431885}, {"token": "simple", "logprob": -4.683252811431885}, {"token": "facebook", "logprob": -4.831690311431885}, {"token": "c", "logprob": -4.847315311431885}, {"token": "XL", "logprob": -4.902002811431885}, {"token": "wiki", "logprob": -5.027002811431885}, {"token": "f", "logprob": -5.042627811431885}, {"token": "un", "logprob": -5.081690311431885}, {"token": "chat", "logprob": -5.081690311431885}, {"token": "wik", "logprob": -5.089502811431885}, {"token": "b", "logprob": -5.097315311431885}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \""}
{"step": 29, "token": "NET", "logprob": -1.9604103565216064, "top_k": [{"token": "Net", "logprob": -0.38228538632392883}, {"token": "NET", "logprob": -1.9604103565216064}, {"token": "\"", "logprob": -2.2885353565216064}, {"token": "H", "logprob": -3.4291603565216064}, {"token": "-", "logprob": -3.7416603565216064}, {"token": "\"Ċ", "logprob": -3.9447853565216064}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XL"}
{"step": 30, "token": "\"", "logprob": -0.5282104015350342, "top_k": [{"token": "\"", "logprob": -0.5282104015350342}, {"token": "-base", "logprob": -2.465710401535034}, {"token": "\"Ċ", "logprob": -2.629772901535034}, {"token": "_L", "logprob": -3.004772901535034}, {"token": "-large", "logprob": -3.075085401535034}, {"token": "-", "logprob": -3.512585401535034}, {"token": "\",", "logprob": -3.715710401535034}, {"token": "-L", "logprob": -3.715710401535034}, {"token": "-small", "logprob": -3.809460401535034}, {"token": "_M", "logprob": -4.200085639953613}, {"token": "_", "logprob": -4.348523139953613}, {"token": "-M", "logprob": -4.348523139953613}, {"token": "/", "logprob": -4.793835639953613}, {"token": "-F", "logprob": -4.856335639953613}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET"}
{"step": 31, "token": "ĠĠĊ", "logprob": -0.2926834225654602, "top_k": [{"token": "Ċ", "logprob": -0.2926834225654602}, {"token": "#", "logprob": -2.0739333629608154}, {"token": "", "logprob": -2.6520583629608154}, {"token": "Ċ", "logprob": -2.8551833629608154}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\""}
{"step": 32, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n"}
{"step": 33, "token": "Ġ#", "logprob": -3.1212010383605957, "top_k": [{"token": "text", "logprob": -1.3399511575698853}, {"token": "max", "logprob": -1.5587011575698853}, {"token": "num", "logprob": -2.5352635383605957}, {"token": "tokenizer", "logprob": -2.9024510383605957}, {"token": "prompt", "logprob": -2.9258885383605957}, {"token": "#", "logprob": -3.1212010383605957}, {"token": "n", "logprob": -3.4337010383605957}, {"token": "seed", "logprob": -3.5274510383605957}, {"token": "context", "logprob": -3.6212010383605957}, {"token": "output", "logprob": -3.7930760383605957}, {"token": "vocab", "logprob": -3.8321385383605957}, {"token": "length", "logprob": -3.9571385383605957}, {"token": "model", "logprob": -3.9649510383605957}, {"token": "top", "logprob": -4.082138538360596}, {"token": "min", "logprob": -4.121201038360596}, {"token": "temperature", "logprob": -4.191513538360596}, {"token": "input", "logprob": -4.293076038360596}, {"token": "language", "logprob": -4.410263538360596}, {"token": "sentence", "logprob": -4.621201038360596}, {"token": "generate", "logprob": -4.730576038360596}, {"token": "lang", "logprob": -4.785263538360596}, {"token": "use", "logprob": -4.816513538360596}, {"token": "source", "logprob": -4.816513538360596}, {"token": "args", "logprob": -4.832138538360596}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n  "}
{"step": 34, "token": "ĠI", "logprob": -4.227091312408447, "top_k": [{"token": "text", "logprob": -1.7427163124084473}, {"token": "text", "logprob": -2.4380288124084473}, {"token": "Le", "logprob": -2.5161538124084473}, {"token": "le", "logprob": -2.8442788124084473}, {"token": "ou", "logprob": -2.8833413124084473}, {"token": "La", "logprob": -2.8989663124084473}, {"token": "Les", "logprob": -3.0708413124084473}, {"token": "L", "logprob": -3.5239663124084473}, {"token": "", "logprob": -3.5552163124084473}, {"token": "XL", "logprob": -3.5942788124084473}, {"token": "Pour", "logprob": -3.6333413124084473}, {"token": "Text", "logprob": -3.8364663124084473}, {"token": "modÃ¨le", "logprob": -3.9145913124084473}, {"token": "\"", "logprob": -3.9770913124084473}, {"token": "Cho", "logprob": -3.9849038124084473}, {"token": "pour", "logprob": -4.031778812408447}, {"token": "On", "logprob": -4.117716312408447}, {"token": "texte", "logprob": -4.148966312408447}, {"token": "lang", "logprob": -4.148966312408447}, {"token": "I", "logprob": -4.227091312408447}, {"token": "les", "logprob": -4.250528812408447}, {"token": "(", "logprob": -4.258341312408447}, {"token": "la", "logprob": -4.273966312408447}, {"token": "Arguments", "logprob": -4.352091312408447}, {"token": "model", "logprob": -4.359903812408447}, {"token": "taille", "logprob": -4.414591312408447}, {"token": "Si", "logprob": -4.445841312408447}, {"token": "The", "logprob": -4.492716312408447}, {"token": "J", "logprob": -4.586466312408447}, {"token": "Je", "logprob": -4.586466312408447}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   #"}
{"step": 35, "token": "ci", "logprob": -0.09550449252128601, "top_k": [{"token": "ci", "logprob": -0.09550449252128601}, {"token": "CI", "logprob": -2.6658170223236084}, {"token": "use", "logprob": -3.8376920223236084}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # I"}
{"step": 36, "token": ",", "logprob": -0.7599156498908997, "top_k": [{"token": ",", "logprob": -0.7599156498908997}, {"token": "je", "logprob": -1.8849155902862549}, {"token": "on", "logprob": -2.173978090286255}, {"token": "j", "logprob": -2.298978090286255}, {"token": "le", "logprob": -3.173978090286255}, {"token": "vous", "logprob": -3.259915590286255}, {"token": "la", "logprob": -3.892728090286255}, {"token": "les", "logprob": -4.088040828704834}, {"token": "il", "logprob": -4.127103328704834}, {"token": "nous", "logprob": -4.408353328704834}, {"token": "text", "logprob": -4.548978328704834}, {"token": "l", "logprob": -4.595853328704834}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici"}
{"step": 37, "token": "Ġon", "logprob": -1.6490157842636108, "top_k": [{"token": "je", "logprob": -1.5787032842636108}, {"token": "on", "logprob": -1.6490157842636108}, {"token": "j", "logprob": -2.0787034034729004}, {"token": "XL", "logprob": -2.4927659034729004}, {"token": "le", "logprob": -2.5552659034729004}, {"token": "vous", "logprob": -2.6412034034729004}, {"token": "text", "logprob": -3.1099534034729004}, {"token": "il", "logprob": -3.2349534034729004}, {"token": "la", "logprob": -3.5630784034729004}, {"token": "\"", "logprob": -3.6958909034729004}, {"token": "les", "logprob": -3.7349534034729004}, {"token": "nous", "logprob": -3.9146409034729004}, {"token": "l", "logprob": -4.0083909034729}, {"token": "c", "logprob": -4.2818284034729}, {"token": "pour", "logprob": -4.3755784034729}, {"token": "un", "logprob": -4.5318284034729}, {"token": "si", "logprob": -4.7662034034729}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici,"}
{"step": 38, "token": "Ġutilise", "logprob": -2.3215386867523193, "top_k": [{"token": "peut", "logprob": -1.4387261867523193}, {"token": "chois", "logprob": -1.5949761867523193}, {"token": "dÃ©f", "logprob": -2.2590386867523193}, {"token": "utilise", "logprob": -2.3215386867523193}, {"token": "spÃ©c", "logprob": -2.6809136867523193}, {"token": "prÃ©c", "logprob": -3.6106011867523193}, {"token": "mod", "logprob": -3.6418511867523193}, {"token": "a", "logprob": -3.6731011867523193}, {"token": "ind", "logprob": -3.8059136867523193}, {"token": "ne", "logprob": -3.8840386867523193}, {"token": "donne", "logprob": -3.9934136867523193}, {"token": "passe", "logprob": -4.032476425170898}, {"token": "fix", "logprob": -4.063726425170898}, {"token": "sÃ©lection", "logprob": -4.110601425170898}, {"token": "change", "logprob": -4.134038925170898}, {"token": "gÃ©n", "logprob": -4.219976425170898}, {"token": "dÃ©", "logprob": -4.305913925170898}, {"token": "prend", "logprob": -4.337163925170898}, {"token": "cho", "logprob": -4.454351425170898}, {"token": "aj", "logprob": -4.524663925170898}, {"token": "met", "logprob": -4.680913925170898}, {"token": "va", "logprob": -4.759038925170898}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on"}
{"step": 39, "token": "Ġle", "logprob": -1.4975930452346802, "top_k": [{"token": "le", "logprob": -1.4975930452346802}, {"token": "la", "logprob": -1.6694680452346802}, {"token": "un", "logprob": -1.8882180452346802}, {"token": "XL", "logprob": -1.9194680452346802}, {"token": "une", "logprob": -2.3569679260253906}, {"token": "l", "logprob": -2.5600929260253906}, {"token": "les", "logprob": -3.2007179260253906}, {"token": "\"", "logprob": -3.5444679260253906}, {"token": "des", "logprob": -4.208530426025391}, {"token": "X", "logprob": -4.505405426025391}, {"token": "G", "logprob": -4.802280426025391}, {"token": "en", "logprob": -4.864780426025391}, {"token": "generate", "logprob": -5.036655426025391}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise"}
{"step": 40, "token": "ĠmodÃ¨le", "logprob": -0.1931157410144806, "top_k": [{"token": "modÃ¨le", "logprob": -0.1931157410144806}, {"token": "texte", "logprob": -2.732178211212158}, {"token": "model", "logprob": -3.739990711212158}, {"token": "gÃ©nÃ©", "logprob": -3.794678211212158}, {"token": "mode", "logprob": -3.997803211212158}, {"token": "param", "logprob": -4.224365711212158}, {"token": "fichier", "logprob": -4.599365711212158}, {"token": "corpus", "logprob": -4.716553211212158}, {"token": "module", "logprob": -5.091553211212158}, {"token": "lang", "logprob": -5.091553211212158}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le"}
{"step": 41, "token": "Ġde", "logprob": -2.123993158340454, "top_k": [{"token": "XL", "logprob": -0.3583681583404541}, {"token": "de", "logprob": -2.123993158340454}, {"token": "d", "logprob": -3.248993158340454}, {"token": "\"", "logprob": -3.413055658340454}, {"token": "G", "logprob": -3.944305658340454}, {"token": "X", "logprob": -3.991180658340454}, {"token": "B", "logprob": -4.217742919921875}, {"token": "fin", "logprob": -4.670867919921875}, {"token": "du", "logprob": -4.733367919921875}, {"token": "par", "logprob": -4.741180419921875}, {"token": "en", "logprob": -4.748992919921875}, {"token": "H", "logprob": -4.827117919921875}, {"token": "g", "logprob": -4.905242919921875}, {"token": "L", "logprob": -5.061492919921875}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle"}
{"step": 42, "token": "Ġlang", "logprob": -3.147235155105591, "top_k": [{"token": "texte", "logprob": -1.3425476551055908}, {"token": "gÃ©nÃ©", "logprob": -1.4050476551055908}, {"token": "langue", "logprob": -2.092547655105591}, {"token": "base", "logprob": -2.897235155105591}, {"token": "text", "logprob": -3.076922655105591}, {"token": "lang", "logprob": -3.147235155105591}, {"token": "XL", "logprob": -3.240985155105591}, {"token": "la", "logprob": -3.811297655105591}, {"token": "prÃ©", "logprob": -3.920672655105591}, {"token": "modÃ¨le", "logprob": -4.053484916687012}, {"token": "gÃ©n", "logprob": -4.131609916687012}, {"token": "pr", "logprob": -4.170672416687012}, {"token": "Lang", "logprob": -4.561297416687012}, {"token": "grande", "logprob": -4.615984916687012}, {"token": "l", "logprob": -4.662859916687012}, {"token": "plus", "logprob": -4.733172416687012}, {"token": "rÃ©", "logprob": -4.772234916687012}, {"token": "type", "logprob": -4.834734916687012}, {"token": "G", "logprob": -4.850359916687012}, {"token": "large", "logprob": -4.936297416687012}, {"token": "X", "logprob": -4.990984916687012}, {"token": "chat", "logprob": -5.037859916687012}, {"token": "corpus", "logprob": -5.037859916687012}, {"token": "rÃ©seau", "logprob": -5.092547416687012}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de"}
{"step": 43, "token": "age", "logprob": 0.0, "top_k": [{"token": "age", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de lang"}
{"step": 44, "token": "ĠXL", "logprob": -1.0914746522903442, "top_k": [{"token": "XL", "logprob": -1.0914746522903442}, {"token": "franÃ§ais", "logprob": -1.9742871522903442}, {"token": "anglais", "logprob": -2.1930370330810547}, {"token": "en", "logprob": -2.6383495330810547}, {"token": "de", "logprob": -2.7086620330810547}, {"token": "\"", "logprob": -2.7242870330810547}, {"token": "fin", "logprob": -3.6930370330810547}, {"token": "fr", "logprob": -4.013349533081055}, {"token": "X", "logprob": -4.028974533081055}, {"token": "le", "logprob": -4.099287033081055}, {"token": "eng", "logprob": -4.286787033081055}, {"token": "d", "logprob": -4.310224533081055}, {"token": "bas", "logprob": -4.419599533081055}, {"token": "gÃ©n", "logprob": -4.450849533081055}, {"token": "du", "logprob": -4.489912033081055}, {"token": "qui", "logprob": -4.552412033081055}, {"token": "pour", "logprob": -4.583662033081055}, {"token": "(", "logprob": -4.732099533081055}, {"token": "alle", "logprob": -4.763349533081055}, {"token": "G", "logprob": -4.825849533081055}, {"token": "gÃ©nÃ©", "logprob": -5.036787033081055}, {"token": "g", "logprob": -5.091474533081055}, {"token": "des", "logprob": -5.107099533081055}, {"token": "English", "logprob": -5.138349533081055}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage"}
{"step": 45, "token": "NET", "logprob": -0.5967332720756531, "top_k": [{"token": "NET", "logprob": -0.5967332720756531}, {"token": "Net", "logprob": -0.7998582720756531}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XL"}
{"step": 46, "token": "Ġpour", "logprob": -2.1448354721069336, "top_k": [{"token": ".", "logprob": -1.5354604721069336}, {"token": ",", "logprob": -1.8167104721069336}, {"token": "Ċ", "logprob": -1.8479604721069336}, {"token": "pour", "logprob": -2.1448354721069336}, {"token": "(", "logprob": -2.3010854721069336}, {"token": "qui", "logprob": -2.7463979721069336}, {"token": "en", "logprob": -3.4495229721069336}, {"token": "de", "logprob": -3.6682729721069336}, {"token": "et", "logprob": -3.8401479721069336}, {"token": "avec", "logprob": -3.9182729721069336}, {"token": "Ċ", "logprob": -4.019835472106934}, {"token": "Âł", "logprob": -4.176085472106934}, {"token": "sur", "logprob": -4.191710472106934}, {"token": "car", "logprob": -4.215147972106934}, {"token": "Ãł", "logprob": -4.535460472106934}, {"token": "Ċ", "logprob": -4.551085472106934}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET"}
{"step": 47, "token": "ĠcrÃ©er", "logprob": -3.29935359954834, "top_k": [{"token": "gÃ©n", "logprob": -0.7446660995483398}, {"token": "le", "logprob": -2.01810359954834}, {"token": "la", "logprob": -2.14310359954834}, {"token": "crÃ©er", "logprob": -3.29935359954834}, {"token": "g", "logprob": -3.33841609954834}, {"token": "les", "logprob": -3.33841609954834}, {"token": "l", "logprob": -3.65091609954834}, {"token": "transformer", "logprob": -3.70560359954834}, {"token": "avoir", "logprob": -3.80716609954834}, {"token": "gÃ©nÃ©", "logprob": -3.93997859954834}, {"token": "produ", "logprob": -4.01029109954834}, {"token": "un", "logprob": -4.43997859954834}, {"token": "ce", "logprob": -4.53372859954834}, {"token": "obten", "logprob": -4.71341609954834}, {"token": "faire", "logprob": -4.75247859954834}, {"token": "une", "logprob": -4.79154109954834}, {"token": "dÃ©", "logprob": -4.79935359954834}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour"}
{"step": 48, "token": "Ġdes", "logprob": -2.7298054695129395, "top_k": [{"token": "le", "logprob": -0.5891804695129395}, {"token": "un", "logprob": -1.4173054695129395}, {"token": "des", "logprob": -2.7298054695129395}, {"token": "notre", "logprob": -2.8548054695129395}, {"token": "du", "logprob": -3.0110554695129395}, {"token": "une", "logprob": -3.4798054695129395}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer"}
{"step": 49, "token": "Ġphrases", "logprob": -3.466503381729126, "top_k": [{"token": "text", "logprob": -0.09931594133377075}, {"token": "phrases", "logprob": -3.466503381729126}, {"token": "rÃ©", "logprob": -3.482128381729126}, {"token": "paragraph", "logprob": -3.989940881729126}, {"token": "cont", "logprob": -4.263378620147705}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des"}
{"step": 50, "token": ".", "logprob": -1.746598482131958, "top_k": [{"token": ".", "logprob": -1.746598482131958}, {"token": "Ċ", "logprob": -1.809098482131958}, {"token": "en", "logprob": -2.012223482131958}, {"token": "de", "logprob": -2.051285982131958}, {"token": "simil", "logprob": -3.012223482131958}, {"token": "al", "logprob": -3.488785982131958}, {"token": ",", "logprob": -3.488785982131958}, {"token": "Ãł", "logprob": -3.527848482131958}, {"token": "simples", "logprob": -3.590348482131958}, {"token": "plus", "logprob": -3.629410982131958}, {"token": "gÃ©n", "logprob": -3.902848482131958}, {"token": "et", "logprob": -3.973160982131958}, {"token": "d", "logprob": -3.980973482131958}, {"token": "avec", "logprob": -4.020035743713379}, {"token": "orig", "logprob": -4.082535743713379}, {"token": "qui", "logprob": -4.168473243713379}, {"token": "Ċ", "logprob": -4.301285743713379}, {"token": "cour", "logprob": -4.348160743713379}, {"token": "(", "logprob": -4.410660743713379}, {"token": "Ċ", "logprob": -4.465348243713379}, {"token": "compl", "logprob": -4.473160743713379}, {"token": "dans", "logprob": -4.527848243713379}, {"token": "sur", "logprob": -4.629410743713379}, {"token": "litt", "logprob": -4.801285743713379}, {"token": "du", "logprob": -4.887223243713379}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases"}
{"step": 51, "token": "ĠĠĊ", "logprob": -0.06987319886684418, "top_k": [{"token": "Ċ", "logprob": -0.06987319886684418}, {"token": "Ċ", "logprob": -3.397998094558716}, {"token": "On", "logprob": -3.851123094558716}, {"token": "Il", "logprob": -4.358935832977295}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases."}
{"step": 52, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n"}
{"step": 53, "token": "Ġtext", "logprob": -1.6970207691192627, "top_k": [{"token": "#", "logprob": -1.0720207691192627}, {"token": "text", "logprob": -1.6970207691192627}, {"token": "max", "logprob": -2.2048332691192627}, {"token": "num", "logprob": -2.8220207691192627}, {"token": "tokenizer", "logprob": -3.0329582691192627}, {"token": "n", "logprob": -3.5407707691192627}, {"token": "model", "logprob": -3.6892082691192627}, {"token": "language", "logprob": -3.7829582691192627}, {"token": "generate", "logprob": -3.8142082691192627}, {"token": "prompt", "logprob": -3.8845207691192627}, {"token": "context", "logprob": -3.9860832691192627}, {"token": "vocab", "logprob": -4.056395530700684}, {"token": "lang", "logprob": -4.157958030700684}, {"token": "output", "logprob": -4.197020530700684}, {"token": "length", "logprob": -4.243895530700684}, {"token": "args", "logprob": -4.275145530700684}, {"token": "##", "logprob": -4.282958030700684}, {"token": "seed", "logprob": -4.447020530700684}, {"token": "min", "logprob": -4.720458030700684}, {"token": "sentence", "logprob": -4.837645530700684}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n  "}
{"step": 54, "token": "_generate", "logprob": -4.198410987854004, "top_k": [{"token": "_length", "logprob": -1.581223487854004}, {"token": "_model", "logprob": -1.651535987854004}, {"token": "_max", "logprob": -2.573410987854004}, {"token": "_generation", "logprob": -2.674973487854004}, {"token": "_prompt", "logprob": -2.690598487854004}, {"token": "_generator", "logprob": -2.721848487854004}, {"token": "_gen", "logprob": -2.823410987854004}, {"token": "_type", "logprob": -3.182785987854004}, {"token": "=", "logprob": -3.315598487854004}, {"token": "_token", "logprob": -3.932785987854004}, {"token": "_source", "logprob": -3.948410987854004}, {"token": "_size", "logprob": -4.042160987854004}, {"token": "_generate", "logprob": -4.198410987854004}, {"token": "_format", "logprob": -4.268723487854004}, {"token": "_style", "logprob": -4.331223487854004}, {"token": "_input", "logprob": -4.432785987854004}, {"token": "_prom", "logprob": -4.534348487854004}, {"token": "_len", "logprob": -4.565598487854004}, {"token": "_dataset", "logprob": -4.690598487854004}, {"token": "_data", "logprob": -4.721848487854004}, {"token": "_encoder", "logprob": -4.815598487854004}, {"token": "_seed", "logprob": -4.870285987854004}, {"token": "_params", "logprob": -4.901535987854004}, {"token": "_lang", "logprob": -4.971848487854004}, {"token": "_n", "logprob": -5.003098487854004}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text"}
{"step": 55, "token": "_type", "logprob": -4.093980312347412, "top_k": [{"token": "=", "logprob": -0.3986676335334778}, {"token": "_model", "logprob": -2.156480073928833}, {"token": "_length", "logprob": -2.640855073928833}, {"token": "_kwargs", "logprob": -3.359605073928833}, {"token": "_max", "logprob": -3.554917573928833}, {"token": "_args", "logprob": -3.703355073928833}, {"token": "_params", "logprob": -3.867417573928833}, {"token": "_type", "logprob": -4.093980312347412}, {"token": "_function", "logprob": -4.726792812347412}, {"token": "_func", "logprob": -4.914292812347412}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate"}
{"step": 56, "token": "Ġ=", "logprob": 0.0, "top_k": [{"token": "=", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type"}
{"step": 57, "token": "Ġ\"", "logprob": 0.0, "top_k": [{"token": "\"", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type ="}
{"step": 58, "token": "fill", "logprob": -2.754190444946289, "top_k": [{"token": "text", "logprob": -1.6838778257369995}, {"token": "summary", "logprob": -2.722940444946289}, {"token": "fill", "logprob": -2.754190444946289}, {"token": "full", "logprob": -2.988565444946289}, {"token": "sentence", "logprob": -3.027627944946289}, {"token": "long", "logprob": -3.074502944946289}, {"token": "story", "logprob": -3.074502944946289}, {"token": "simple", "logprob": -3.137002944946289}, {"token": "sum", "logprob": -3.269815444946289}, {"token": "un", "logprob": -3.285440444946289}, {"token": "random", "logprob": -3.308877944946289}, {"token": "paragraph", "logprob": -3.441690444946289}, {"token": "short", "logprob": -3.691690444946289}, {"token": "copy", "logprob": -3.707315444946289}, {"token": "1", "logprob": -3.785440444946289}, {"token": "plain", "logprob": -3.793252944946289}, {"token": "sequence", "logprob": -3.801065444946289}, {"token": "chat", "logprob": -4.004190444946289}, {"token": "par", "logprob": -4.051065444946289}, {"token": "seq", "logprob": -4.137002944946289}, {"token": "generate", "logprob": -4.254190444946289}, {"token": "normal", "logprob": -4.254190444946289}, {"token": "n", "logprob": -4.301065444946289}, {"token": "complete", "logprob": -4.387002944946289}, {"token": "g", "logprob": -4.426065444946289}, {"token": "pl", "logprob": -4.433877944946289}, {"token": "description", "logprob": -4.644815444946289}, {"token": "human", "logprob": -4.722940444946289}, {"token": "context", "logprob": -4.730752944946289}, {"token": "news", "logprob": -4.754190444946289}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \""}
{"step": 59, "token": "-in", "logprob": -1.6412243843078613, "top_k": [{"token": "\"", "logprob": -0.7427868247032166}, {"token": "-in", "logprob": -1.6412243843078613}, {"token": "_in", "logprob": -1.8912243843078613}, {"token": "er", "logprob": -2.9224743843078613}, {"token": "_text", "logprob": -3.4693493843078613}, {"token": "ing", "logprob": -3.5396618843078613}, {"token": "_mask", "logprob": -3.7818493843078613}, {"token": "-mask", "logprob": -3.8912243843078613}, {"token": "_missing", "logprob": -4.430286884307861}, {"token": "in", "logprob": -4.555286884307861}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill"}
{"step": 60, "token": "-the", "logprob": 0.0, "top_k": [{"token": "-the", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in"}
{"step": 61, "token": "-bl", "logprob": 0.0, "top_k": [{"token": "-bl", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the"}
{"step": 62, "token": "ank", "logprob": -0.266189843416214, "top_k": [{"token": "ank", "logprob": -0.266189843416214}, {"token": "anks", "logprob": -1.4536898136138916}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-bl"}
{"step": 63, "token": "\"", "logprob": 0.0, "top_k": [{"token": "\"", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank"}
{"step": 64, "token": "ĠĠĊ", "logprob": 0.0, "top_k": [{"token": "Ċ", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\""}
{"step": 65, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n"}
{"step": 66, "token": "Ġ#", "logprob": 0.0, "top_k": [{"token": "#", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n  "}
{"step": 67, "token": "ĠI", "logprob": -1.0029077529907227, "top_k": [{"token": "I", "logprob": -1.0029077529907227}, {"token": "On", "logprob": -1.5654077529907227}, {"token": "Le", "logprob": -2.4325952529907227}, {"token": "La", "logprob": -3.3154077529907227}, {"token": "L", "logprob": -3.4169702529907227}, {"token": "C", "logprob": -3.4325952529907227}, {"token": "Cette", "logprob": -3.6513452529907227}, {"token": "Je", "logprob": -3.6747827529907227}, {"token": "Les", "logprob": -3.7138452529907227}, {"token": "Il", "logprob": -3.7841577529907227}, {"token": "Ce", "logprob": -3.7841577529907227}, {"token": "F", "logprob": -3.7997827529907227}, {"token": "Pour", "logprob": -3.8388452529907227}, {"token": "Nous", "logprob": -4.034157752990723}, {"token": "J", "logprob": -4.096657752990723}, {"token": "A", "logprob": -4.416970252990723}, {"token": "Dans", "logprob": -4.416970252990723}, {"token": "Cec", "logprob": -4.471657752990723}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   #"}
{"step": 68, "token": "ci", "logprob": 0.0, "top_k": [{"token": "ci", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # I"}
{"step": 69, "token": ",", "logprob": 0.0, "top_k": [{"token": ",", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici"}
{"step": 70, "token": "Ġon", "logprob": -0.1054176390171051, "top_k": [{"token": "on", "logprob": -0.1054176390171051}, {"token": "je", "logprob": -3.1991677284240723}, {"token": "le", "logprob": -3.3554177284240723}, {"token": "nous", "logprob": -3.7147927284240723}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici,"}
{"step": 71, "token": "Ġutilise", "logprob": -0.3946489691734314, "top_k": [{"token": "utilise", "logprob": -0.3946489691734314}, {"token": "demande", "logprob": -2.621211528778076}, {"token": "gÃ©n", "logprob": -2.800899028778076}, {"token": "cr", "logprob": -3.332149028778076}, {"token": "spÃ©c", "logprob": -3.691524028778076}, {"token": "veut", "logprob": -3.816524028778076}, {"token": "aj", "logprob": -4.027461528778076}, {"token": "chois", "logprob": -4.160274028778076}, {"token": "a", "logprob": -4.379024028778076}, {"token": "rempl", "logprob": -4.472774028778076}, {"token": "rem", "logprob": -4.504024028778076}, {"token": "ind", "logprob": -4.527461528778076}, {"token": "sÃ©lection", "logprob": -4.558711528778076}, {"token": "souha", "logprob": -4.566524028778076}, {"token": "va", "logprob": -4.621211528778076}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on"}
{"step": 72, "token": "Ġle", "logprob": -0.46357274055480957, "top_k": [{"token": "le", "logprob": -0.46357274055480957}, {"token": "la", "logprob": -1.6041977405548096}, {"token": "un", "logprob": -2.3385727405548096}, {"token": "l", "logprob": -3.2291977405548096}, {"token": "une", "logprob": -3.3854477405548096}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise"}
{"step": 73, "token": "Ġtype", "logprob": -0.6043462753295898, "top_k": [{"token": "type", "logprob": -0.6043462753295898}, {"token": "modÃ¨le", "logprob": -1.4949712753295898}, {"token": "mode", "logprob": -2.40903377532959}, {"token": "gÃ©nÃ©", "logprob": -2.47153377532959}, {"token": "genre", "logprob": -2.90122127532959}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le"}
{"step": 74, "token": "Ġde", "logprob": -0.16996492445468903, "top_k": [{"token": "de", "logprob": -0.16996492445468903}, {"token": "d", "logprob": -2.4668400287628174}, {"token": "\"", "logprob": -2.6387150287628174}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type"}
{"step": 75, "token": "ĠgÃ©nÃ©", "logprob": -0.3702339828014374, "top_k": [{"token": "gÃ©nÃ©", "logprob": -0.3702339828014374}, {"token": "texte", "logprob": -1.7764840126037598}, {"token": "gÃ©n", "logprob": -3.0733590126037598}, {"token": "rempl", "logprob": -3.2842965126037598}, {"token": "phrase", "logprob": -3.6983590126037598}, {"token": "text", "logprob": -4.03429651260376}, {"token": "crÃ©ation", "logprob": -4.26867151260376}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de"}
{"step": 76, "token": "ration", "logprob": 0.0, "top_k": [{"token": "ration", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de géné"}
{"step": 77, "token": "Ġde", "logprob": -0.9858905076980591, "top_k": [{"token": "de", "logprob": -0.9858905076980591}, {"token": "\"", "logprob": -1.407765507698059}, {"token": "du", "logprob": -2.5015153884887695}, {"token": "fill", "logprob": -2.5093278884887695}, {"token": "des", "logprob": -2.6890153884887695}, {"token": "d", "logprob": -3.1265153884887695}, {"token": "Ãł", "logprob": -3.5483903884887695}, {"token": "rempl", "logprob": -3.7905778884887695}, {"token": "XL", "logprob": -3.9155778884887695}, {"token": "text", "logprob": -4.2124528884887695}, {"token": "'", "logprob": -4.5327653884887695}, {"token": "en", "logprob": -4.5796403884887695}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération"}
{"step": 78, "token": "Ġtext", "logprob": -2.6188082695007324, "top_k": [{"token": "texte", "logprob": -0.696933388710022}, {"token": "phrases", "logprob": -1.228183388710022}, {"token": "phrase", "logprob": -1.993808388710022}, {"token": "text", "logprob": -2.6188082695007324}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de"}
{"step": 79, "token": "es", "logprob": 0.0, "top_k": [{"token": "es", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de text"}
{"step": 80, "token": "Ġ(", "logprob": -3.9151949882507324, "top_k": [{"token": "\"", "logprob": -0.688632607460022}, {"token": "fill", "logprob": -1.719882607460022}, {"token": "rempl", "logprob": -2.7355074882507324}, {"token": "de", "logprob": -3.1495699882507324}, {"token": ",", "logprob": -3.5167574882507324}, {"token": "Ãł", "logprob": -3.8058199882507324}, {"token": "'", "logprob": -3.8448824882507324}, {"token": "F", "logprob": -3.8761324882507324}, {"token": "(", "logprob": -3.9151949882507324}, {"token": "Â«", "logprob": -4.008944988250732}, {"token": "pour", "logprob": -4.235507488250732}, {"token": "qui", "logprob": -4.344882488250732}, {"token": "Fill", "logprob": -4.477694988250732}, {"token": "avec", "logprob": -4.548007488250732}, {"token": "en", "logprob": -4.610507488250732}, {"token": "appel", "logprob": -4.626132488250732}, {"token": ".", "logprob": -4.641757488250732}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes"}
{"step": 81, "token": "fill", "logprob": -1.286802053451538, "top_k": [{"token": "fill", "logprob": -1.286802053451538}, {"token": "en", "logprob": -2.310239553451538}, {"token": "par", "logprob": -2.693052053451538}, {"token": "text", "logprob": -2.708677053451538}, {"token": "le", "logprob": -3.060239553451538}, {"token": "ou", "logprob": -3.193052053451538}, {"token": "com", "logprob": -3.263364553451538}, {"token": "pour", "logprob": -3.568052053451538}, {"token": "qui", "logprob": -3.638364553451538}, {"token": "d", "logprob": -3.716489553451538}, {"token": "type", "logprob": -3.857114553451538}, {"token": "ici", "logprob": -3.966489553451538}, {"token": "t", "logprob": -4.028989791870117}, {"token": "generate", "logprob": -4.052427291870117}, {"token": "voir", "logprob": -4.091489791870117}, {"token": "genre", "logprob": -4.107114791870117}, {"token": "g", "logprob": -4.114927291870117}, {"token": "aut", "logprob": -4.153989791870117}, {"token": "s", "logprob": -4.224302291870117}, {"token": "les", "logprob": -4.357114791870117}, {"token": "la", "logprob": -4.474302291870117}, {"token": "rem", "logprob": -4.505552291870117}, {"token": "c", "logprob": -4.552427291870117}, {"token": "ci", "logprob": -4.568052291870117}, {"token": "r", "logprob": -4.583677291870117}, {"token": "here", "logprob": -4.607114791870117}, {"token": "phrase", "logprob": -4.630552291870117}, {"token": "l", "logprob": -4.638364791870117}, {"token": "un", "logprob": -4.661802291870117}, {"token": "Ãł", "logprob": -4.739927291870117}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes ("}
{"step": 82, "token": "-in", "logprob": 0.0, "top_k": [{"token": "-in", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill"}
{"step": 83, "token": "-the", "logprob": 0.0, "top_k": [{"token": "-the", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in"}
{"step": 84, "token": "-bl", "logprob": 0.0, "top_k": [{"token": "-bl", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the"}
{"step": 85, "token": "ank", "logprob": 0.0, "top_k": [{"token": "ank", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-bl"}
{"step": 86, "token": ")", "logprob": -0.7745532393455505, "top_k": [{"token": ")", "logprob": -0.7745532393455505}, {"token": ").", "logprob": -1.0870532989501953}, {"token": ",", "logprob": -2.3683032989501953}, {"token": "),", "logprob": -2.7276782989501953}, {"token": "pour", "logprob": -3.1495532989501953}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank"}
{"step": 87, "token": "Ġpour", "logprob": -0.3600892722606659, "top_k": [{"token": "pour", "logprob": -0.3600892722606659}, {"token": "Ċ", "logprob": -2.0944643020629883}, {"token": "qui", "logprob": -2.5007143020629883}, {"token": "avec", "logprob": -3.8444643020629883}, {"token": "afin", "logprob": -3.8913393020629883}, {"token": "et", "logprob": -4.110089302062988}, {"token": "Ãł", "logprob": -4.164776802062988}, {"token": "dans", "logprob": -4.406964302062988}, {"token": "(", "logprob": -4.485089302062988}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank)"}
{"step": 88, "token": "ĠcrÃ©er", "logprob": -0.875227153301239, "top_k": [{"token": "crÃ©er", "logprob": -0.875227153301239}, {"token": "gÃ©n", "logprob": -1.4377272129058838}, {"token": "rempl", "logprob": -1.5158522129058838}, {"token": "la", "logprob": -3.531477212905884}, {"token": "que", "logprob": -3.789289712905884}, {"token": "le", "logprob": -3.922102212905884}, {"token": "les", "logprob": -4.109601974487305}, {"token": "produ", "logprob": -4.562726974487305}, {"token": "compl", "logprob": -4.593976974487305}, {"token": "obten", "logprob": -4.617414474487305}, {"token": "comb", "logprob": -4.875226974487305}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour"}
{"step": 89, "token": "Ġdes", "logprob": -0.17765860259532928, "top_k": [{"token": "des", "logprob": -0.17765860259532928}, {"token": "du", "logprob": -2.7557835578918457}, {"token": "un", "logprob": -2.7557835578918457}, {"token": "une", "logprob": -3.3339085578918457}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer"}
{"step": 90, "token": "Ġphrases", "logprob": 0.0, "top_k": [{"token": "phrases", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des"}
{"step": 91, "token": ".", "logprob": -0.08643000572919846, "top_k": [{"token": ".", "logprob": -0.08643000572919846}, {"token": "avec", "logprob": -3.023930072784424}, {"token": "compl", "logprob": -3.898930072784424}, {"token": "de", "logprob": -4.273930072784424}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases"}
{"step": 92, "token": "ĠĠĊ", "logprob": 0.0, "top_k": [{"token": "Ċ", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases."}
{"step": 93, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n"}
{"step": 94, "token": "Ġtext", "logprob": -0.9596402645111084, "top_k": [{"token": "text", "logprob": -0.9596402645111084}, {"token": "max", "logprob": -1.7565152645111084}, {"token": "num", "logprob": -2.2330777645111084}, {"token": "prompt", "logprob": -2.7877652645111084}, {"token": "#", "logprob": -3.3112027645111084}, {"token": "n", "logprob": -3.3268277645111084}, {"token": "seed", "logprob": -3.4518277645111084}, {"token": "context", "logprob": -3.5065152645111084}, {"token": "top", "logprob": -3.7408902645111084}, {"token": "length", "logprob": -3.9752652645111084}, {"token": "language", "logprob": -4.3346405029296875}, {"token": "input", "logprob": -4.3815155029296875}, {"token": "temperature", "logprob": -4.3893280029296875}, {"token": "model", "logprob": -4.4127655029296875}, {"token": "min", "logprob": -4.4440155029296875}, {"token": "tokenizer", "logprob": -4.5455780029296875}, {"token": "output", "logprob": -4.5846405029296875}, {"token": "sentence", "logprob": -4.7252655029296875}, {"token": "lang", "logprob": -4.9049530029296875}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n  "}
{"step": 95, "token": "_generate", "logprob": -0.5220179557800293, "top_k": [{"token": "_generate", "logprob": -0.5220179557800293}, {"token": "_length", "logprob": -1.9673304557800293}, {"token": "_max", "logprob": -2.1704554557800293}, {"token": "_generation", "logprob": -3.1860804557800293}, {"token": "_prompt", "logprob": -3.7720179557800293}, {"token": "_input", "logprob": -3.8188929557800293}, {"token": "_model", "logprob": -4.482955455780029}, {"token": "_source", "logprob": -4.522017955780029}, {"token": "_gen", "logprob": -4.686080455780029}, {"token": "_num", "logprob": -4.701705455780029}, {"token": "_seed", "logprob": -4.740767955780029}, {"token": "_n", "logprob": -5.061080455780029}, {"token": "_token", "logprob": -5.123580455780029}, {"token": "_fill", "logprob": -5.311080455780029}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text"}
{"step": 96, "token": "_length", "logprob": -1.1539305448532104, "top_k": [{"token": "_length", "logprob": -1.1539305448532104}, {"token": "_max", "logprob": -1.7633055448532104}, {"token": "_num", "logprob": -2.4742431640625}, {"token": "_type", "logprob": -2.9586181640625}, {"token": "_prompt", "logprob": -2.9742431640625}, {"token": "_number", "logprob": -3.4586181640625}, {"token": "_n", "logprob": -3.7398681640625}, {"token": "_size", "logprob": -3.7945556640625}, {"token": "_mode", "logprob": -3.8492431640625}, {"token": "_model", "logprob": -3.9508056640625}, {"token": "_topic", "logprob": -4.0601806640625}, {"token": "_input", "logprob": -4.0914306640625}, {"token": "_temperature", "logprob": -4.1304931640625}, {"token": "_top", "logprob": -4.2320556640625}, {"token": "_params", "logprob": -4.3101806640625}, {"token": "_kwargs", "logprob": -4.3648681640625}, {"token": "_token", "logprob": -4.3648681640625}, {"token": "_context", "logprob": -4.4117431640625}, {"token": "_text", "logprob": -4.4429931640625}, {"token": "_args", "logprob": -4.4664306640625}, {"token": "_len", "logprob": -4.5054931640625}, {"token": "_language", "logprob": -4.5523681640625}, {"token": "_count", "logprob": -4.6070556640625}, {"token": "_seed", "logprob": -4.7945556640625}, {"token": "_tokens", "logprob": -4.8726806640625}, {"token": "_limit", "logprob": -4.8883056640625}, {"token": "_content", "logprob": -4.9664306640625}, {"token": "_lang", "logprob": -4.9820556640625}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate"}
{"step": 97, "token": "Ġ=", "logprob": 0.0, "top_k": [{"token": "=", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length"}
{"step": 98, "token": "Ġ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length ="}
{"step": 99, "token": "500", "logprob": -2.51859188079834, "top_k": [{"token": "100", "logprob": -1.5498420000076294}, {"token": "50", "logprob": -2.42484188079834}, {"token": "200", "logprob": -2.42484188079834}, {"token": "500", "logprob": -2.51859188079834}, {"token": "10", "logprob": -2.75296688079834}, {"token": "150", "logprob": -2.81546688079834}, {"token": "300", "logprob": -2.97171688079834}, {"token": "20", "logprob": -3.08109188079834}, {"token": "250", "logprob": -3.29984188079834}, {"token": "5", "logprob": -3.45609188079834}, {"token": "30", "logprob": -3.53421688079834}, {"token": "400", "logprob": -3.75296688079834}, {"token": "128", "logprob": -4.08109188079834}, {"token": "15", "logprob": -4.12796688079834}, {"token": "120", "logprob": -4.14359188079834}, {"token": "256", "logprob": -4.17484188079834}, {"token": "1", "logprob": -4.19046688079834}, {"token": "40", "logprob": -4.23734188079834}, {"token": "25", "logprob": -4.29984188079834}, {"token": "80", "logprob": -4.31546688079834}, {"token": "3", "logprob": -4.45609188079834}, {"token": "512", "logprob": -4.51859188079834}, {"token": "60", "logprob": -4.53421688079834}, {"token": "2", "logprob": -4.54984188079834}, {"token": "70", "logprob": -4.75296688079834}, {"token": "140", "logprob": -4.81546688079834}, {"token": "64", "logprob": -4.95609188079834}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = "}
{"step": 100, "token": "ĠĠĊ", "logprob": -0.266189843416214, "top_k": [{"token": "Ċ", "logprob": -0.266189843416214}, {"token": "0", "logprob": -1.4536898136138916}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500"}
{"step": 101, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n"}
{"step": 102, "token": "Ġ#", "logprob": 0.0, "top_k": [{"token": "#", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n  "}
{"step": 103, "token": "ĠI", "logprob": 0.0, "top_k": [{"token": "I", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   #"}
{"step": 104, "token": "ci", "logprob": 0.0, "top_k": [{"token": "ci", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # I"}
{"step": 105, "token": ",", "logprob": 0.0, "top_k": [{"token": ",", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici"}
{"step": 106, "token": "Ġon", "logprob": 0.0, "top_k": [{"token": "on", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici,"}
{"step": 107, "token": "Ġutilise", "logprob": -0.3780148923397064, "top_k": [{"token": "utilise", "logprob": -0.3780148923397064}, {"token": "spÃ©c", "logprob": -2.2139523029327393}, {"token": "dÃ©f", "logprob": -2.9170773029327393}, {"token": "donne", "logprob": -3.4327023029327393}, {"token": "cr", "logprob": -3.5186398029327393}, {"token": "gÃ©n", "logprob": -3.5186398029327393}, {"token": "dÃ©", "logprob": -3.8233273029327393}, {"token": "fix", "logprob": -3.9405148029327393}, {"token": "ind", "logprob": -3.9873898029327393}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on"}
{"step": 108, "token": "Ġla", "logprob": -0.3630524277687073, "top_k": [{"token": "la", "logprob": -0.3630524277687073}, {"token": "une", "logprob": -1.4411773681640625}, {"token": "le", "logprob": -2.6911773681640625}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise"}
{"step": 109, "token": "Ġlong", "logprob": 0.0, "top_k": [{"token": "long", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la"}
{"step": 110, "token": "ueur", "logprob": 0.0, "top_k": [{"token": "ueur", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la long"}
{"step": 111, "token": "Ġdes", "logprob": -2.251368999481201, "top_k": [{"token": "de", "logprob": -0.5013691186904907}, {"token": "du", "logprob": -1.8607441186904907}, {"token": "des", "logprob": -2.251368999481201}, {"token": "maxim", "logprob": -2.579493999481201}, {"token": "(", "logprob": -2.852931499481201}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur"}
{"step": 112, "token": "Ġphrases", "logprob": -0.1095343679189682, "top_k": [{"token": "phrases", "logprob": -0.1095343679189682}, {"token": "text", "logprob": -2.26578426361084}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des"}
{"step": 113, "token": "ĠÃł", "logprob": -1.0151399374008179, "top_k": [{"token": "(", "logprob": -0.9682649374008179}, {"token": "Ãł", "logprob": -1.0151399374008179}, {"token": "gÃ©n", "logprob": -2.5151400566101074}, {"token": "que", "logprob": -2.5932650566101074}, {"token": "pour", "logprob": -2.6401400566101074}, {"token": "crÃ©", "logprob": -3.4760775566101074}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases"}
{"step": 114, "token": "ĠgÃ©n", "logprob": -0.18495334684848785, "top_k": [{"token": "gÃ©n", "logprob": -0.18495334684848785}, {"token": "crÃ©er", "logprob": -1.7787033319473267}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à"}
{"step": 115, "token": "Ã©rer", "logprob": 0.0, "top_k": [{"token": "Ã©rer", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à gén"}
{"step": 116, "token": "Ġ(", "logprob": -0.541875958442688, "top_k": [{"token": "(", "logprob": -0.541875958442688}, {"token": ".", "logprob": -0.979375958442688}, {"token": "pour", "logprob": -3.1512508392333984}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer"}
{"step": 117, "token": "500", "logprob": -0.13123120367527008, "top_k": [{"token": "500", "logprob": -0.13123120367527008}, {"token": "en", "logprob": -2.5843563079833984}, {"token": "ici", "logprob": -4.256231307983398}, {"token": "50", "logprob": -4.279668807983398}, {"token": "d", "logprob": -4.529668807983398}, {"token": "100", "logprob": -4.740606307983398}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer ("}
{"step": 118, "token": "Ġpar", "logprob": -2.0525989532470703, "top_k": [{"token": ").", "logprob": -1.0447865724563599}, {"token": "mots", "logprob": -1.4666615724563599}, {"token": ")", "logprob": -1.8729115724563599}, {"token": "par", "logprob": -2.0525989532470703}, {"token": "caract", "logprob": -2.2244739532470703}, {"token": "pour", "logprob": -3.5994739532470703}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500"}
{"step": 119, "token": "ĠdÃ©f", "logprob": 0.0, "top_k": [{"token": "dÃ©f", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par"}
{"step": 120, "token": "aut", "logprob": 0.0, "top_k": [{"token": "aut", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par déf"}
{"step": 121, "token": ")", "logprob": -2.4496536254882812, "top_k": [{"token": ").", "logprob": -0.09027863293886185}, {"token": ")", "logprob": -2.4496536254882812}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut"}
{"step": 122, "token": "ĠĠĊ", "logprob": -0.8104487657546997, "top_k": [{"token": "pour", "logprob": -0.7948237657546997}, {"token": "Ċ", "logprob": -0.8104487657546997}, {"token": "Ãł", "logprob": -3.63857364654541}, {"token": "Âł", "logprob": -3.68544864654541}, {"token": ".", "logprob": -3.93544864654541}, {"token": "et", "logprob": -4.02919864654541}, {"token": "(", "logprob": -4.20107364654541}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)"}
{"step": 123, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n"}
{"step": 124, "token": "Ġnum", "logprob": -3.1205222606658936, "top_k": [{"token": "text", "logprob": -0.3236473500728607}, {"token": "```", "logprob": -3.0189597606658936}, {"token": "num", "logprob": -3.1205222606658936}, {"token": "seed", "logprob": -3.2377097606658936}, {"token": "max", "logprob": -3.3080222606658936}, {"token": "#", "logprob": -3.3314597606658936}, {"token": "``", "logprob": -3.9877097606658936}, {"token": "```Ċ", "logprob": -4.198647499084473}, {"token": "prompt", "logprob": -4.300209999084473}, {"token": "context", "logprob": -4.378334999084473}, {"token": "language", "logprob": -4.401772499084473}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n  "}
{"step": 125, "token": "_samples", "logprob": -1.483513355255127, "top_k": [{"token": "_samples", "logprob": -1.483513355255127}, {"token": "_be", "logprob": -1.881950855255127}, {"token": "_generate", "logprob": -2.131950855255127}, {"token": "_of", "logprob": -2.335075855255127}, {"token": "_return", "logprob": -2.600700855255127}, {"token": "_texts", "logprob": -2.913200855255127}, {"token": "_sentences", "logprob": -3.061638355255127}, {"token": "_text", "logprob": -3.217888355255127}, {"token": "_gener", "logprob": -3.421013355255127}, {"token": "_examples", "logprob": -3.694450855255127}, {"token": "_generated", "logprob": -3.905388355255127}, {"token": "_words", "logprob": -4.069450855255127}, {"token": "_generation", "logprob": -4.421013355255127}, {"token": "_paragraph", "logprob": -4.421013355255127}, {"token": "_pl", "logprob": -4.585075855255127}, {"token": "_to", "logprob": -4.788200855255127}, {"token": "_iterations", "logprob": -4.803825855255127}, {"token": "_results", "logprob": -4.842888355255127}, {"token": "_tokens", "logprob": -4.850700855255127}, {"token": "_prompt", "logprob": -4.897575855255127}, {"token": "_prom", "logprob": -4.913200855255127}, {"token": "_seed", "logprob": -4.928825855255127}, {"token": "_ex", "logprob": -4.991325855255127}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num"}
{"step": 126, "token": "Ġ=", "logprob": 0.0, "top_k": [{"token": "=", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples"}
{"step": 127, "token": "Ġ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples ="}
{"step": 128, "token": "5", "logprob": -2.0828757286071777, "top_k": [{"token": "1", "logprob": -0.676625669002533}, {"token": "10", "logprob": -1.9735007286071777}, {"token": "5", "logprob": -2.0828757286071777}, {"token": "3", "logprob": -2.5828757286071777}, {"token": "2", "logprob": -2.9735007286071777}, {"token": "100", "logprob": -2.9891257286071777}, {"token": "20", "logprob": -3.5360007286071777}, {"token": "50", "logprob": -3.8172507286071777}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = "}
{"step": 129, "token": "ĠĠĊ", "logprob": 0.0, "top_k": [{"token": "Ċ", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5"}
{"step": 130, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n"}
{"step": 131, "token": "Ġ#", "logprob": 0.0, "top_k": [{"token": "#", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n  "}
{"step": 132, "token": "ĠI", "logprob": 0.0, "top_k": [{"token": "I", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   #"}
{"step": 133, "token": "ci", "logprob": 0.0, "top_k": [{"token": "ci", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # I"}
{"step": 134, "token": ",", "logprob": 0.0, "top_k": [{"token": ",", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici"}
{"step": 135, "token": "Ġon", "logprob": 0.0, "top_k": [{"token": "on", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici,"}
{"step": 136, "token": "Ġutilise", "logprob": -0.13656750321388245, "top_k": [{"token": "utilise", "logprob": -0.13656750321388245}, {"token": "gÃ©n", "logprob": -2.0584425926208496}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on"}
{"step": 137, "token": "Ġle", "logprob": 0.0, "top_k": [{"token": "le", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise"}
{"step": 138, "token": "Ġnombre", "logprob": 0.0, "top_k": [{"token": "nombre", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le"}
{"step": 139, "token": "Ġd", "logprob": -0.8084917664527893, "top_k": [{"token": "de", "logprob": -0.5897417664527893}, {"token": "d", "logprob": -0.8084917664527893}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre"}
{"step": 140, "token": "'Ã©", "logprob": -0.16491271555423737, "top_k": [{"token": "'Ã©", "logprob": -0.16491271555423737}, {"token": "'ex", "logprob": -1.8836627006530762}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d"}
{"step": 141, "token": "chant", "logprob": 0.0, "top_k": [{"token": "chant", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'é"}
{"step": 142, "token": "ill", "logprob": 0.0, "top_k": [{"token": "ill", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échant"}
{"step": 143, "token": "ons", "logprob": 0.0, "top_k": [{"token": "ons", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantill"}
{"step": 144, "token": "ĠÃł", "logprob": -0.424868643283844, "top_k": [{"token": "Ãł", "logprob": -0.424868643283844}, {"token": "de", "logprob": -1.6123685836791992}, {"token": "(", "logprob": -2.049868583679199}, {"token": "gÃ©n", "logprob": -4.018618583679199}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons"}
{"step": 145, "token": "ĠgÃ©n", "logprob": -0.061035625636577606, "top_k": [{"token": "gÃ©n", "logprob": -0.061035625636577606}, {"token": "utiliser", "logprob": -2.826660633087158}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à"}
{"step": 146, "token": "Ã©rer", "logprob": 0.0, "top_k": [{"token": "Ã©rer", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à gén"}
{"step": 147, "token": "Ġ(", "logprob": -0.11918776482343674, "top_k": [{"token": "(", "logprob": -0.11918776482343674}, {"token": "pour", "logprob": -2.666062831878662}, {"token": ".", "logprob": -3.150437831878662}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer"}
{"step": 148, "token": "5", "logprob": 0.0, "top_k": [{"token": "5", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer ("}
{"step": 149, "token": "Ġpar", "logprob": 0.0, "top_k": [{"token": "par", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5"}
{"step": 150, "token": "ĠdÃ©f", "logprob": 0.0, "top_k": [{"token": "dÃ©f", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par"}
{"step": 151, "token": "aut", "logprob": 0.0, "top_k": [{"token": "aut", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par déf"}
{"step": 152, "token": ")", "logprob": -0.19302435219287872, "top_k": [{"token": ")", "logprob": -0.19302435219287872}, {"token": ").", "logprob": -1.7398993968963623}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut"}
{"step": 153, "token": "ĠĠĊ", "logprob": 0.0, "top_k": [{"token": "Ċ", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)"}
{"step": 154, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n"}
{"step": 155, "token": "Ġnum", "logprob": -1.860525369644165, "top_k": [{"token": "seed", "logprob": -1.258962869644165}, {"token": "text", "logprob": -1.829275369644165}, {"token": "num", "logprob": -1.860525369644165}, {"token": "max", "logprob": -2.758962869644165}, {"token": "```", "logprob": -2.829275369644165}, {"token": "temperature", "logprob": -3.048025369644165}, {"token": "#", "logprob": -3.555837869644165}, {"token": "``", "logprob": -3.649587869644165}, {"token": "prompt", "logprob": -3.798025369644165}, {"token": "top", "logprob": -3.907400369644165}, {"token": "context", "logprob": -4.040212631225586}, {"token": "output", "logprob": -4.087087631225586}, {"token": "model", "logprob": -4.102712631225586}, {"token": "sample", "logprob": -4.227712631225586}, {"token": "random", "logprob": -4.329275131225586}, {"token": "use", "logprob": -4.360525131225586}, {"token": "```Ċ", "logprob": -4.415212631225586}, {"token": "generate", "logprob": -4.602712631225586}, {"token": "batch", "logprob": -4.610525131225586}, {"token": "return", "logprob": -4.657400131225586}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n  "}
{"step": 156, "token": "_be", "logprob": -1.0777331590652466, "top_k": [{"token": "_be", "logprob": -1.0777331590652466}, {"token": "_return", "logprob": -1.3355456590652466}, {"token": "_words", "logprob": -3.358983039855957}, {"token": "_tokens", "logprob": -3.452733039855957}, {"token": "_sentences", "logprob": -3.679295539855957}, {"token": "_samples", "logprob": -3.694920539855957}, {"token": "_epochs", "logprob": -3.733983039855957}, {"token": "_top", "logprob": -3.741795539855957}, {"token": "_rows", "logprob": -3.757420539855957}, {"token": "_b", "logprob": -3.999608039855957}, {"token": "_gener", "logprob": -4.108983039855957}, {"token": "_chars", "logprob": -4.226170539855957}, {"token": "_batch", "logprob": -4.265233039855957}, {"token": "_beam", "logprob": -4.273045539855957}, {"token": "_iterations", "logprob": -4.382420539855957}, {"token": "_seed", "logprob": -4.437108039855957}, {"token": "_head", "logprob": -4.437108039855957}, {"token": "_output", "logprob": -4.523045539855957}, {"token": "_w", "logprob": -4.562108039855957}, {"token": "_steps", "logprob": -4.569920539855957}, {"token": "_outputs", "logprob": -4.616795539855957}, {"token": "_of", "logprob": -4.718358039855957}, {"token": "_generate", "logprob": -4.780858039855957}, {"token": "_length", "logprob": -4.812108039855957}, {"token": "_in", "logprob": -4.905858039855957}, {"token": "_ch", "logprob": -4.913670539855957}, {"token": "_context", "logprob": -4.952733039855957}, {"token": "_workers", "logprob": -4.983983039855957}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num"}
{"step": 157, "token": "ams", "logprob": 0.0, "top_k": [{"token": "ams", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_be"}
{"step": 158, "token": "Ġ=", "logprob": 0.0, "top_k": [{"token": "=", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams"}
{"step": 159, "token": "Ġ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams ="}
{"step": 160, "token": "2", "logprob": -1.9834566116333008, "top_k": [{"token": "5", "logprob": -1.1865816116333008}, {"token": "4", "logprob": -1.4990816116333008}, {"token": "2", "logprob": -1.9834566116333008}, {"token": "3", "logprob": -1.9990816116333008}, {"token": "1", "logprob": -2.030331611633301}, {"token": "10", "logprob": -2.702206611633301}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = "}
{"step": 161, "token": "ĠĠĊ", "logprob": 0.0, "top_k": [{"token": "Ċ", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2"}
{"step": 162, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n"}
{"step": 163, "token": "Ġ#", "logprob": 0.0, "top_k": [{"token": "#", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n  "}
{"step": 164, "token": "ĠI", "logprob": 0.0, "top_k": [{"token": "I", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   #"}
{"step": 165, "token": "ci", "logprob": 0.0, "top_k": [{"token": "ci", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # I"}
{"step": 166, "token": ",", "logprob": 0.0, "top_k": [{"token": ",", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici"}
{"step": 167, "token": "Ġon", "logprob": 0.0, "top_k": [{"token": "on", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici,"}
{"step": 168, "token": "Ġutilise", "logprob": 0.0, "top_k": [{"token": "utilise", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on"}
{"step": 169, "token": "Ġle", "logprob": 0.0, "top_k": [{"token": "le", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise"}
{"step": 170, "token": "Ġnombre", "logprob": 0.0, "top_k": [{"token": "nombre", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le"}
{"step": 171, "token": "Ġde", "logprob": 0.0, "top_k": [{"token": "de", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre"}
{"step": 172, "token": "Ġb", "logprob": -2.7055397033691406, "top_k": [{"token": "beams", "logprob": -1.9321022033691406}, {"token": "be", "logprob": -2.0336647033691406}, {"token": "bou", "logprob": -2.0727272033691406}, {"token": "p", "logprob": -2.5727272033691406}, {"token": "b", "logprob": -2.7055397033691406}, {"token": "branches", "logprob": -3.0805397033691406}, {"token": "jets", "logprob": -3.4086647033691406}, {"token": "fais", "logprob": -3.4711647033691406}, {"token": "tours", "logprob": -3.5414772033691406}, {"token": "d", "logprob": -3.7680397033691406}, {"token": "point", "logprob": -3.8461647033691406}, {"token": "br", "logprob": -3.9555397033691406}, {"token": "beam", "logprob": -3.9633522033691406}, {"token": "bal", "logprob": -3.9867897033691406}, {"token": "\"", "logprob": -3.9946022033691406}, {"token": "points", "logprob": -4.025852203369141}, {"token": "tra", "logprob": -4.033664703369141}, {"token": "fe", "logprob": -4.119602203369141}, {"token": "ray", "logprob": -4.143039703369141}, {"token": "gr", "logprob": -4.189914703369141}, {"token": "feu", "logprob": -4.244602203369141}, {"token": "bomb", "logprob": -4.299289703369141}, {"token": "tr", "logprob": -4.377414703369141}, {"token": "rails", "logprob": -4.525852203369141}, {"token": "cou", "logprob": -4.603977203369141}, {"token": "pan", "logprob": -4.627414703369141}, {"token": "lign", "logprob": -4.666477203369141}, {"token": "tubes", "logprob": -4.674289703369141}, {"token": "fils", "logprob": -4.752414703369141}, {"token": "bar", "logprob": -4.869602203369141}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de"}
{"step": 173, "token": "Ã¢", "logprob": -2.1574959754943848, "top_k": [{"token": "anni", "logprob": -1.6887460947036743}, {"token": "Ãª", "logprob": -1.9074960947036743}, {"token": "Ã¢", "logprob": -2.1574959754943848}, {"token": "iais", "logprob": -2.2199959754943848}, {"token": "ords", "logprob": -2.8840584754943848}, {"token": "rou", "logprob": -3.0246834754943848}, {"token": "Ã»", "logprob": -3.2121834754943848}, {"token": "Ã¨", "logprob": -3.5637459754943848}, {"token": "Ã©c", "logprob": -3.6653084754943848}, {"token": "ouch", "logprob": -3.8371834754943848}, {"token": "Åĵ", "logprob": -3.8528084754943848}, {"token": "Ãĥ", "logprob": -3.8684334754943848}, {"token": "annes", "logprob": -3.9543709754943848}, {"token": "acs", "logprob": -4.001245975494385}, {"token": "refs", "logprob": -4.009058475494385}, {"token": "Ã©", "logprob": -4.071558475494385}, {"token": "ennes", "logprob": -4.212183475494385}, {"token": "oust", "logprob": -4.321558475494385}, {"token": "anches", "logprob": -4.329370975494385}, {"token": "oute", "logprob": -4.391870975494385}, {"token": "Ã¢t", "logprob": -4.462183475494385}, {"token": "ille", "logprob": -4.469995975494385}, {"token": "ous", "logprob": -4.501245975494385}, {"token": "Ã´", "logprob": -4.563745975494385}, {"token": "rea", "logprob": -4.665308475494385}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de b"}
{"step": 174, "token": "tons", "logprob": -0.15121398866176605, "top_k": [{"token": "tons", "logprob": -0.15121398866176605}, {"token": "ton", "logprob": -1.9637140035629272}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâ"}
{"step": 175, "token": "ĠÃł", "logprob": -0.24774070084095, "top_k": [{"token": "Ãł", "logprob": -0.24774070084095}, {"token": "(", "logprob": -2.4352407455444336}, {"token": "pour", "logprob": -2.6227407455444336}, {"token": "de", "logprob": -2.8258657455444336}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons"}
{"step": 176, "token": "Ġutiliser", "logprob": 0.0, "top_k": [{"token": "utiliser", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à"}
{"step": 177, "token": "Ġpour", "logprob": -0.358582079410553, "top_k": [{"token": "pour", "logprob": -0.358582079410553}, {"token": "(", "logprob": -1.3742070198059082}, {"token": "dans", "logprob": -3.030457019805908}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser"}
{"step": 178, "token": "Ġla", "logprob": -0.5278103947639465, "top_k": [{"token": "la", "logprob": -0.5278103947639465}, {"token": "gÃ©n", "logprob": -1.7153103351593018}, {"token": "le", "logprob": -2.2934353351593018}, {"token": "les", "logprob": -2.7699978351593018}, {"token": "l", "logprob": -3.3246853351593018}, {"token": "Ã©", "logprob": -4.129373073577881}, {"token": "effect", "logprob": -4.230935573577881}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour"}
{"step": 179, "token": "ĠgÃ©nÃ©", "logprob": -0.11567297577857971, "top_k": [{"token": "gÃ©nÃ©", "logprob": -0.11567297577857971}, {"token": "recherche", "logprob": -2.560985565185547}, {"token": "gÃ©n", "logprob": -4.115673065185547}, {"token": "sÃ©lection", "logprob": -4.154735565185547}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la"}
{"step": 180, "token": "ration", "logprob": 0.0, "top_k": [{"token": "ration", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la géné"}
{"step": 181, "token": "Ġdes", "logprob": -1.8932840824127197, "top_k": [{"token": "(", "logprob": -0.7995340824127197}, {"token": "de", "logprob": -1.1901590824127197}, {"token": "des", "logprob": -1.8932840824127197}, {"token": "du", "logprob": -2.3464090824127197}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération"}
{"step": 182, "token": "Ġphrases", "logprob": -0.19302435219287872, "top_k": [{"token": "phrases", "logprob": -0.19302435219287872}, {"token": "text", "logprob": -1.7398993968963623}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des"}
{"step": 183, "token": "Ġ(", "logprob": 0.0, "top_k": [{"token": "(", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases"}
{"step": 184, "token": "2", "logprob": 0.0, "top_k": [{"token": "2", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases ("}
{"step": 185, "token": "Ġpar", "logprob": 0.0, "top_k": [{"token": "par", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2"}
{"step": 186, "token": "ĠdÃ©f", "logprob": 0.0, "top_k": [{"token": "dÃ©f", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par"}
{"step": 187, "token": "aut", "logprob": 0.0, "top_k": [{"token": "aut", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par déf"}
{"step": 188, "token": ")", "logprob": 0.0, "top_k": [{"token": ")", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut"}
{"step": 189, "token": "ĠĠĊ", "logprob": 0.0, "top_k": [{"token": "Ċ", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)"}
{"step": 190, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n"}
{"step": 191, "token": "Ġmax", "logprob": -2.4281978607177734, "top_k": [{"token": "num", "logprob": -1.1938228607177734}, {"token": "seed", "logprob": -1.9594478607177734}, {"token": "temperature", "logprob": -1.9594478607177734}, {"token": "text", "logprob": -2.1391353607177734}, {"token": "max", "logprob": -2.4281978607177734}, {"token": "top", "logprob": -3.6156978607177734}, {"token": "#", "logprob": -3.8344478607177734}, {"token": "random", "logprob": -4.186010360717773}, {"token": "min", "logprob": -4.240697860717773}, {"token": "do", "logprob": -4.342260360717773}, {"token": "use", "logprob": -4.342260360717773}, {"token": "sample", "logprob": -4.350072860717773}, {"token": "```", "logprob": -4.482885360717773}, {"token": "``", "logprob": -4.490697860717773}, {"token": "model", "logprob": -4.498510360717773}, {"token": "batch", "logprob": -4.545385360717773}, {"token": "vocab", "logprob": -4.568822860717773}, {"token": "length", "logprob": -4.592260360717773}, {"token": "output", "logprob": -4.631322860717773}, {"token": "context", "logprob": -4.662572860717773}, {"token": "temp", "logprob": -4.811010360717773}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n  "}
{"step": 192, "token": "_seq", "logprob": -1.9686784744262695, "top_k": [{"token": "_length", "logprob": -0.8358659744262695}, {"token": "_new", "logprob": -1.8827409744262695}, {"token": "_seq", "logprob": -1.9686784744262695}, {"token": "_tokens", "logprob": -2.8671159744262695}, {"token": "_len", "logprob": -3.4374284744262695}, {"token": "_output", "logprob": -3.5936784744262695}, {"token": "_gen", "logprob": -3.6249284744262695}, {"token": "_dec", "logprob": -4.0155534744262695}, {"token": "_decoder", "logprob": -4.0780534744262695}, {"token": "_text", "logprob": -4.1874284744262695}, {"token": "_num", "logprob": -4.2499284744262695}, {"token": "_sequence", "logprob": -4.3905534744262695}, {"token": "_token", "logprob": -4.4139909744262695}, {"token": "_samples", "logprob": -4.5468034744262695}, {"token": "_train", "logprob": -4.7343034744262695}, {"token": "_target", "logprob": -4.8358659744262695}, {"token": "_sentences", "logprob": -4.8514909744262695}, {"token": "_sentence", "logprob": -4.8983659744262695}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max"}
{"step": 193, "token": "_len", "logprob": -0.8981232643127441, "top_k": [{"token": "_length", "logprob": -0.5231232643127441}, {"token": "_len", "logprob": -0.8981232643127441}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq"}
{"step": 194, "token": "Ġ=", "logprob": 0.0, "top_k": [{"token": "=", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len"}
{"step": 195, "token": "Ġ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len ="}
{"step": 196, "token": "500", "logprob": -1.9228030443191528, "top_k": [{"token": "100", "logprob": -1.4696780443191528}, {"token": "500", "logprob": -1.9228030443191528}, {"token": "512", "logprob": -2.4228029251098633}, {"token": "200", "logprob": -2.5790529251098633}, {"token": "50", "logprob": -2.7821779251098633}, {"token": "256", "logprob": -2.8759279251098633}, {"token": "102", "logprob": -3.0009279251098633}, {"token": "128", "logprob": -3.5009279251098633}, {"token": "300", "logprob": -3.5165529251098633}, {"token": "150", "logprob": -3.5165529251098633}, {"token": "20", "logprob": -3.5321779251098633}, {"token": "400", "logprob": -3.6571779251098633}, {"token": "10", "logprob": -3.7509279251098633}, {"token": "250", "logprob": -3.7821779251098633}, {"token": "64", "logprob": -3.9853029251098633}, {"token": "30", "logprob": -4.047802925109863}, {"token": "204", "logprob": -4.079052925109863}, {"token": "40", "logprob": -4.250927925109863}, {"token": "32", "logprob": -4.344677925109863}, {"token": "80", "logprob": -4.485302925109863}, {"token": "60", "logprob": -4.719677925109863}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = "}
{"step": 197, "token": "ĠĠĊ", "logprob": -0.11792946606874466, "top_k": [{"token": "Ċ", "logprob": -0.11792946606874466}, {"token": "0", "logprob": -2.196054458618164}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500"}
{"step": 198, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n"}
{"step": 199, "token": "Ġ#", "logprob": 0.0, "top_k": [{"token": "#", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n  "}
{"step": 200, "token": "ĠI", "logprob": 0.0, "top_k": [{"token": "I", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   #"}
{"step": 201, "token": "ci", "logprob": 0.0, "top_k": [{"token": "ci", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # I"}
{"step": 202, "token": ",", "logprob": 0.0, "top_k": [{"token": ",", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici"}
{"step": 203, "token": "Ġon", "logprob": 0.0, "top_k": [{"token": "on", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici,"}
{"step": 204, "token": "Ġutilise", "logprob": 0.0, "top_k": [{"token": "utilise", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on"}
{"step": 205, "token": "Ġla", "logprob": 0.0, "top_k": [{"token": "la", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise"}
{"step": 206, "token": "Ġlong", "logprob": 0.0, "top_k": [{"token": "long", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la"}
{"step": 207, "token": "ueur", "logprob": 0.0, "top_k": [{"token": "ueur", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la long"}
{"step": 208, "token": "Ġmaxim", "logprob": -0.16030777990818024, "top_k": [{"token": "maxim", "logprob": -0.16030777990818024}, {"token": "maximum", "logprob": -2.1759328842163086}, {"token": "du", "logprob": -3.3634328842163086}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur"}
{"step": 209, "token": "ale", "logprob": 0.0, "top_k": [{"token": "ale", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maxim"}
{"step": 210, "token": "Ġdes", "logprob": -0.2726176679134369, "top_k": [{"token": "des", "logprob": -0.2726176679134369}, {"token": "de", "logprob": -1.7257426977157593}, {"token": "du", "logprob": -2.8038675785064697}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale"}
{"step": 211, "token": "Ġphrases", "logprob": -0.11620324850082397, "top_k": [{"token": "phrases", "logprob": -0.11620324850082397}, {"token": "sÃ©", "logprob": -2.2099533081054688}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des"}
{"step": 212, "token": "ĠÃł", "logprob": -0.17326511442661285, "top_k": [{"token": "Ãł", "logprob": -0.17326511442661285}, {"token": "(", "logprob": -2.267015218734741}, {"token": "gÃ©n", "logprob": -2.892015218734741}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases"}
{"step": 213, "token": "ĠgÃ©n", "logprob": 0.0, "top_k": [{"token": "gÃ©n", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à"}
{"step": 214, "token": "Ã©rer", "logprob": 0.0, "top_k": [{"token": "Ã©rer", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à gén"}
{"step": 215, "token": "Ġ(", "logprob": 0.0, "top_k": [{"token": "(", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer"}
{"step": 216, "token": "500", "logprob": 0.0, "top_k": [{"token": "500", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer ("}
{"step": 217, "token": "Ġpar", "logprob": 0.0, "top_k": [{"token": "par", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500"}
{"step": 218, "token": "ĠdÃ©f", "logprob": 0.0, "top_k": [{"token": "dÃ©f", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par"}
{"step": 219, "token": "aut", "logprob": 0.0, "top_k": [{"token": "aut", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par déf"}
{"step": 220, "token": ")", "logprob": 0.0, "top_k": [{"token": ")", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut"}
{"step": 221, "token": "ĠĠĊ", "logprob": 0.0, "top_k": [{"token": "Ċ", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)"}
{"step": 222, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n"}
{"step": 223, "token": "Ġmin", "logprob": -2.533761739730835, "top_k": [{"token": "seed", "logprob": -1.721261739730835}, {"token": "num", "logprob": -1.736886739730835}, {"token": "temperature", "logprob": -2.057199239730835}, {"token": "max", "logprob": -2.525949239730835}, {"token": "min", "logprob": -2.533761739730835}, {"token": "text", "logprob": -2.846261739730835}, {"token": "top", "logprob": -3.400949239730835}, {"token": "do", "logprob": -3.713449239730835}, {"token": "#", "logprob": -3.736886739730835}, {"token": "batch", "logprob": -3.760324239730835}, {"token": "model", "logprob": -3.799386739730835}, {"token": "sample", "logprob": -4.065011978149414}, {"token": "use", "logprob": -4.104074478149414}, {"token": "```", "logprob": -4.236886978149414}, {"token": "output", "logprob": -4.252511978149414}, {"token": "**", "logprob": -4.385324478149414}, {"token": "vocab", "logprob": -4.400949478149414}, {"token": "random", "logprob": -4.416574478149414}, {"token": "temp", "logprob": -4.479074478149414}, {"token": "stop", "logprob": -4.611886978149414}, {"token": "tokenizer", "logprob": -4.690011978149414}, {"token": "``", "logprob": -4.705636978149414}, {"token": "prompt", "logprob": -4.729074478149414}, {"token": "return", "logprob": -4.760324478149414}, {"token": "eos", "logprob": -4.783761978149414}, {"token": "context", "logprob": -4.799386978149414}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n  "}
{"step": 224, "token": "_seq", "logprob": -0.38001513481140137, "top_k": [{"token": "_seq", "logprob": -0.38001513481140137}, {"token": "_length", "logprob": -2.4268901348114014}, {"token": "_dec", "logprob": -2.6065776348114014}, {"token": "_freq", "logprob": -3.1534526348114014}, {"token": "_tokens", "logprob": -3.4737651348114014}, {"token": "_gen", "logprob": -3.8722026348114014}, {"token": "_token", "logprob": -3.9815776348114014}, {"token": "_len", "logprob": -4.1847028732299805}, {"token": "_word", "logprob": -4.3331403732299805}, {"token": "_frequency", "logprob": -4.3800153732299805}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min"}
{"step": 225, "token": "_len", "logprob": 0.0, "top_k": [{"token": "_len", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq"}
{"step": 226, "token": "Ġ=", "logprob": 0.0, "top_k": [{"token": "=", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len"}
{"step": 227, "token": "Ġ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len ="}
{"step": 228, "token": "300", "logprob": -3.6728501319885254, "top_k": [{"token": "100", "logprob": -1.547850251197815}, {"token": "10", "logprob": -1.875975251197815}, {"token": "50", "logprob": -1.922850251197815}, {"token": "500", "logprob": -2.3916001319885254}, {"token": "20", "logprob": -2.5322251319885254}, {"token": "5", "logprob": -2.6416001319885254}, {"token": "200", "logprob": -3.1103501319885254}, {"token": "1", "logprob": -3.2666001319885254}, {"token": "30", "logprob": -3.4853501319885254}, {"token": "0", "logprob": -3.5478501319885254}, {"token": "150", "logprob": -3.5791001319885254}, {"token": "300", "logprob": -3.6728501319885254}, {"token": "250", "logprob": -4.000975131988525}, {"token": "2", "logprob": -4.047850131988525}, {"token": "3", "logprob": -4.235350131988525}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = "}
{"step": 229, "token": "ĠĠĊ", "logprob": 0.0, "top_k": [{"token": "Ċ", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300"}
{"step": 230, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n"}
{"step": 231, "token": "Ġ#", "logprob": 0.0, "top_k": [{"token": "#", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n  "}
{"step": 232, "token": "ĠI", "logprob": 0.0, "top_k": [{"token": "I", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   #"}
{"step": 233, "token": "ci", "logprob": 0.0, "top_k": [{"token": "ci", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # I"}
{"step": 234, "token": ",", "logprob": 0.0, "top_k": [{"token": ",", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici"}
{"step": 235, "token": "Ġon", "logprob": 0.0, "top_k": [{"token": "on", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici,"}
{"step": 236, "token": "Ġutilise", "logprob": 0.0, "top_k": [{"token": "utilise", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on"}
{"step": 237, "token": "Ġla", "logprob": 0.0, "top_k": [{"token": "la", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise"}
{"step": 238, "token": "Ġlong", "logprob": 0.0, "top_k": [{"token": "long", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la"}
{"step": 239, "token": "ueur", "logprob": 0.0, "top_k": [{"token": "ueur", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la long"}
{"step": 240, "token": "Ġminim", "logprob": 0.0, "top_k": [{"token": "minim", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur"}
{"step": 241, "token": "ale", "logprob": 0.0, "top_k": [{"token": "ale", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minim"}
{"step": 242, "token": "Ġdes", "logprob": 0.0, "top_k": [{"token": "des", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale"}
{"step": 243, "token": "Ġphrases", "logprob": 0.0, "top_k": [{"token": "phrases", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des"}
{"step": 244, "token": "ĠÃł", "logprob": 0.0, "top_k": [{"token": "Ãł", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases"}
{"step": 245, "token": "ĠgÃ©n", "logprob": 0.0, "top_k": [{"token": "gÃ©n", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à"}
{"step": 246, "token": "Ã©rer", "logprob": 0.0, "top_k": [{"token": "Ã©rer", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à gén"}
{"step": 247, "token": "Ġ(", "logprob": 0.0, "top_k": [{"token": "(", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer"}
{"step": 248, "token": "300", "logprob": 0.0, "top_k": [{"token": "300", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer ("}
{"step": 249, "token": "Ġpar", "logprob": 0.0, "top_k": [{"token": "par", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300"}
{"step": 250, "token": "ĠdÃ©f", "logprob": 0.0, "top_k": [{"token": "dÃ©f", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par"}
{"step": 251, "token": "aut", "logprob": 0.0, "top_k": [{"token": "aut", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par déf"}
{"step": 252, "token": ")", "logprob": 0.0, "top_k": [{"token": ")", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut"}
{"step": 253, "token": "ĠĠĊ", "logprob": 0.0, "top_k": [{"token": "Ċ", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)"}
{"step": 254, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n"}
{"step": 255, "token": "Ġn", "logprob": -4.677668571472168, "top_k": [{"token": "temperature", "logprob": -1.7948558330535889}, {"token": "num", "logprob": -1.8339183330535889}, {"token": "seed", "logprob": -2.177668333053589}, {"token": "max", "logprob": -2.216730833053589}, {"token": "text", "logprob": -2.810480833053589}, {"token": "min", "logprob": -2.872980833053589}, {"token": "top", "logprob": -3.185480833053589}, {"token": "do", "logprob": -3.497980833053589}, {"token": "batch", "logprob": -3.685480833053589}, {"token": "model", "logprob": -3.716730833053589}, {"token": "sample", "logprob": -3.833918333053589}, {"token": "use", "logprob": -3.927668333053589}, {"token": "#", "logprob": -4.068293571472168}, {"token": "temp", "logprob": -4.208918571472168}, {"token": "stop", "logprob": -4.240168571472168}, {"token": "output", "logprob": -4.357356071472168}, {"token": "prompt", "logprob": -4.396418571472168}, {"token": "vocab", "logprob": -4.412043571472168}, {"token": "length", "logprob": -4.419856071472168}, {"token": "random", "logprob": -4.458918571472168}, {"token": "tokenizer", "logprob": -4.552668571472168}, {"token": "context", "logprob": -4.552668571472168}, {"token": "return", "logprob": -4.638606071472168}, {"token": "eos", "logprob": -4.638606071472168}, {"token": "n", "logprob": -4.677668571472168}, {"token": "**", "logprob": -4.724543571472168}, {"token": "sampling", "logprob": -4.974543571472168}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n  "}
{"step": 256, "token": "_top", "logprob": -2.754861354827881, "top_k": [{"token": "best", "logprob": -1.0439238548278809}, {"token": "_best", "logprob": -1.3720488548278809}, {"token": "_ctx", "logprob": -2.270486354827881}, {"token": "_top", "logprob": -2.754861354827881}, {"token": "_gpu", "logprob": -2.989236354827881}, {"token": "top", "logprob": -3.051736354827881}, {"token": "gram", "logprob": -3.059548854827881}, {"token": "=", "logprob": -3.707986354827881}, {"token": "_be", "logprob": -3.817361354827881}, {"token": "_", "logprob": -4.543923854827881}, {"token": "_t", "logprob": -4.903298854827881}, {"token": "_positions", "logprob": -5.028298854827881}, {"token": "_context", "logprob": -5.122048854827881}, {"token": "_k", "logprob": -5.137673854827881}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n"}
{"step": 257, "token": "_n", "logprob": -2.0572621822357178, "top_k": [{"token": "_words", "logprob": -0.822887122631073}, {"token": "_n", "logprob": -2.0572621822357178}, {"token": "_predictions", "logprob": -2.4556996822357178}, {"token": "=", "logprob": -2.5806996822357178}, {"token": "_", "logprob": -2.7681996822357178}, {"token": "_p", "logprob": -3.0025746822357178}, {"token": "_tokens", "logprob": -3.1588246822357178}, {"token": "_k", "logprob": -3.4947621822357178}, {"token": "_sentences", "logprob": -3.8385121822357178}, {"token": "_samples", "logprob": -4.033824443817139}, {"token": "_t", "logprob": -4.119761943817139}, {"token": "k", "logprob": -4.479136943817139}, {"token": "n", "logprob": -4.541636943817139}, {"token": "_e", "logprob": -4.736949443817139}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top"}
{"step": 258, "token": "Ġ=", "logprob": 0.0, "top_k": [{"token": "=", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n"}
{"step": 259, "token": "Ġ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n ="}
{"step": 260, "token": "20", "logprob": -2.385998249053955, "top_k": [{"token": "5", "logprob": -1.839123249053955}, {"token": "10", "logprob": -2.057873249053955}, {"token": "100", "logprob": -2.198498249053955}, {"token": "1", "logprob": -2.214123249053955}, {"token": "2", "logprob": -2.276623249053955}, {"token": "20", "logprob": -2.385998249053955}, {"token": "50", "logprob": -2.417248249053955}, {"token": "0", "logprob": -2.698498249053955}, {"token": "3", "logprob": -3.448498249053955}, {"token": "40", "logprob": -3.495373249053955}, {"token": "500", "logprob": -3.604748249053955}, {"token": "30", "logprob": -3.760998249053955}, {"token": "4", "logprob": -4.151623249053955}, {"token": "15", "logprob": -4.292248249053955}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = "}
{"step": 261, "token": "ĠĠĊ", "logprob": 0.0, "top_k": [{"token": "Ċ", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20"}
{"step": 262, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n"}
{"step": 263, "token": "Ġ#", "logprob": 0.0, "top_k": [{"token": "#", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n  "}
{"step": 264, "token": "ĠI", "logprob": 0.0, "top_k": [{"token": "I", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   #"}
{"step": 265, "token": "ci", "logprob": 0.0, "top_k": [{"token": "ci", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # I"}
{"step": 266, "token": ",", "logprob": 0.0, "top_k": [{"token": ",", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici"}
{"step": 267, "token": "Ġon", "logprob": 0.0, "top_k": [{"token": "on", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici,"}
{"step": 268, "token": "Ġutilise", "logprob": 0.0, "top_k": [{"token": "utilise", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on"}
{"step": 269, "token": "Ġle", "logprob": -0.07888974994421005, "top_k": [{"token": "le", "logprob": -0.07888974994421005}, {"token": "la", "logprob": -2.578889846801758}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise"}
{"step": 270, "token": "Ġnombre", "logprob": 0.0, "top_k": [{"token": "nombre", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le"}
{"step": 271, "token": "Ġde", "logprob": -0.12692804634571075, "top_k": [{"token": "de", "logprob": -0.12692804634571075}, {"token": "d", "logprob": -2.1269280910491943}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre"}
{"step": 272, "token": "Ġphrases", "logprob": -0.4481614828109741, "top_k": [{"token": "phrases", "logprob": -0.4481614828109741}, {"token": "rÃ©sultats", "logprob": -2.3856616020202637}, {"token": "top", "logprob": -2.8700366020202637}, {"token": "n", "logprob": -3.2059741020202637}, {"token": "meilleurs", "logprob": -3.4481616020202637}, {"token": "mots", "logprob": -3.4715991020202637}, {"token": "term", "logprob": -3.5575366020202637}, {"token": "propositions", "logprob": -3.5653491020202637}, {"token": "text", "logprob": -4.229411602020264}, {"token": "b", "logprob": -4.330974102020264}, {"token": "solutions", "logprob": -4.338786602020264}, {"token": "phrase", "logprob": -4.463786602020264}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de"}
{"step": 273, "token": "ĠÃł", "logprob": -0.16988778114318848, "top_k": [{"token": "Ãł", "logprob": -0.16988778114318848}, {"token": "les", "logprob": -2.2558252811431885}, {"token": "gÃ©n", "logprob": -3.4902002811431885}, {"token": "de", "logprob": -3.8652002811431885}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases"}
{"step": 274, "token": "ĠgÃ©n", "logprob": -0.9259394407272339, "top_k": [{"token": "gÃ©n", "logprob": -0.9259394407272339}, {"token": "consid", "logprob": -2.4884395599365234}, {"token": "utiliser", "logprob": -2.5040645599365234}, {"token": "aff", "logprob": -2.5900020599365234}, {"token": "prendre", "logprob": -2.7071895599365234}, {"token": "retour", "logprob": -2.7696895599365234}, {"token": "choisir", "logprob": -3.0821895599365234}, {"token": "sÃ©lection", "logprob": -3.2071895599365234}, {"token": "prÃ©s", "logprob": -3.6212520599365234}, {"token": "ret", "logprob": -3.7931270599365234}, {"token": "incl", "logprob": -3.8478145599365234}, {"token": "propos", "logprob": -3.8868770599365234}, {"token": "gard", "logprob": -3.9259395599365234}, {"token": "re", "logprob": -3.9650020599365234}, {"token": "ren", "logprob": -4.660314559936523}, {"token": "extra", "logprob": -4.707189559936523}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à"}
{"step": 275, "token": "Ã©rer", "logprob": 0.0, "top_k": [{"token": "Ã©rer", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à gén"}
{"step": 276, "token": "Ġ(", "logprob": -0.6286694407463074, "top_k": [{"token": "(", "logprob": -0.6286694407463074}, {"token": "pour", "logprob": -1.5036695003509521}, {"token": "Ãł", "logprob": -2.519294500350952}, {"token": "par", "logprob": -2.738044500350952}, {"token": "dans", "logprob": -3.066169500350952}, {"token": "en", "logprob": -3.425544500350952}, {"token": "au", "logprob": -3.909919500350952}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer"}
{"step": 277, "token": "20", "logprob": 0.0, "top_k": [{"token": "20", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer ("}
{"step": 278, "token": "Ġpar", "logprob": 0.0, "top_k": [{"token": "par", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20"}
{"step": 279, "token": "ĠdÃ©f", "logprob": 0.0, "top_k": [{"token": "dÃ©f", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par"}
{"step": 280, "token": "aut", "logprob": 0.0, "top_k": [{"token": "aut", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par déf"}
{"step": 281, "token": ")", "logprob": 0.0, "top_k": [{"token": ")", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut"}
{"step": 282, "token": "ĠĠĊ", "logprob": 0.0, "top_k": [{"token": "Ċ", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)"}
{"step": 283, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n"}
{"step": 284, "token": "Ġbatch", "logprob": -4.2336649894714355, "top_k": [{"token": "temperature", "logprob": -1.3742899894714355}, {"token": "seed", "logprob": -2.1321024894714355}, {"token": "n", "logprob": -2.2414774894714355}, {"token": "num", "logprob": -2.2805399894714355}, {"token": "max", "logprob": -2.9211649894714355}, {"token": "top", "logprob": -3.0383524894714355}, {"token": "text", "logprob": -3.3977274894714355}, {"token": "do", "logprob": -3.4211649894714355}, {"token": "#", "logprob": -3.8821024894714355}, {"token": "min", "logprob": -3.8821024894714355}, {"token": "sample", "logprob": -3.8977274894714355}, {"token": "length", "logprob": -3.9211649894714355}, {"token": "use", "logprob": -3.9211649894714355}, {"token": "random", "logprob": -4.1008524894714355}, {"token": "temp", "logprob": -4.2024149894714355}, {"token": "batch", "logprob": -4.2336649894714355}, {"token": "model", "logprob": -4.3117899894714355}, {"token": "output", "logprob": -4.4055399894714355}, {"token": "**", "logprob": -4.4055399894714355}, {"token": "no", "logprob": -4.4992899894714355}, {"token": "stop", "logprob": -4.5149149894714355}, {"token": "return", "logprob": -4.5852274894714355}, {"token": "prompt", "logprob": -4.7258524894714355}, {"token": "alpha", "logprob": -4.7258524894714355}, {"token": "eos", "logprob": -4.7649149894714355}, {"token": "context", "logprob": -4.7727274894714355}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n  "}
{"step": 285, "token": "_size", "logprob": 0.0, "top_k": [{"token": "_size", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch"}
{"step": 286, "token": "Ġ=", "logprob": 0.0, "top_k": [{"token": "=", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size"}
{"step": 287, "token": "Ġ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size ="}
{"step": 288, "token": "10", "logprob": -2.837773323059082, "top_k": [{"token": "1", "logprob": -0.9315232038497925}, {"token": "32", "logprob": -2.337773323059082}, {"token": "5", "logprob": -2.494023323059082}, {"token": "10", "logprob": -2.837773323059082}, {"token": "2", "logprob": -2.884648323059082}, {"token": "16", "logprob": -3.056523323059082}, {"token": "20", "logprob": -3.103398323059082}, {"token": "64", "logprob": -3.134648323059082}, {"token": "8", "logprob": -3.134648323059082}, {"token": "4", "logprob": -3.384648323059082}, {"token": "100", "logprob": -3.415898323059082}, {"token": "128", "logprob": -3.587773323059082}, {"token": "50", "logprob": -3.837773323059082}, {"token": "500", "logprob": -4.040898323059082}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = "}
{"step": 289, "token": "ĠĠĊ", "logprob": 0.0, "top_k": [{"token": "Ċ", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10"}
{"step": 290, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n"}
{"step": 291, "token": "Ġ#", "logprob": 0.0, "top_k": [{"token": "#", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n  "}
{"step": 292, "token": "ĠI", "logprob": 0.0, "top_k": [{"token": "I", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   #"}
{"step": 293, "token": "ci", "logprob": 0.0, "top_k": [{"token": "ci", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # I"}
{"step": 294, "token": ",", "logprob": 0.0, "top_k": [{"token": ",", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici"}
{"step": 295, "token": "Ġon", "logprob": 0.0, "top_k": [{"token": "on", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici,"}
{"step": 296, "token": "Ġutilise", "logprob": 0.0, "top_k": [{"token": "utilise", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on"}
{"step": 297, "token": "Ġle", "logprob": -0.1534215360879898, "top_k": [{"token": "le", "logprob": -0.1534215360879898}, {"token": "la", "logprob": -1.9502965211868286}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise"}
{"step": 298, "token": "Ġnombre", "logprob": 0.0, "top_k": [{"token": "nombre", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le"}
{"step": 299, "token": "Ġd", "logprob": -0.6253045797348022, "top_k": [{"token": "d", "logprob": -0.6253045797348022}, {"token": "de", "logprob": -0.7659295797348022}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre"}
{"step": 300, "token": "'Ã©", "logprob": -0.21744267642498016, "top_k": [{"token": "'Ã©", "logprob": -0.21744267642498016}, {"token": "'", "logprob": -2.279942750930786}, {"token": "'ex", "logprob": -2.373692750930786}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d"}
{"step": 301, "token": "chant", "logprob": 0.0, "top_k": [{"token": "chant", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'é"}
{"step": 302, "token": "ill", "logprob": 0.0, "top_k": [{"token": "ill", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échant"}
{"step": 303, "token": "ons", "logprob": 0.0, "top_k": [{"token": "ons", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantill"}
{"step": 304, "token": "Ġpar", "logprob": -0.8219353556632996, "top_k": [{"token": "Ãł", "logprob": -0.6969353556632996}, {"token": "par", "logprob": -0.8219353556632996}, {"token": "de", "logprob": -3.2125604152679443}, {"token": "dans", "logprob": -3.8141229152679443}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons"}
{"step": 305, "token": "ĠgÃ©nÃ©", "logprob": -1.9012503623962402, "top_k": [{"token": "lot", "logprob": -0.963750422000885}, {"token": "batch", "logprob": -1.4950003623962402}, {"token": "gÃ©nÃ©", "logprob": -1.9012503623962402}, {"token": "b", "logprob": -3.1746878623962402}, {"token": "it", "logprob": -3.3387503623962402}, {"token": "lots", "logprob": -3.3700003623962402}, {"token": "groupe", "logprob": -3.4950003623962402}, {"token": "bou", "logprob": -3.9481253623962402}, {"token": "rapport", "logprob": -3.9637503623962402}, {"token": "tran", "logprob": -4.00281286239624}, {"token": "Ã©", "logprob": -4.10437536239624}, {"token": "exemple", "logprob": -4.16687536239624}, {"token": "fois", "logprob": -4.24500036239624}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par"}
{"step": 306, "token": "ration", "logprob": 0.0, "top_k": [{"token": "ration", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par géné"}
{"step": 307, "token": "Ġ(", "logprob": -0.1556587964296341, "top_k": [{"token": "(", "logprob": -0.1556587964296341}, {"token": "de", "logprob": -1.9369088411331177}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération"}
{"step": 308, "token": "10", "logprob": 0.0, "top_k": [{"token": "10", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération ("}
{"step": 309, "token": "Ġpar", "logprob": 0.0, "top_k": [{"token": "par", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10"}
{"step": 310, "token": "ĠdÃ©f", "logprob": 0.0, "top_k": [{"token": "dÃ©f", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par"}
{"step": 311, "token": "aut", "logprob": 0.0, "top_k": [{"token": "aut", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par déf"}
{"step": 312, "token": ")", "logprob": 0.0, "top_k": [{"token": ")", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut"}
{"step": 313, "token": "ĠĠĊ", "logprob": 0.0, "top_k": [{"token": "Ċ", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)"}
{"step": 314, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n"}
{"step": 315, "token": "Ġtemperature", "logprob": -1.5745038986206055, "top_k": [{"token": "seed", "logprob": -1.5354413986206055}, {"token": "temperature", "logprob": -1.5745038986206055}, {"token": "num", "logprob": -2.1916913986206055}, {"token": "max", "logprob": -2.5432538986206055}, {"token": "text", "logprob": -3.2307538986206055}, {"token": "do", "logprob": -3.2932538986206055}, {"token": "n", "logprob": -3.4182538986206055}, {"token": "use", "logprob": -3.5823163986206055}, {"token": "top", "logprob": -3.5979413986206055}, {"token": "model", "logprob": -4.0041913986206055}, {"token": "sample", "logprob": -4.0588788986206055}, {"token": "#", "logprob": -4.1526288986206055}, {"token": "random", "logprob": -4.1995038986206055}, {"token": "length", "logprob": -4.2620038986206055}, {"token": "temp", "logprob": -4.2698163986206055}, {"token": "output", "logprob": -4.3479413986206055}, {"token": "min", "logprob": -4.4260663986206055}, {"token": "return", "logprob": -4.4885663986206055}, {"token": "device", "logprob": -4.4963788986206055}, {"token": "prompt", "logprob": -4.5120038986206055}, {"token": "context", "logprob": -4.5198163986206055}, {"token": "**", "logprob": -4.5510663986206055}, {"token": "shuffle", "logprob": -4.7151288986206055}, {"token": "tokenizer", "logprob": -4.7229413986206055}, {"token": "warm", "logprob": -4.7854413986206055}, {"token": "stop", "logprob": -4.8557538986206055}, {"token": "no", "logprob": -4.8948163986206055}, {"token": "sampling", "logprob": -4.9495038986206055}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n  "}
{"step": 316, "token": "Ġ=", "logprob": 0.0, "top_k": [{"token": "=", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature"}
{"step": 317, "token": "Ġ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature ="}
{"step": 318, "token": "0", "logprob": -0.46239328384399414, "top_k": [{"token": "0", "logprob": -0.46239328384399414}, {"token": "1", "logprob": -0.9936432838439941}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = "}
{"step": 319, "token": ".", "logprob": 0.0, "top_k": [{"token": ".", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0"}
{"step": 320, "token": "3", "logprob": -3.5331482887268066, "top_k": [{"token": "7", "logprob": -1.2675232887268066}, {"token": "5", "logprob": -1.4550232887268066}, {"token": "8", "logprob": -2.1112732887268066}, {"token": "9", "logprob": -2.2206482887268066}, {"token": "6", "logprob": -2.4862732887268066}, {"token": "0", "logprob": -3.0487732887268066}, {"token": "75", "logprob": -3.3300232887268066}, {"token": "4", "logprob": -3.5018982887268066}, {"token": "2", "logprob": -3.5175232887268066}, {"token": "3", "logprob": -3.5331482887268066}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0."}
{"step": 321, "token": "ĠĠĊ", "logprob": 0.0, "top_k": [{"token": "Ċ", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3"}
{"step": 322, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n"}
{"step": 323, "token": "Ġ#", "logprob": 0.0, "top_k": [{"token": "#", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n  "}
{"step": 324, "token": "ĠI", "logprob": 0.0, "top_k": [{"token": "I", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   #"}
{"step": 325, "token": "ci", "logprob": 0.0, "top_k": [{"token": "ci", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # I"}
{"step": 326, "token": ",", "logprob": 0.0, "top_k": [{"token": ",", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici"}
{"step": 327, "token": "Ġon", "logprob": 0.0, "top_k": [{"token": "on", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici,"}
{"step": 328, "token": "Ġutilise", "logprob": 0.0, "top_k": [{"token": "utilise", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on"}
{"step": 329, "token": "Ġla", "logprob": 0.0, "top_k": [{"token": "la", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise"}
{"step": 330, "token": "Ġtemp", "logprob": 0.0, "top_k": [{"token": "temp", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la"}
{"step": 331, "token": "Ã©r", "logprob": 0.0, "top_k": [{"token": "Ã©r", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la temp"}
{"step": 332, "token": "ature", "logprob": 0.0, "top_k": [{"token": "ature", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la tempér"}
{"step": 333, "token": "Ġ(", "logprob": -1.7484736442565918, "top_k": [{"token": "de", "logprob": -0.482848584651947}, {"token": "(", "logprob": -1.7484736442565918}, {"token": "pour", "logprob": -2.279723644256592}, {"token": "du", "logprob": -2.842223644256592}, {"token": "des", "logprob": -3.029723644256592}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température"}
{"step": 334, "token": "0", "logprob": -0.04076450690627098, "top_k": [{"token": "0", "logprob": -0.04076450690627098}, {"token": "par", "logprob": -4.072014331817627}, {"token": "3", "logprob": -4.431389331817627}, {"token": "un", "logprob": -4.509514331817627}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température ("}
{"step": 335, "token": ".", "logprob": 0.0, "top_k": [{"token": ".", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0"}
{"step": 336, "token": "3", "logprob": 0.0, "top_k": [{"token": "3", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0."}
{"step": 337, "token": "Ġpar", "logprob": 0.0, "top_k": [{"token": "par", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3"}
{"step": 338, "token": "ĠdÃ©f", "logprob": 0.0, "top_k": [{"token": "dÃ©f", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par"}
{"step": 339, "token": "aut", "logprob": 0.0, "top_k": [{"token": "aut", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par déf"}
{"step": 340, "token": ")", "logprob": 0.0, "top_k": [{"token": ")", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut"}
{"step": 341, "token": "ĠĠĊ", "logprob": -0.06993623077869415, "top_k": [{"token": "Ċ", "logprob": -0.06993623077869415}, {"token": "pour", "logprob": -2.6949362754821777}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)"}
{"step": 342, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n"}
{"step": 343, "token": "Ġnum", "logprob": -3.29909086227417, "top_k": [{"token": "top", "logprob": -0.9319033622741699}, {"token": "repetition", "logprob": -2.31471586227417}, {"token": "seed", "logprob": -2.61159086227417}, {"token": "num", "logprob": -3.29909086227417}, {"token": "model", "logprob": -3.32252836227417}, {"token": "stop", "logprob": -3.60377836227417}, {"token": "use", "logprob": -3.62721586227417}, {"token": "text", "logprob": -3.65846586227417}, {"token": "do", "logprob": -3.72877836227417}, {"token": "length", "logprob": -3.79127836227417}, {"token": "sample", "logprob": -3.90846586227417}, {"token": "context", "logprob": -3.93971586227417}, {"token": "bos", "logprob": -3.96315336227417}, {"token": "no", "logprob": -3.98659086227417}, {"token": "random", "logprob": -4.00221586227417}, {"token": "max", "logprob": -4.01002836227417}, {"token": "repeat", "logprob": -4.18190336227417}, {"token": "alpha", "logprob": -4.26002836227417}, {"token": "sampling", "logprob": -4.26784086227417}, {"token": "device", "logprob": -4.29127836227417}, {"token": "prefix", "logprob": -4.30690336227417}, {"token": "min", "logprob": -4.31471586227417}, {"token": "prompt", "logprob": -4.66627836227417}, {"token": "dropout", "logprob": -4.89284086227417}, {"token": "skip", "logprob": -4.96315336227417}, {"token": "return", "logprob": -4.97877836227417}, {"token": "#", "logprob": -5.01784086227417}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n  "}
{"step": 344, "token": "_return", "logprob": -0.9521639943122864, "top_k": [{"token": "_w", "logprob": -0.7099764943122864}, {"token": "_return", "logprob": -0.9521639943122864}, {"token": "_workers", "logprob": -3.3271639347076416}, {"token": "_train", "logprob": -4.053726673126221}, {"token": "_re", "logprob": -4.194351673126221}, {"token": "_epochs", "logprob": -4.506851673126221}, {"token": "_steps", "logprob": -4.819351673126221}, {"token": "_g", "logprob": -4.827164173126221}, {"token": "_layers", "logprob": -4.967789173126221}, {"token": "_neg", "logprob": -4.983414173126221}, {"token": "_training", "logprob": -4.991226673126221}, {"token": "_un", "logprob": -5.053726673126221}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num"}
{"step": 345, "token": "_sequences", "logprob": 0.0, "top_k": [{"token": "_sequences", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return"}
{"step": 346, "token": "Ġ=", "logprob": 0.0, "top_k": [{"token": "=", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences"}
{"step": 347, "token": "Ġ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences ="}
{"step": 348, "token": "5", "logprob": -1.7015225887298584, "top_k": [{"token": "1", "logprob": -0.5140225887298584}, {"token": "5", "logprob": -1.7015225887298584}, {"token": "10", "logprob": -2.5608975887298584}, {"token": "3", "logprob": -2.5608975887298584}, {"token": "2", "logprob": -2.7327725887298584}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = "}
{"step": 349, "token": "ĠĠĊ", "logprob": 0.0, "top_k": [{"token": "Ċ", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5"}
{"step": 350, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n"}
{"step": 351, "token": "Ġ#", "logprob": 0.0, "top_k": [{"token": "#", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n  "}
{"step": 352, "token": "ĠI", "logprob": 0.0, "top_k": [{"token": "I", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   #"}
{"step": 353, "token": "ci", "logprob": 0.0, "top_k": [{"token": "ci", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # I"}
{"step": 354, "token": ",", "logprob": 0.0, "top_k": [{"token": ",", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici"}
{"step": 355, "token": "Ġon", "logprob": 0.0, "top_k": [{"token": "on", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici,"}
{"step": 356, "token": "Ġutilise", "logprob": 0.0, "top_k": [{"token": "utilise", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on"}
{"step": 357, "token": "Ġle", "logprob": 0.0, "top_k": [{"token": "le", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise"}
{"step": 358, "token": "Ġnombre", "logprob": 0.0, "top_k": [{"token": "nombre", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le"}
{"step": 359, "token": "Ġde", "logprob": 0.0, "top_k": [{"token": "de", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre"}
{"step": 360, "token": "Ġphrases", "logprob": -1.1228034496307373, "top_k": [{"token": "sÃ©", "logprob": -0.5915535092353821}, {"token": "phrases", "logprob": -1.1228034496307373}, {"token": "rÃ©", "logprob": -2.6696784496307373}, {"token": "ret", "logprob": -3.4978034496307373}, {"token": "fois", "logprob": -3.8337409496307373}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de"}
{"step": 361, "token": "ĠÃł", "logprob": -0.2770546078681946, "top_k": [{"token": "Ãł", "logprob": -0.2770546078681946}, {"token": "ret", "logprob": -1.8864295482635498}, {"token": "de", "logprob": -3.35517954826355}, {"token": "ren", "logprob": -3.55049204826355}, {"token": "gÃ©n", "logprob": -3.62080454826355}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases"}
{"step": 362, "token": "ĠgÃ©n", "logprob": -0.8325440287590027, "top_k": [{"token": "gÃ©n", "logprob": -0.8325440287590027}, {"token": "retour", "logprob": -0.8481690287590027}, {"token": "ren", "logprob": -1.9887940883636475}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à"}
{"step": 363, "token": "Ã©rer", "logprob": 0.0, "top_k": [{"token": "Ã©rer", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à gén"}
{"step": 364, "token": "Ġpour", "logprob": -2.0608112812042236, "top_k": [{"token": "(", "logprob": -0.27956119179725647}, {"token": "pour", "logprob": -2.0608112812042236}, {"token": "par", "logprob": -2.4670612812042236}, {"token": "Ãł", "logprob": -3.4514362812042236}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer"}
{"step": 365, "token": "Ġchaque", "logprob": -0.15846320986747742, "top_k": [{"token": "chaque", "logprob": -0.15846320986747742}, {"token": "chac", "logprob": -3.0178382396698}, {"token": "la", "logprob": -3.0959632396698}, {"token": "une", "logprob": -3.5178382396698}, {"token": "le", "logprob": -3.7834632396698}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour"}
{"step": 366, "token": "ĠÃ©", "logprob": -0.5754003524780273, "top_k": [{"token": "Ã©", "logprob": -0.5754003524780273}, {"token": "gÃ©nÃ©", "logprob": -1.6769628524780273}, {"token": "phrase", "logprob": -1.8879003524780273}, {"token": "entr", "logprob": -3.2785253524780273}, {"token": "modÃ¨le", "logprob": -3.8566503524780273}, {"token": "exemple", "logprob": -3.9894628524780273}, {"token": "demande", "logprob": -4.395712852478027}, {"token": "texte", "logprob": -4.653525352478027}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque"}
{"step": 367, "token": "chant", "logprob": 0.0, "top_k": [{"token": "chant", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque é"}
{"step": 368, "token": "illon", "logprob": 0.0, "top_k": [{"token": "illon", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échant"}
{"step": 369, "token": "Ġ(", "logprob": 0.0, "top_k": [{"token": "(", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon"}
{"step": 370, "token": "5", "logprob": 0.0, "top_k": [{"token": "5", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon ("}
{"step": 371, "token": "Ġpar", "logprob": 0.0, "top_k": [{"token": "par", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5"}
{"step": 372, "token": "ĠdÃ©f", "logprob": 0.0, "top_k": [{"token": "dÃ©f", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par"}
{"step": 373, "token": "aut", "logprob": 0.0, "top_k": [{"token": "aut", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par déf"}
{"step": 374, "token": ")", "logprob": 0.0, "top_k": [{"token": ")", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut"}
{"step": 375, "token": "ĠĠĊ", "logprob": 0.0, "top_k": [{"token": "Ċ", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)"}
{"step": 376, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n"}
{"step": 377, "token": "Ġmodel", "logprob": -3.417121171951294, "top_k": [{"token": "seed", "logprob": -1.565558671951294}, {"token": "num", "logprob": -1.963996171951294}, {"token": "text", "logprob": -2.495246171951294}, {"token": "do", "logprob": -3.120246171951294}, {"token": "top", "logprob": -3.135871171951294}, {"token": "use", "logprob": -3.167121171951294}, {"token": "prompt", "logprob": -3.299933671951294}, {"token": "max", "logprob": -3.385871171951294}, {"token": "model", "logprob": -3.417121171951294}, {"token": "return", "logprob": -3.542121171951294}, {"token": "#", "logprob": -3.573371171951294}, {"token": "stop", "logprob": -3.581183671951294}, {"token": "**", "logprob": -3.768683671951294}, {"token": "sample", "logprob": -3.948371171951294}, {"token": "```", "logprob": -3.971808671951294}, {"token": "output", "logprob": -3.979621171951294}, {"token": "context", "logprob": -4.073370933532715}, {"token": "save", "logprob": -4.182745933532715}, {"token": "length", "logprob": -4.182745933532715}, {"token": "repetition", "logprob": -4.315558433532715}, {"token": "random", "logprob": -4.338995933532715}, {"token": "n", "logprob": -4.346808433532715}, {"token": "prefix", "logprob": -4.370245933532715}, {"token": "``", "logprob": -4.510870933532715}, {"token": "tokenizer", "logprob": -4.549933433532715}, {"token": "device", "logprob": -4.565558433532715}, {"token": "repeat", "logprob": -4.573370933532715}, {"token": "##", "logprob": -4.604620933532715}, {"token": "bos", "logprob": -4.628058433532715}, {"token": "no", "logprob": -4.753058433532715}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n  "}
{"step": 378, "token": "_name", "logprob": -0.8678543567657471, "top_k": [{"token": "_name", "logprob": -0.8678543567657471}, {"token": "=", "logprob": -1.938166856765747}, {"token": "_path", "logprob": -2.055354356765747}, {"token": "_type", "logprob": -2.235041856765747}, {"token": "_dir", "logprob": -2.649104356765747}, {"token": "_id", "logprob": -3.625666856765747}, {"token": "_prefix", "logprob": -3.719416856765747}, {"token": "_save", "logprob": -3.735041856765747}, {"token": "_kwargs", "logprob": -4.008479118347168}, {"token": "_checkpoint", "logprob": -4.516291618347168}, {"token": "_config", "logprob": -4.617854118347168}, {"token": "_input", "logprob": -4.688166618347168}, {"token": "_output", "logprob": -4.891291618347168}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model"}
{"step": 379, "token": "_or", "logprob": -1.4179681539535522, "top_k": [{"token": "=", "logprob": -0.27734318375587463}, {"token": "_or", "logprob": -1.4179681539535522}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name"}
{"step": 380, "token": "_path", "logprob": 0.0, "top_k": [{"token": "_path", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or"}
{"step": 381, "token": "Ġ=", "logprob": 0.0, "top_k": [{"token": "=", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path"}
{"step": 382, "token": "Ġ\"", "logprob": -0.10019482672214508, "top_k": [{"token": "\"", "logprob": -0.10019482672214508}, {"token": "\"./", "logprob": -2.9126949310302734}, {"token": "\"/", "logprob": -3.1939449310302734}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path ="}
{"step": 383, "token": "bert", "logprob": -3.105520009994507, "top_k": [{"token": "XL", "logprob": -2.128957509994507}, {"token": "h", "logprob": -2.261770009994507}, {"token": "facebook", "logprob": -2.300832509994507}, {"token": "google", "logprob": -2.363332509994507}, {"token": "g", "logprob": -2.816457509994507}, {"token": "m", "logprob": -2.863332509994507}, {"token": "xl", "logprob": -2.863332509994507}, {"token": "H", "logprob": -2.871145009994507}, {"token": "bert", "logprob": -3.105520009994507}, {"token": "dist", "logprob": -3.308645009994507}, {"token": "x", "logprob": -3.324270009994507}, {"token": "hf", "logprob": -3.496145009994507}, {"token": "text", "logprob": -3.621145009994507}, {"token": "t", "logprob": -3.808645009994507}, {"token": "models", "logprob": -4.011770248413086}, {"token": "https", "logprob": -4.058645248413086}, {"token": "nl", "logprob": -4.105520248413086}, {"token": "open", "logprob": -4.535207748413086}, {"token": "vin", "logprob": -4.613332748413086}, {"token": "path", "logprob": -4.660207748413086}, {"token": "deep", "logprob": -4.683645248413086}, {"token": "G", "logprob": -4.738332748413086}, {"token": "db", "logprob": -4.753957748413086}, {"token": "big", "logprob": -4.800832748413086}, {"token": "Google", "logprob": -4.824270248413086}, {"token": "model", "logprob": -4.871145248413086}, {"token": "D", "logprob": -4.871145248413086}, {"token": "X", "logprob": -4.878957748413086}, {"token": "par", "logprob": -4.878957748413086}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \""}
{"step": 384, "token": "-base", "logprob": -0.42328080534935, "top_k": [{"token": "-base", "logprob": -0.42328080534935}, {"token": "-large", "logprob": -1.0639058351516724}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert"}
{"step": 385, "token": "-", "logprob": -0.49997538328170776, "top_k": [{"token": "-", "logprob": -0.49997538328170776}, {"token": "-mult", "logprob": -1.5156004428863525}, {"token": "-c", "logprob": -1.7499754428863525}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base"}
{"step": 386, "token": "unc", "logprob": 0.0, "top_k": [{"token": "unc", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-"}
{"step": 387, "token": "ased", "logprob": 0.0, "top_k": [{"token": "ased", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-unc"}
{"step": 388, "token": "\"", "logprob": 0.0, "top_k": [{"token": "\"", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased"}
{"step": 389, "token": "ĠĠĊ", "logprob": 0.0, "top_k": [{"token": "Ċ", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\""}
{"step": 390, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n"}
{"step": 391, "token": "Ġ#", "logprob": 0.0, "top_k": [{"token": "#", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n  "}
{"step": 392, "token": "ĠI", "logprob": 0.0, "top_k": [{"token": "I", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   #"}
{"step": 393, "token": "ci", "logprob": 0.0, "top_k": [{"token": "ci", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # I"}
{"step": 394, "token": ",", "logprob": 0.0, "top_k": [{"token": ",", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici"}
{"step": 395, "token": "Ġon", "logprob": 0.0, "top_k": [{"token": "on", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici,"}
{"step": 396, "token": "Ġutilise", "logprob": 0.0, "top_k": [{"token": "utilise", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on"}
{"step": 397, "token": "Ġle", "logprob": 0.0, "top_k": [{"token": "le", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise"}
{"step": 398, "token": "ĠmodÃ¨le", "logprob": -1.2792108058929443, "top_k": [{"token": "nom", "logprob": -0.3260858356952667}, {"token": "modÃ¨le", "logprob": -1.2792108058929443}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le"}
{"step": 399, "token": "ĠBert", "logprob": -3.368556261062622, "top_k": [{"token": "de", "logprob": -0.30605632066726685}, {"token": "d", "logprob": -2.462306261062622}, {"token": "Ãł", "logprob": -2.970118761062622}, {"token": "(", "logprob": -3.290431261062622}, {"token": "Bert", "logprob": -3.368556261062622}, {"token": "utilis", "logprob": -3.399806261062622}, {"token": "B", "logprob": -3.813868761062622}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle"}
{"step": 400, "token": "-", "logprob": -2.564006805419922, "top_k": [{"token": "pour", "logprob": -1.4233819246292114}, {"token": "(", "logprob": -1.7593194246292114}, {"token": "-base", "logprob": -2.228069305419922}, {"token": "de", "logprob": -2.540569305419922}, {"token": "base", "logprob": -2.564006805419922}, {"token": "-", "logprob": -2.564006805419922}, {"token": "bas", "logprob": -2.954631805419922}, {"token": "Base", "logprob": -3.009319305419922}, {"token": "utilis", "logprob": -3.173381805419922}, {"token": "en", "logprob": -3.743694305419922}, {"token": "Ãł", "logprob": -3.954631805419922}, {"token": ",", "logprob": -3.970256805419922}, {"token": "Base", "logprob": -4.017131805419922}, {"token": "par", "logprob": -4.314006805419922}, {"token": "avec", "logprob": -4.603069305419922}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert"}
{"step": 401, "token": "Base", "logprob": 0.0, "top_k": [{"token": "Base", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-"}
{"step": 402, "token": "Ġ(", "logprob": -2.8085718154907227, "top_k": [{"token": "-", "logprob": -0.8476344347000122}, {"token": "-Un", "logprob": -0.9570094347000122}, {"token": "-U", "logprob": -2.5976343154907227}, {"token": "(", "logprob": -2.8085718154907227}, {"token": "pour", "logprob": -2.9413843154907227}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base"}
{"step": 403, "token": "en", "logprob": -3.770793914794922, "top_k": [{"token": "bert", "logprob": -0.4817313551902771}, {"token": "cas", "logprob": -2.606731414794922}, {"token": "par", "logprob": -2.848918914794922}, {"token": "B", "logprob": -3.239543914794922}, {"token": "BERT", "logprob": -3.286418914794922}, {"token": "unc", "logprob": -3.536418914794922}, {"token": "en", "logprob": -3.770793914794922}, {"token": "base", "logprob": -3.841106414794922}, {"token": "ou", "logprob": -4.005168914794922}, {"token": "https", "logprob": -4.137981414794922}, {"token": "avec", "logprob": -4.372356414794922}, {"token": "b", "logprob": -4.583293914794922}, {"token": "pour", "logprob": -4.747356414794922}, {"token": "on", "logprob": -4.817668914794922}, {"token": "un", "logprob": -4.934856414794922}, {"token": "Unc", "logprob": -5.012981414794922}, {"token": "Ãł", "logprob": -5.044231414794922}, {"token": "0", "logprob": -5.059856414794922}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base ("}
{"step": 404, "token": "Ġutilis", "logprob": -3.196579694747925, "top_k": [{"token": "cas", "logprob": -0.92314213514328}, {"token": "anglais", "logprob": -1.1340796947479248}, {"token": "franÃ§ais", "logprob": -2.516892194747925}, {"token": "cours", "logprob": -2.712204694747925}, {"token": "utilis", "logprob": -3.196579694747925}, {"token": "ca", "logprob": -3.688767194747925}, {"token": "bas", "logprob": -3.790329694747925}, {"token": "mode", "logprob": -4.782516956329346}, {"token": "version", "logprob": -4.805954456329346}, {"token": "fonction", "logprob": -4.891891956329346}, {"token": "raison", "logprob": -4.891891956329346}, {"token": "langue", "logprob": -4.923141956329346}, {"token": "-c", "logprob": -5.024704456329346}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en"}
{"step": 405, "token": "ant", "logprob": 0.0, "top_k": [{"token": "ant", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilis"}
{"step": 406, "token": "Ġun", "logprob": -2.2298989295959473, "top_k": [{"token": "le", "logprob": -1.5033363103866577}, {"token": "la", "logprob": -1.5502113103866577}, {"token": "l", "logprob": -1.7220863103866577}, {"token": "un", "logprob": -2.2298989295959473}, {"token": "les", "logprob": -2.3939614295959473}, {"token": "une", "logprob": -2.4720864295959473}, {"token": "Bert", "logprob": -2.7533364295959473}, {"token": "\"", "logprob": -3.9095864295959473}, {"token": "des", "logprob": -3.9252114295959473}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant"}
{"step": 407, "token": "Ġenc", "logprob": -4.330978870391846, "top_k": [{"token": "cas", "logprob": -1.0028538703918457}, {"token": "modÃ¨le", "logprob": -1.1512913703918457}, {"token": "acc", "logprob": -3.0419163703918457}, {"token": "corpus", "logprob": -3.4637913703918457}, {"token": "lang", "logprob": -3.5966038703918457}, {"token": "h", "logprob": -3.7059788703918457}, {"token": "fichier", "logprob": -3.7450413703918457}, {"token": "ensemble", "logprob": -3.7606663703918457}, {"token": "c", "logprob": -4.198166370391846}, {"token": "texte", "logprob": -4.198166370391846}, {"token": "cache", "logprob": -4.205978870391846}, {"token": "chemin", "logprob": -4.330978870391846}, {"token": "enc", "logprob": -4.330978870391846}, {"token": "nom", "logprob": -4.354416370391846}, {"token": "jeu", "logprob": -4.393478870391846}, {"token": "Ã©", "logprob": -4.409103870391846}, {"token": "cadre", "logprob": -4.487228870391846}, {"token": "en", "logprob": -4.627853870391846}, {"token": "code", "logprob": -4.674728870391846}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un"}
{"step": 408, "token": "od", "logprob": 0.0, "top_k": [{"token": "od", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un enc"}
{"step": 409, "token": "age", "logprob": 0.0, "top_k": [{"token": "age", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encod"}
{"step": 410, "token": "Ġde", "logprob": -1.1619200706481934, "top_k": [{"token": "de", "logprob": -1.1619200706481934}, {"token": "d", "logprob": -1.9666075706481934}, {"token": "Unicode", "logprob": -2.3884825706481934}, {"token": "en", "logprob": -2.9666075706481934}, {"token": "un", "logprob": -3.0056700706481934}, {"token": "b", "logprob": -3.0291075706481934}, {"token": "B", "logprob": -3.1384825706481934}, {"token": "cas", "logprob": -3.3337950706481934}, {"token": "unicode", "logprob": -3.5134825706481934}, {"token": "UTF", "logprob": -3.8494200706481934}, {"token": "c", "logprob": -3.9587950706481934}, {"token": "du", "logprob": -4.115045070648193}, {"token": "al", "logprob": -4.154107570648193}, {"token": "non", "logprob": -4.216607570648193}, {"token": "des", "logprob": -4.365045070648193}, {"token": "sans", "logprob": -4.372857570648193}, {"token": "bas", "logprob": -4.591607570648193}, {"token": "unic", "logprob": -4.599420070648193}, {"token": "Ãł", "logprob": -4.638482570648193}, {"token": "U", "logprob": -4.716607570648193}, {"token": "par", "logprob": -4.771295070648193}, {"token": "sur", "logprob": -4.779107570648193}, {"token": "simple", "logprob": -4.872857570648193}, {"token": "\"", "logprob": -4.911920070648193}, {"token": "anglais", "logprob": -4.943170070648193}, {"token": "mult", "logprob": -4.966607570648193}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage"}
{"step": 411, "token": "Ġcaract", "logprob": -2.9553966522216797, "top_k": [{"token": "", "logprob": -1.3694592714309692}, {"token": "lang", "logprob": -1.9397717714309692}, {"token": "base", "logprob": -2.0803966522216797}, {"token": "langue", "logprob": -2.0882091522216797}, {"token": "cas", "logprob": -2.4163341522216797}, {"token": "caract", "logprob": -2.9553966522216797}, {"token": "texte", "logprob": -3.2132091522216797}, {"token": "type", "logprob": -3.2210216522216797}, {"token": "cha", "logprob": -3.4866466522216797}, {"token": "la", "logprob": -3.7210216522216797}, {"token": "code", "logprob": -4.08039665222168}, {"token": "prÃ©", "logprob": -4.42414665222168}, {"token": "token", "logprob": -4.54133415222168}, {"token": "vocab", "logprob": -4.56477165222168}, {"token": "long", "logprob": -4.66633415222168}, {"token": "taille", "logprob": -4.72102165222168}, {"token": "chiff", "logprob": -4.74445915222168}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de"}
{"step": 412, "token": "Ã¨re", "logprob": -1.2124420404434204, "top_k": [{"token": "Ã¨res", "logprob": -0.3530670702457428}, {"token": "Ã¨re", "logprob": -1.2124420404434204}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caract"}
{"step": 413, "token": "Ġnon", "logprob": -2.9590446949005127, "top_k": [{"token": "Unicode", "logprob": -1.5293571949005127}, {"token": "unicode", "logprob": -2.0527946949005127}, {"token": "UTF", "logprob": -2.7637321949005127}, {"token": "en", "logprob": -2.7871696949005127}, {"token": ")", "logprob": -2.9199821949005127}, {"token": "de", "logprob": -2.9590446949005127}, {"token": "non", "logprob": -2.9590446949005127}, {"token": "ASCII", "logprob": -3.0996696949005127}, {"token": "un", "logprob": -3.2090446949005127}, {"token": "sans", "logprob": -3.2402946949005127}, {"token": "c", "logprob": -3.3887321949005127}, {"token": "unic", "logprob": -3.5684196949005127}, {"token": "al", "logprob": -3.9121696949005127}, {"token": "utf", "logprob": -3.9981071949005127}, {"token": "\"", "logprob": -4.013731956481934}, {"token": "Ãł", "logprob": -4.130919456481934}, {"token": ",", "logprob": -4.154356956481934}, {"token": "d", "logprob": -4.287169456481934}, {"token": "", "logprob": -4.302794456481934}, {"token": "B", "logprob": -4.380919456481934}, {"token": "cas", "logprob": -4.466856956481934}, {"token": "dans", "logprob": -4.490294456481934}, {"token": "b", "logprob": -4.552794456481934}, {"token": "(", "logprob": -4.724669456481934}, {"token": "univers", "logprob": -4.787169456481934}, {"token": "pour", "logprob": -4.873106956481934}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère"}
{"step": 414, "token": "Ġaccent", "logprob": -2.692274570465088, "top_k": [{"token": "-c", "logprob": -1.5828994512557983}, {"token": "cas", "logprob": -2.012587070465088}, {"token": "cod", "logprob": -2.411024570465088}, {"token": "-", "logprob": -2.629774570465088}, {"token": "accent", "logprob": -2.692274570465088}, {"token": "enc", "logprob": -2.965712070465088}, {"token": "-mar", "logprob": -3.309462070465088}, {"token": "mar", "logprob": -3.356337070465088}, {"token": "-com", "logprob": -3.512587070465088}, {"token": "al", "logprob": -3.551649570465088}, {"token": "c", "logprob": -3.559462070465088}, {"token": "mod", "logprob": -3.715712070465088}, {"token": "dÃ©", "logprob": -3.731337070465088}, {"token": "Unicode", "logprob": -3.926649570465088}, {"token": "ASCII", "logprob": -4.200087070465088}, {"token": "unicode", "logprob": -4.207899570465088}, {"token": "case", "logprob": -4.231337070465088}, {"token": "cass", "logprob": -4.356337070465088}, {"token": "limit", "logprob": -4.387587070465088}, {"token": "-ca", "logprob": -4.395399570465088}, {"token": "b", "logprob": -4.411024570465088}, {"token": "standard", "logprob": -4.606337070465088}, {"token": "compress", "logprob": -4.676649570465088}, {"token": "spÃ©c", "logprob": -4.770399570465088}, {"token": "-ac", "logprob": -4.778212070465088}, {"token": "sens", "logprob": -4.817274570465088}, {"token": "-al", "logprob": -4.871962070465088}, {"token": "ca", "logprob": -4.895399570465088}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non"}
{"step": 415, "token": "uÃ©", "logprob": 0.0, "top_k": [{"token": "uÃ©", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accent"}
{"step": 416, "token": ")", "logprob": -0.29791513085365295, "top_k": [{"token": ")", "logprob": -0.29791513085365295}, {"token": ",", "logprob": -2.67291522026062}, {"token": ").", "logprob": -2.93854022026062}, {"token": "pour", "logprob": -3.11041522026062}, {"token": "(", "logprob": -3.16510272026062}, {"token": "et", "logprob": -3.38385272026062}, {"token": "en", "logprob": -4.204164981842041}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué"}
{"step": 417, "token": "Ġpour", "logprob": -0.7605419158935547, "top_k": [{"token": "Ċ", "logprob": -0.7136669158935547}, {"token": "pour", "logprob": -0.7605419158935547}, {"token": "Ãł", "logprob": -3.8074169158935547}, {"token": "(", "logprob": -3.8855419158935547}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué)"}
{"step": 418, "token": "ĠcrÃ©er", "logprob": -1.8653138875961304, "top_k": [{"token": "gÃ©n", "logprob": -0.7246888875961304}, {"token": "la", "logprob": -1.1309388875961304}, {"token": "crÃ©er", "logprob": -1.8653138875961304}, {"token": "les", "logprob": -3.27156400680542}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour"}
{"step": 419, "token": "Ġdes", "logprob": -0.19302435219287872, "top_k": [{"token": "des", "logprob": -0.19302435219287872}, {"token": "les", "logprob": -1.7398993968963623}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer"}
{"step": 420, "token": "Ġphrases", "logprob": 0.0, "top_k": [{"token": "phrases", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des"}
{"step": 421, "token": ".", "logprob": -0.12325379252433777, "top_k": [{"token": ".", "logprob": -0.12325379252433777}, {"token": "Ċ", "logprob": -2.15450382232666}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases"}
{"step": 422, "token": "ĠĠĊ", "logprob": 0.0, "top_k": [{"token": "Ċ", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases."}
{"step": 423, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n"}
{"step": 424, "token": "Ġ##", "logprob": -4.163677215576172, "top_k": [{"token": "prompt", "logprob": -1.6324273347854614}, {"token": "seed", "logprob": -2.280864715576172}, {"token": "do", "logprob": -2.437114715576172}, {"token": "tokenizer", "logprob": -2.585552215576172}, {"token": "num", "logprob": -2.648052215576172}, {"token": "text", "logprob": -2.812114715576172}, {"token": "output", "logprob": -2.827739715576172}, {"token": "device", "logprob": -2.960552215576172}, {"token": "max", "logprob": -2.999614715576172}, {"token": "use", "logprob": -3.280864715576172}, {"token": "model", "logprob": -3.335552215576172}, {"token": "#", "logprob": -3.554302215576172}, {"token": "context", "logprob": -3.898052215576172}, {"token": "return", "logprob": -3.913677215576172}, {"token": "top", "logprob": -4.007427215576172}, {"token": "stop", "logprob": -4.163677215576172}, {"token": "##", "logprob": -4.163677215576172}, {"token": "vocab", "logprob": -4.444927215576172}, {"token": "pad", "logprob": -4.460552215576172}, {"token": "**", "logprob": -4.476177215576172}, {"token": "```", "logprob": -4.476177215576172}, {"token": "generation", "logprob": -4.546489715576172}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n  "}
{"step": 425, "token": "ĠLa", "logprob": -3.3708136081695557, "top_k": [{"token": "On", "logprob": -2.0661261081695557}, {"token": "Si", "logprob": -2.3630011081695557}, {"token": "I", "logprob": -2.4176886081695557}, {"token": "Pour", "logprob": -2.4411261081695557}, {"token": "Les", "logprob": -2.7692511081695557}, {"token": "Le", "logprob": -2.7848761081695557}, {"token": "Vous", "logprob": -3.2614386081695557}, {"token": "En", "logprob": -3.3395636081695557}, {"token": "La", "logprob": -3.3708136081695557}, {"token": "L", "logprob": -3.6755011081695557}, {"token": "Dans", "logprob": -3.7380011081695557}, {"token": "Ce", "logprob": -3.8161261081695557}, {"token": "Attention", "logprob": -3.8395636081695557}, {"token": "Note", "logprob": -3.9333136081695557}, {"token": "Vo", "logprob": -3.9801886081695557}, {"token": "Il", "logprob": -4.050500869750977}, {"token": "Ċ", "logprob": -4.113000869750977}, {"token": "C", "logprob": -4.120813369750977}, {"token": "V", "logprob": -4.159875869750977}, {"token": "", "logprob": -4.206750869750977}, {"token": "A", "logprob": -4.277063369750977}, {"token": "D", "logprob": -4.347375869750977}, {"token": "Fin", "logprob": -4.363000869750977}, {"token": "Rem", "logprob": -4.425500869750977}, {"token": "G", "logprob": -4.488000869750977}, {"token": "Aj", "logprob": -4.542688369750977}, {"token": "Cette", "logprob": -4.542688369750977}, {"token": "Nous", "logprob": -4.550500869750977}, {"token": "ÃĢ", "logprob": -4.573938369750977}, {"token": "Ex", "logprob": -4.628625869750977}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ##"}
{"step": 426, "token": "Ġliste", "logprob": -2.7628300189971924, "top_k": [{"token": "fonction", "logprob": -0.7940800189971924}, {"token": "gÃ©nÃ©", "logprob": -2.2237675189971924}, {"token": "liste", "logprob": -2.7628300189971924}, {"token": "commande", "logprob": -3.0362675189971924}, {"token": "version", "logprob": -3.2081425189971924}, {"token": "variable", "logprob": -3.6612675189971924}, {"token": "bibli", "logprob": -3.7628300189971924}, {"token": "documentation", "logprob": -3.8253300189971924}, {"token": "valeur", "logprob": -3.9659550189971924}, {"token": "langue", "logprob": -4.090954780578613}, {"token": "configuration", "logprob": -4.176892280578613}, {"token": "crÃ©ation", "logprob": -4.192517280578613}, {"token": "ligne", "logprob": -4.215954780578613}, {"token": "mÃ©thode", "logprob": -4.262829780578613}, {"token": "phrase", "logprob": -4.301892280578613}, {"token": "suite", "logprob": -4.395642280578613}, {"token": "classe", "logprob": -4.403454780578613}, {"token": "fin", "logprob": -4.411267280578613}, {"token": "pl", "logprob": -4.434704780578613}, {"token": "premiÃ¨re", "logprob": -4.458142280578613}, {"token": "long", "logprob": -4.512829780578613}, {"token": "base", "logprob": -4.536267280578613}, {"token": "partie", "logprob": -4.637829780578613}, {"token": "lib", "logprob": -4.708142280578613}, {"token": "taille", "logprob": -4.770642280578613}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La"}
{"step": 427, "token": "Ġde", "logprob": -1.6781506538391113, "top_k": [{"token": "des", "logprob": -0.9125255942344666}, {"token": "suiv", "logprob": -1.6390881538391113}, {"token": "de", "logprob": -1.6781506538391113}, {"token": "ci", "logprob": -2.1078381538391113}, {"token": "d", "logprob": -2.3422131538391113}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste"}
{"step": 428, "token": "Ġfich", "logprob": -3.488178253173828, "top_k": [{"token": "mod", "logprob": -1.9256781339645386}, {"token": "vos", "logprob": -2.042865753173828}, {"token": "param", "logprob": -2.050678253173828}, {"token": "modÃ¨le", "logprob": -2.410053253173828}, {"token": "ces", "logprob": -2.964740753173828}, {"token": "gÃ©nÃ©", "logprob": -3.120990753173828}, {"token": "lang", "logprob": -3.191303253173828}, {"token": "text", "logprob": -3.199115753173828}, {"token": "mots", "logprob": -3.331928253173828}, {"token": "tous", "logprob": -3.347553253173828}, {"token": "fich", "logprob": -3.488178253173828}, {"token": "la", "logprob": -3.769428253173828}, {"token": "arguments", "logprob": -3.855365753173828}, {"token": "donnÃ©es", "logprob": -3.902240753173828}, {"token": "prÃ©", "logprob": -4.058490753173828}, {"token": "caract", "logprob": -4.253803253173828}, {"token": "variables", "logprob": -4.253803253173828}, {"token": "fon", "logprob": -4.316303253173828}, {"token": "toutes", "logprob": -4.363178253173828}, {"token": "fonction", "logprob": -4.394428253173828}, {"token": "l", "logprob": -4.394428253173828}, {"token": "valeurs", "logprob": -4.480365753173828}, {"token": "phrases", "logprob": -4.511615753173828}, {"token": "nos", "logprob": -4.636615753173828}, {"token": "command", "logprob": -4.667865753173828}, {"token": "codes", "logprob": -4.753803253173828}, {"token": "modules", "logprob": -4.785053253173828}, {"token": "ce", "logprob": -4.855365753173828}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de"}
{"step": 429, "token": "iers", "logprob": 0.0, "top_k": [{"token": "iers", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fich"}
{"step": 430, "token": "Ġest", "logprob": -2.627448320388794, "top_k": [{"token": "de", "logprob": -1.283698320388794}, {"token": "cont", "logprob": -1.760260820388794}, {"token": "d", "logprob": -2.510260820388794}, {"token": "est", "logprob": -2.627448320388794}, {"token": "Ãł", "logprob": -2.885260820388794}, {"token": "texte", "logprob": -3.307135820388794}, {"token": "peut", "logprob": -3.518073320388794}, {"token": "ci", "logprob": -3.580573320388794}, {"token": "suiv", "logprob": -3.760260820388794}, {"token": "doit", "logprob": -3.783698320388794}, {"token": "text", "logprob": -3.846198320388794}, {"token": "pour", "logprob": -3.900885820388794}, {"token": "des", "logprob": -3.955573320388794}, {"token": "correspond", "logprob": -4.221198081970215}, {"token": ".", "logprob": -4.314948081970215}, {"token": "que", "logprob": -4.354010581970215}, {"token": "(", "logprob": -4.361823081970215}, {"token": "dans", "logprob": -4.361823081970215}, {"token": "\"", "logprob": -4.596198081970215}, {"token": "se", "logprob": -4.643073081970215}, {"token": "qui", "logprob": -4.697760581970215}, {"token": "et", "logprob": -4.752448081970215}, {"token": "n", "logprob": -4.830573081970215}, {"token": "CSV", "logprob": -4.846198081970215}, {"token": "par", "logprob": -4.932135581970215}, {"token": "avec", "logprob": -4.939948081970215}, {"token": "ne", "logprob": -5.010260581970215}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers"}
{"step": 431, "token": "ĠÃł", "logprob": -3.6314423084259033, "top_k": [{"token": "vide", "logprob": -2.3189423084259033}, {"token": "la", "logprob": -2.5767548084259033}, {"token": "un", "logprob": -2.5923798084259033}, {"token": "une", "logprob": -2.6158173084259033}, {"token": "ici", "logprob": -2.8658173084259033}, {"token": "four", "logprob": -2.8814423084259033}, {"token": "gÃ©n", "logprob": -2.9439423084259033}, {"token": "disponible", "logprob": -3.1158173084259033}, {"token": "dÃ©", "logprob": -3.1783173084259033}, {"token": "utilis", "logprob": -3.2173798084259033}, {"token": "le", "logprob": -3.3267548084259033}, {"token": "l", "logprob": -3.4517548084259033}, {"token": "Ãł", "logprob": -3.6314423084259033}, {"token": "Ã©galement", "logprob": -3.6392548084259033}, {"token": "crÃ©", "logprob": -3.7486298084259033}, {"token": "oblig", "logprob": -3.7720673084259033}, {"token": "pass", "logprob": -3.8345673084259033}, {"token": "dans", "logprob": -3.8501923084259033}, {"token": "en", "logprob": -3.9361298084259033}, {"token": "celle", "logprob": -4.006442070007324}, {"token": "nÃ©cessaire", "logprob": -4.178317070007324}, {"token": "ren", "logprob": -4.272067070007324}, {"token": "spÃ©c", "logprob": -4.584567070007324}, {"token": "rempl", "logprob": -4.631442070007324}, {"token": "stock", "logprob": -4.693942070007324}, {"token": "compos", "logprob": -4.717379570007324}, {"token": "Ã©", "logprob": -4.725192070007324}, {"token": "sous", "logprob": -4.787692070007324}, {"token": "rÃ©cup", "logprob": -4.811129570007324}, {"token": "compl", "logprob": -4.818942070007324}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est"}
{"step": 432, "token": "Ġren", "logprob": -3.0587148666381836, "top_k": [{"token": "compl", "logprob": -1.7149648666381836}, {"token": "crÃ©er", "logprob": -2.3087148666381836}, {"token": "aj", "logprob": -2.3321523666381836}, {"token": "trouver", "logprob": -2.5274648666381836}, {"token": "mettre", "logprob": -2.8712148666381836}, {"token": "gÃ©n", "logprob": -3.0196523666381836}, {"token": "ren", "logprob": -3.0587148666381836}, {"token": "rempl", "logprob": -3.2696523666381836}, {"token": "la", "logprob": -3.3477773666381836}, {"token": "ins", "logprob": -3.3634023666381836}, {"token": "fourn", "logprob": -3.3790273666381836}, {"token": "rÃ©cup", "logprob": -3.4102773666381836}, {"token": "modifier", "logprob": -3.4727773666381836}, {"token": "tÃ©lÃ©", "logprob": -3.5587148666381836}, {"token": "partir", "logprob": -3.8009023666381836}, {"token": "dÃ©", "logprob": -3.9102773666381836}, {"token": "l", "logprob": -3.9884023666381836}, {"token": "utiliser", "logprob": -4.160277366638184}, {"token": "incl", "logprob": -4.379027366638184}, {"token": "placer", "logprob": -4.379027366638184}, {"token": "prÃ©c", "logprob": -4.621214866638184}, {"token": "votre", "logprob": -4.621214866638184}, {"token": "vous", "logprob": -4.668089866638184}, {"token": "four", "logprob": -4.675902366638184}, {"token": "prendre", "logprob": -4.691527366638184}, {"token": "cop", "logprob": -4.863402366638184}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à"}
{"step": 433, "token": "se", "logprob": 0.0, "top_k": [{"token": "se", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à ren"}
{"step": 434, "token": "igner", "logprob": 0.0, "top_k": [{"token": "igner", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à rense"}
{"step": 435, "token": "Ġpar", "logprob": -2.9030840396881104, "top_k": [{"token": "dans", "logprob": -1.4343340396881104}, {"token": "ici", "logprob": -2.1296465396881104}, {"token": "en", "logprob": -2.2702715396881104}, {"token": "Ãł", "logprob": -2.6062090396881104}, {"token": "Ċ", "logprob": -2.8562090396881104}, {"token": "man", "logprob": -2.8640215396881104}, {"token": "par", "logprob": -2.9030840396881104}, {"token": ".", "logprob": -2.9499590396881104}, {"token": "pour", "logprob": -3.0358965396881104}, {"token": "avec", "logprob": -3.5358965396881104}, {"token": ",", "logprob": -3.6999590396881104}, {"token": "au", "logprob": -3.8483965396881104}, {"token": "comme", "logprob": -3.9108965396881104}, {"token": "sous", "logprob": -3.9343340396881104}, {"token": "ci", "logprob": -4.137458801269531}, {"token": "(", "logprob": -4.176521301269531}, {"token": "selon", "logprob": -4.231208801269531}, {"token": "Âł", "logprob": -4.317146301269531}, {"token": "si", "logprob": -4.410896301269531}, {"token": "et", "logprob": -4.614021301269531}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner"}
{"step": 436, "token": "Ġle", "logprob": -1.199841022491455, "top_k": [{"token": "l", "logprob": -1.121716022491455}, {"token": "le", "logprob": -1.199841022491455}, {"token": "vous", "logprob": -1.824841022491455}, {"token": "la", "logprob": -2.770153522491455}, {"token": "les", "logprob": -3.199841022491455}, {"token": "so", "logprob": -3.731091022491455}, {"token": "elle", "logprob": -3.863903522491455}, {"token": "un", "logprob": -3.910778522491455}, {"token": "votre", "logprob": -4.215466022491455}, {"token": "exemple", "logprob": -4.223278522491455}, {"token": "rapport", "logprob": -4.270153522491455}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par"}
{"step": 437, "token": "ĠdÃ©velop", "logprob": -2.122382402420044, "top_k": [{"token": "client", "logprob": -1.911444902420044}, {"token": "dÃ©velop", "logprob": -2.122382402420044}, {"token": "user", "logprob": -2.138007402420044}, {"token": "b", "logprob": -2.223944902420044}, {"token": "script", "logprob": -2.317694902420044}, {"token": "programme", "logprob": -2.388007402420044}, {"token": "propri", "logprob": -3.302069902420044}, {"token": "utilisateur", "logprob": -3.513007402420044}, {"token": "fichier", "logprob": -3.591132402420044}, {"token": "systÃ¨me", "logprob": -4.005194664001465}, {"token": "gestion", "logprob": -4.169257164001465}, {"token": "prof", "logprob": -4.184882164001465}, {"token": "bot", "logprob": -4.231757164001465}, {"token": "bon", "logprob": -4.341132164001465}, {"token": "param", "logprob": -4.356757164001465}, {"token": "cherche", "logprob": -4.372382164001465}, {"token": "serve", "logprob": -4.419257164001465}, {"token": "fait", "logprob": -4.427069664001465}, {"token": "format", "logprob": -4.442694664001465}, {"token": "code", "logprob": -4.591132164001465}, {"token": "module", "logprob": -4.598944664001465}, {"token": "visite", "logprob": -4.622382164001465}, {"token": "joueur", "logprob": -4.755194664001465}, {"token": "navigate", "logprob": -4.763007164001465}, {"token": "cas", "logprob": -4.825507164001465}, {"token": "form", "logprob": -4.856757164001465}, {"token": "modÃ¨le", "logprob": -4.927069664001465}, {"token": "service", "logprob": -4.950507164001465}, {"token": "responsable", "logprob": -4.981757164001465}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le"}
{"step": 438, "token": "pe", "logprob": 0.0, "top_k": [{"token": "pe", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le dévelop"}
{"step": 439, "token": "ur", "logprob": 0.0, "top_k": [{"token": "ur", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développe"}
{"step": 440, "token": "Âł", "logprob": -4.001567363739014, "top_k": [{"token": ".", "logprob": -1.0328174829483032}, {"token": "Ċ", "logprob": -1.5328174829483032}, {"token": "de", "logprob": -2.8140673637390137}, {"token": ",", "logprob": -2.9781298637390137}, {"token": "(", "logprob": -3.0328173637390137}, {"token": "du", "logprob": -3.1500048637390137}, {"token": "pour", "logprob": -3.4000048637390137}, {"token": "et", "logprob": -3.4937548637390137}, {"token": "Ċ", "logprob": -3.6578173637390137}, {"token": "en", "logprob": -3.6890673637390137}, {"token": "dans", "logprob": -3.9156298637390137}, {"token": ".Ċ", "logprob": -3.9859423637390137}, {"token": "Âł", "logprob": -4.001567363739014}, {"token": "Ċ", "logprob": -4.079692363739014}, {"token": "Ãł", "logprob": -4.282817363739014}, {"token": "afin", "logprob": -4.696879863739014}, {"token": "ou", "logprob": -4.860942363739014}, {"token": "Ċ", "logprob": -4.923442363739014}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur"}
{"step": 441, "token": ":", "logprob": -0.2997216582298279, "top_k": [{"token": ":", "logprob": -0.2997216582298279}, {"token": "!", "logprob": -1.8153467178344727}, {"token": "Ċ", "logprob": -3.1903467178344727}, {"token": "!Ċ", "logprob": -3.5106592178344727}, {"token": ":Ċ", "logprob": -3.6825342178344727}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur "}
{"step": 442, "token": "ĠĠĊ", "logprob": 0.0, "top_k": [{"token": "Ċ", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :"}
{"step": 443, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n"}
{"step": 444, "token": "Ġ##", "logprob": -1.5012630224227905, "top_k": [{"token": "##", "logprob": -1.5012630224227905}, {"token": "input", "logprob": -1.6106380224227905}, {"token": "files", "logprob": -2.18095064163208}, {"token": "#", "logprob": -2.18095064163208}, {"token": "text", "logprob": -2.41532564163208}, {"token": "file", "logprob": -2.68095064163208}, {"token": "prompt", "logprob": -3.44657564163208}, {"token": "data", "logprob": -3.46220064163208}, {"token": "filenames", "logprob": -3.66532564163208}, {"token": "path", "logprob": -3.79032564163208}, {"token": "filename", "logprob": -3.86845064163208}, {"token": "dataset", "logprob": -4.15751314163208}, {"token": "texts", "logprob": -4.28251314163208}, {"token": "train", "logprob": -4.64188814163208}, {"token": "with", "logprob": -4.85282564163208}, {"token": "model", "logprob": -4.91532564163208}, {"token": "inputs", "logprob": -4.93876314163208}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n  "}
{"step": 445, "token": "ĠI", "logprob": -3.568464994430542, "top_k": [{"token": "Les", "logprob": -2.498152494430542}, {"token": "text", "logprob": -2.779402494430542}, {"token": "Le", "logprob": -2.896589994430542}, {"token": "files", "logprob": -2.943464994430542}, {"token": "La", "logprob": -2.998152494430542}, {"token": "list", "logprob": -3.216902494430542}, {"token": "-", "logprob": -3.240339994430542}, {"token": "file", "logprob": -3.365339994430542}, {"token": "path", "logprob": -3.373152494430542}, {"token": "On", "logprob": -3.451277494430542}, {"token": "", "logprob": -3.459089994430542}, {"token": "Il", "logprob": -3.513777494430542}, {"token": "input", "logprob": -3.537214994430542}, {"token": "", "logprob": -3.560652494430542}, {"token": "I", "logprob": -3.568464994430542}, {"token": "\"", "logprob": -3.599714994430542}, {"token": "data", "logprob": -3.654402494430542}, {"token": "L", "logprob": -3.795027494430542}, {"token": "fichier", "logprob": -3.896589994430542}, {"token": "Pour", "logprob": -3.935652494430542}, {"token": "model", "logprob": -4.005965232849121}, {"token": "Ex", "logprob": -4.084090232849121}, {"token": "on", "logprob": -4.107527732849121}, {"token": "[\"", "logprob": -4.123152732849121}, {"token": "filename", "logprob": -4.146590232849121}, {"token": "fich", "logprob": -4.146590232849121}, {"token": "https", "logprob": -4.162215232849121}, {"token": "F", "logprob": -4.177840232849121}, {"token": "les", "logprob": -4.232527732849121}, {"token": "(", "logprob": -4.263777732849121}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ##"}
{"step": 446, "token": "ci", "logprob": 0.0, "top_k": [{"token": "ci", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## I"}
{"step": 447, "token": ",", "logprob": 0.0, "top_k": [{"token": ",", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici"}
{"step": 448, "token": "Ġon", "logprob": 0.0, "top_k": [{"token": "on", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici,"}
{"step": 449, "token": "Ġutilise", "logprob": 0.0, "top_k": [{"token": "utilise", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on"}
{"step": 450, "token": "Ġles", "logprob": -1.529676914215088, "top_k": [{"token": "la", "logprob": -1.092176914215088}, {"token": "les", "logprob": -1.529676914215088}, {"token": "une", "logprob": -1.889051914215088}, {"token": "le", "logprob": -2.045301914215088}, {"token": "un", "logprob": -2.076551914215088}, {"token": "des", "logprob": -3.170301914215088}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise"}
{"step": 451, "token": "Ġfich", "logprob": -0.24162735044956207, "top_k": [{"token": "fich", "logprob": -0.24162735044956207}, {"token": "donnÃ©es", "logprob": -2.5697524547576904}, {"token": "list", "logprob": -3.0463149547576904}, {"token": "text", "logprob": -3.5931899547576904}, {"token": "", "logprob": -3.7650649547576904}, {"token": "phrases", "logprob": -4.194752216339111}, {"token": "arguments", "logprob": -4.358814716339111}, {"token": "n", "logprob": -4.421314716339111}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les"}
{"step": 452, "token": "iers", "logprob": 0.0, "top_k": [{"token": "iers", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fich"}
{"step": 453, "token": "Ġqui", "logprob": -4.375067234039307, "top_k": [{"token": "de", "logprob": -0.8594421148300171}, {"token": "texte", "logprob": -2.4531922340393066}, {"token": "pour", "logprob": -2.6485047340393066}, {"token": "d", "logprob": -2.6797547340393066}, {"token": "\"", "logprob": -2.8438172340393066}, {"token": "text", "logprob": -2.8828797340393066}, {"token": "cont", "logprob": -3.4375672340393066}, {"token": "Ãł", "logprob": -3.4375672340393066}, {"token": ".", "logprob": -3.4688172340393066}, {"token": "(", "logprob": -3.7266297340393066}, {"token": "suiv", "logprob": -3.8594422340393066}, {"token": "txt", "logprob": -3.8672547340393066}, {"token": "du", "logprob": -4.195379734039307}, {"token": "qui", "logprob": -4.375067234039307}, {"token": "dans", "logprob": -4.507879734039307}, {"token": "des", "logprob": -4.523504734039307}, {"token": "par", "logprob": -4.656317234039307}, {"token": "en", "logprob": -4.742254734039307}, {"token": "que", "logprob": -4.851629734039307}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers"}
{"step": 454, "token": "Ġcont", "logprob": -1.309049129486084, "top_k": [{"token": "cont", "logprob": -1.309049129486084}, {"token": "ont", "logprob": -2.207486629486084}, {"token": "seront", "logprob": -2.277799129486084}, {"token": "sont", "logprob": -2.316861629486084}, {"token": "vont", "logprob": -2.535611629486084}, {"token": "doivent", "logprob": -2.793424129486084}, {"token": "serv", "logprob": -3.051236629486084}, {"token": "nous", "logprob": -3.262174129486084}, {"token": "correspondent", "logprob": -3.262174129486084}, {"token": "compos", "logprob": -3.309049129486084}, {"token": "se", "logprob": -3.309049129486084}, {"token": "font", "logprob": -4.215299129486084}, {"token": "permet", "logprob": -4.340299129486084}, {"token": "dÃ©", "logprob": -4.410611629486084}, {"token": "suiv", "logprob": -4.418424129486084}, {"token": "dev", "logprob": -4.465299129486084}, {"token": "constituent", "logprob": -4.707486629486084}, {"token": "ren", "logprob": -4.723111629486084}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui"}
{"step": 455, "token": "i", "logprob": 0.0, "top_k": [{"token": "i", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui cont"}
{"step": 456, "token": "ennent", "logprob": 0.0, "top_k": [{"token": "ennent", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui conti"}
{"step": 457, "token": "Ġles", "logprob": -0.646584153175354, "top_k": [{"token": "les", "logprob": -0.646584153175354}, {"token": "des", "logprob": -1.365334153175354}, {"token": "le", "logprob": -2.0528340339660645}, {"token": "la", "logprob": -2.5997090339660645}, {"token": "l", "logprob": -4.0059590339660645}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent"}
{"step": 458, "token": "Ġphrases", "logprob": -0.988258957862854, "top_k": [{"token": "phrases", "logprob": -0.988258957862854}, {"token": "donnÃ©es", "logprob": -1.597633957862854}, {"token": "text", "logprob": -1.957008957862854}, {"token": "mots", "logprob": -2.8710713386535645}, {"token": "questions", "logprob": -3.3163838386535645}, {"token": "exemp", "logprob": -3.7070088386535645}, {"token": "Ã©", "logprob": -3.8554463386535645}, {"token": "entr", "logprob": -4.0038838386535645}, {"token": "list", "logprob": -4.0976338386535645}, {"token": "informations", "logprob": -4.1523213386535645}, {"token": "n", "logprob": -4.1601338386535645}, {"token": "diffÃ©rentes", "logprob": -4.4023213386535645}, {"token": "rÃ©", "logprob": -4.4335713386535645}, {"token": "paragraph", "logprob": -4.4413838386535645}, {"token": "fich", "logprob": -4.6132588386535645}, {"token": "ent", "logprob": -4.6445088386535645}, {"token": "tit", "logprob": -4.6913838386535645}, {"token": "expressions", "logprob": -4.9101338386535645}, {"token": "par", "logprob": -4.9179463386535645}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les"}
{"step": 459, "token": "Ġpour", "logprob": -1.7630231380462646, "top_k": [{"token": "Ãł", "logprob": -0.6458356380462646}, {"token": "pour", "logprob": -1.7630231380462646}, {"token": "(", "logprob": -2.4036481380462646}, {"token": "que", "logprob": -2.6536481380462646}, {"token": "de", "logprob": -2.8333356380462646}, {"token": ".", "logprob": -3.2864606380462646}, {"token": "et", "logprob": -3.8255231380462646}, {"token": ",", "logprob": -4.325523376464844}, {"token": "qui", "logprob": -4.403648376464844}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases"}
{"step": 460, "token": "Ġla", "logprob": -1.9307364225387573, "top_k": [{"token": "crÃ©er", "logprob": -0.9463614225387573}, {"token": "les", "logprob": -1.1494864225387573}, {"token": "la", "logprob": -1.9307364225387573}, {"token": "gÃ©n", "logprob": -2.305736541748047}, {"token": "Ãªtre", "logprob": -3.618236541748047}, {"token": "le", "logprob": -3.751049041748047}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour"}
{"step": 461, "token": "ĠgÃ©nÃ©", "logprob": 0.0, "top_k": [{"token": "gÃ©nÃ©", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la"}
{"step": 462, "token": "ration", "logprob": 0.0, "top_k": [{"token": "ration", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la géné"}
{"step": 463, "token": "Ġde", "logprob": -0.7690832018852234, "top_k": [{"token": "de", "logprob": -0.7690832018852234}, {"token": "des", "logprob": -1.3315832614898682}, {"token": "du", "logprob": -2.003458261489868}, {"token": "(", "logprob": -2.565958261489868}, {"token": ".", "logprob": -2.800333261489868}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération"}
{"step": 464, "token": "Ġtext", "logprob": -0.1659410297870636, "top_k": [{"token": "text", "logprob": -0.1659410297870636}, {"token": "phrases", "logprob": -2.525315999984741}, {"token": "texte", "logprob": -2.619065999984741}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de"}
{"step": 465, "token": "es", "logprob": 0.0, "top_k": [{"token": "es", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de text"}
{"step": 466, "token": "Ġ(", "logprob": -1.094851016998291, "top_k": [{"token": ".", "logprob": -0.4073510468006134}, {"token": "(", "logprob": -1.094851016998291}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes"}
{"step": 467, "token": "c", "logprob": -4.260112285614014, "top_k": [{"token": "par", "logprob": -1.8460499048233032}, {"token": "en", "logprob": -2.5335497856140137}, {"token": "les", "logprob": -2.9397997856140137}, {"token": "voir", "logprob": -2.9710497856140137}, {"token": "5", "logprob": -2.9710497856140137}, {"token": "text", "logprob": -2.9866747856140137}, {"token": "ex", "logprob": -3.0335497856140137}, {"token": "liste", "logprob": -3.1507372856140137}, {"token": "le", "logprob": -3.2601122856140137}, {"token": "d", "logprob": -3.5413622856140137}, {"token": "list", "logprob": -3.6194872856140137}, {"token": "Ãł", "logprob": -3.7054247856140137}, {"token": "exemple", "logprob": -3.7132372856140137}, {"token": "t", "logprob": -3.7601122856140137}, {"token": "cont", "logprob": -3.7835497856140137}, {"token": "on", "logprob": -3.8304247856140137}, {"token": "1", "logprob": -3.8616747856140137}, {"token": "n", "logprob": -3.9788622856140137}, {"token": "l", "logprob": -4.025737285614014}, {"token": "un", "logprob": -4.088237285614014}, {"token": "3", "logprob": -4.127299785614014}, {"token": "10", "logprob": -4.174174785614014}, {"token": "la", "logprob": -4.252299785614014}, {"token": "c", "logprob": -4.260112285614014}, {"token": "50", "logprob": -4.260112285614014}, {"token": "g", "logprob": -4.267924785614014}, {"token": "4", "logprob": -4.346049785614014}, {"token": "s", "logprob": -4.361674785614014}, {"token": "test", "logprob": -4.400737285614014}, {"token": "pour", "logprob": -4.408549785614014}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes ("}
{"step": 468, "token": "ette", "logprob": -0.7946819067001343, "top_k": [{"token": "ette", "logprob": -0.7946819067001343}, {"token": "'est", "logprob": -0.8103069067001343}, {"token": "ertain", "logprob": -3.318119525909424}, {"token": "et", "logprob": -3.364994525909424}, {"token": "inq", "logprob": -3.880619525909424}, {"token": "ible", "logprob": -4.411869525909424}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (c"}
{"step": 469, "token": "Ġliste", "logprob": -0.33475396037101746, "top_k": [{"token": "liste", "logprob": -0.33475396037101746}, {"token": "fonction", "logprob": -1.8894414901733398}, {"token": "variable", "logprob": -3.04569149017334}, {"token": "option", "logprob": -3.43631649017334}, {"token": "partie", "logprob": -3.62381649017334}, {"token": "op", "logprob": -4.09256649017334}, {"token": "Ã©t", "logprob": -4.58475399017334}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette"}
{"step": 470, "token": "Ġsera", "logprob": -2.8855156898498535, "top_k": [{"token": "est", "logprob": -0.7527031302452087}, {"token": "de", "logprob": -2.0027031898498535}, {"token": "peut", "logprob": -2.3620781898498535}, {"token": "doit", "logprob": -2.4558281898498535}, {"token": "n", "logprob": -2.6120781898498535}, {"token": "sera", "logprob": -2.8855156898498535}, {"token": "ne", "logprob": -3.4558281898498535}, {"token": "cont", "logprob": -3.5105156898498535}, {"token": "a", "logprob": -3.7605156898498535}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste"}
{"step": 471, "token": "ĠÃł", "logprob": -2.2584235668182373, "top_k": [{"token": "ren", "logprob": -1.1724860668182373}, {"token": "four", "logprob": -2.1802985668182373}, {"token": "Ãł", "logprob": -2.2584235668182373}, {"token": "mod", "logprob": -2.4459235668182373}, {"token": "gÃ©n", "logprob": -2.7193610668182373}, {"token": "compl", "logprob": -2.9771735668182373}, {"token": "rempl", "logprob": -3.1568610668182373}, {"token": "ajout", "logprob": -3.1959235668182373}, {"token": "crÃ©", "logprob": -3.3834235668182373}, {"token": "dÃ©", "logprob": -3.5787360668182373}, {"token": "automat", "logprob": -3.8521735668182373}, {"token": "mise", "logprob": -4.289673805236816}, {"token": "rem", "logprob": -4.383423805236816}, {"token": "donc", "logprob": -4.524048805236816}, {"token": "utilis", "logprob": -4.563111305236816}, {"token": "rÃ©", "logprob": -4.680298805236816}, {"token": "raj", "logprob": -4.688111305236816}, {"token": "donn", "logprob": -4.820923805236816}, {"token": "Ã©", "logprob": -4.883423805236816}, {"token": "g", "logprob": -4.899048805236816}, {"token": "adapt", "logprob": -4.977173805236816}, {"token": "dÃ©velopp", "logprob": -4.977173805236816}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera"}
{"step": 472, "token": "Ġren", "logprob": -0.14462119340896606, "top_k": [{"token": "ren", "logprob": -0.14462119340896606}, {"token": "compl", "logprob": -2.8477461338043213}, {"token": "rempl", "logprob": -3.7461836338043213}, {"token": "fourn", "logprob": -3.8555586338043213}, {"token": "modifier", "logprob": -4.0039963722229}, {"token": "crÃ©er", "logprob": -4.2930588722229}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à"}
{"step": 473, "token": "se", "logprob": 0.0, "top_k": [{"token": "se", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à ren"}
{"step": 474, "token": "igner", "logprob": 0.0, "top_k": [{"token": "igner", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à rense"}
{"step": 475, "token": "Ġpar", "logprob": 0.0, "top_k": [{"token": "par", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner"}
{"step": 476, "token": "Ġle", "logprob": 0.0, "top_k": [{"token": "le", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par"}
{"step": 477, "token": "ĠdÃ©velop", "logprob": 0.0, "top_k": [{"token": "dÃ©velop", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le"}
{"step": 478, "token": "pe", "logprob": 0.0, "top_k": [{"token": "pe", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le dévelop"}
{"step": 479, "token": "ur", "logprob": 0.0, "top_k": [{"token": "ur", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développe"}
{"step": 480, "token": ").", "logprob": -0.44529402256011963, "top_k": [{"token": ").", "logprob": -0.44529402256011963}, {"token": ")", "logprob": -1.0234190225601196}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur"}
{"step": 481, "token": "ĠĠĊ", "logprob": 0.0, "top_k": [{"token": "Ċ", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur)."}
{"step": 482, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n"}
{"step": 483, "token": "Ġtext", "logprob": -2.1076698303222656, "top_k": [{"token": "##", "logprob": -1.2639199495315552}, {"token": "input", "logprob": -1.8498574495315552}, {"token": "text", "logprob": -2.1076698303222656}, {"token": "files", "logprob": -2.4904823303222656}, {"token": "#", "logprob": -2.5842323303222656}, {"token": "file", "logprob": -2.9436073303222656}, {"token": "filenames", "logprob": -3.4045448303222656}, {"token": "data", "logprob": -3.5686073303222656}, {"token": "filename", "logprob": -3.7951698303222656}, {"token": "with", "logprob": -3.9123573303222656}, {"token": "path", "logprob": -3.9201698303222656}, {"token": "prompt", "logprob": -3.9279823303222656}, {"token": "texts", "logprob": -4.138919830322266}, {"token": "```", "logprob": -4.232669830322266}, {"token": "dataset", "logprob": -4.263919830322266}, {"token": "train", "logprob": -4.373294830322266}, {"token": "seed", "logprob": -4.529544830322266}, {"token": "num", "logprob": -4.787357330322266}, {"token": "model", "logprob": -4.826419830322266}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n  "}
{"step": 484, "token": "_list", "logprob": -3.2715229988098145, "top_k": [{"token": "_files", "logprob": -1.2793354988098145}, {"token": "_file", "logprob": -2.2793354988098145}, {"token": "_generate", "logprob": -2.3496479988098145}, {"token": "_input", "logprob": -2.4902729988098145}, {"token": "_path", "logprob": -2.6543354988098145}, {"token": "_data", "logprob": -3.1621479988098145}, {"token": "_filenames", "logprob": -3.2246479988098145}, {"token": "_list", "logprob": -3.2715229988098145}, {"token": "_generation", "logprob": -3.4824604988098145}, {"token": "_gen", "logprob": -3.5527729988098145}, {"token": "_dataset", "logprob": -3.6308979988098145}, {"token": "_paths", "logprob": -3.7168354988098145}, {"token": "=", "logprob": -3.8652729988098145}, {"token": "_dir", "logprob": -3.8730854988098145}, {"token": "_inputs", "logprob": -4.0215229988098145}, {"token": "_generator", "logprob": -4.2012104988098145}, {"token": "_to", "logprob": -4.3808979988098145}, {"token": "_source", "logprob": -4.4980854988098145}, {"token": "_f", "logprob": -4.5137104988098145}, {"token": "_filename", "logprob": -4.5215229988098145}, {"token": "_model", "logprob": -4.5762104988098145}, {"token": "_folder", "logprob": -4.5918354988098145}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text"}
{"step": 485, "token": "Ġ=", "logprob": -0.12274549156427383, "top_k": [{"token": "=", "logprob": -0.12274549156427383}, {"token": "_path", "logprob": -2.7008705139160156}, {"token": "_file", "logprob": -3.0289955139160156}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list"}
{"step": 486, "token": "Ġ[\"", "logprob": -0.4107971787452698, "top_k": [{"token": "[\"", "logprob": -0.4107971787452698}, {"token": "[", "logprob": -1.770172119140625}, {"token": "['", "logprob": -2.457672119140625}, {"token": "[]", "logprob": -3.176422119140625}, {"token": "\"", "logprob": -3.238922119140625}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list ="}
{"step": 487, "token": "\",", "logprob": -3.1564786434173584, "top_k": [{"token": "text", "logprob": -1.6486661434173584}, {"token": "C", "logprob": -2.7267911434173584}, {"token": "Le", "logprob": -2.8439786434173584}, {"token": "\",", "logprob": -3.1564786434173584}, {"token": "I", "logprob": -3.2502286434173584}, {"token": "A", "logprob": -3.4846036434173584}, {"token": "c", "logprob": -3.5392911434173584}, {"token": "Text", "logprob": -3.5471036434173584}, {"token": "file", "logprob": -3.6330411434173584}, {"token": "Il", "logprob": -3.6486661434173584}, {"token": "Je", "logprob": -3.6642911434173584}, {"token": "f", "logprob": -3.6642911434173584}, {"token": "La", "logprob": -3.7033536434173584}, {"token": "la", "logprob": -3.7267911434173584}, {"token": "The", "logprob": -3.7658536434173584}, {"token": "a", "logprob": -3.8283536434173584}, {"token": "Hello", "logprob": -3.8674161434173584}, {"token": "le", "logprob": -3.9299161434173584}, {"token": "\"]", "logprob": -3.9299161434173584}, {"token": "par", "logprob": -3.9611661434173584}, {"token": "This", "logprob": -3.9767911434173584}, {"token": "hello", "logprob": -4.0002288818359375}, {"token": "L", "logprob": -4.0861663818359375}, {"token": "1", "logprob": -4.1330413818359375}, {"token": "test", "logprob": -4.1486663818359375}, {"token": "phrase", "logprob": -4.1486663818359375}, {"token": "Une", "logprob": -4.1642913818359375}, {"token": "Les", "logprob": -4.1721038818359375}, {"token": "J", "logprob": -4.3283538818359375}, {"token": "https", "logprob": -4.3283538818359375}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list = [\""}
{"step": 488, "token": "Ġ\"\",", "logprob": -0.5643995404243469, "top_k": [{"token": "\"\",", "logprob": -0.5643995404243469}, {"token": "\"", "logprob": -1.1893994808197021}, {"token": "\"\"", "logprob": -2.064399480819702}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list = [\"\","}
{"step": 489, "token": "Ġ\"\",", "logprob": -0.3114028573036194, "top_k": [{"token": "\"\",", "logprob": -0.3114028573036194}, {"token": "\"\"", "logprob": -1.9364027976989746}, {"token": "\"", "logprob": -2.0926527976989746}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list = [\"\", \"\","}
{"step": 490, "token": "Ġ\"\",", "logprob": -0.06993623077869415, "top_k": [{"token": "\"\",", "logprob": -0.06993623077869415}, {"token": "\"\"", "logprob": -2.6949362754821777}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list = [\"\", \"\", \"\","}
{"step": 491, "token": "Ġ\"\"", "logprob": -0.8084237575531006, "top_k": [{"token": "\"\"", "logprob": -0.8084237575531006}, {"token": "\"", "logprob": -1.1521737575531006}, {"token": "\"\",", "logprob": -1.4334237575531006}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list = [\"\", \"\", \"\", \"\","}
{"step": 492, "token": "]", "logprob": -0.5836624503135681, "top_k": [{"token": "]", "logprob": -0.5836624503135681}, {"token": "]Ċ", "logprob": -0.9430374503135681}, {"token": "]", "logprob": -2.943037509918213}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list = [\"\", \"\", \"\", \"\", \"\""}
{"step": 493, "token": "ĠĠĊ", "logprob": 0.0, "top_k": [{"token": "Ċ", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list = [\"\", \"\", \"\", \"\", \"\"]"}
{"step": 494, "token": "ĠĠ", "logprob": 0.0, "top_k": [{"token": "", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list = [\"\", \"\", \"\", \"\", \"\"]  \n"}
{"step": 495, "token": "Ġ#", "logprob": -0.7331876754760742, "top_k": [{"token": "#", "logprob": -0.7331876754760742}, {"token": "##", "logprob": -1.1550626754760742}, {"token": "```", "logprob": -2.186312675476074}, {"token": "```Ċ", "logprob": -3.303500175476074}, {"token": "text", "logprob": -3.561312675476074}, {"token": "``", "logprob": -3.608187675476074}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list = [\"\", \"\", \"\", \"\", \"\"]  \n  "}
{"step": 496, "token": "ĠI", "logprob": 0.0, "top_k": [{"token": "I", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list = [\"\", \"\", \"\", \"\", \"\"]  \n   #"}
{"step": 497, "token": "ci", "logprob": 0.0, "top_k": [{"token": "ci", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list = [\"\", \"\", \"\", \"\", \"\"]  \n   # I"}
{"step": 498, "token": ",", "logprob": 0.0, "top_k": [{"token": ",", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list = [\"\", \"\", \"\", \"\", \"\"]  \n   # Ici"}
{"step": 499, "token": "Ġon", "logprob": 0.0, "top_k": [{"token": "on", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list = [\"\", \"\", \"\", \"\", \"\"]  \n   # Ici,"}
{"step": 500, "token": "Ġutilise", "logprob": 0.0, "top_k": [{"token": "utilise", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list = [\"\", \"\", \"\", \"\", \"\"]  \n   # Ici, on"}
{"step": 501, "token": "Ġla", "logprob": -0.4755128026008606, "top_k": [{"token": "la", "logprob": -0.4755128026008606}, {"token": "les", "logprob": -1.6473877429962158}, {"token": "une", "logprob": -2.272387742996216}, {"token": "le", "logprob": -2.491137742996216}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list = [\"\", \"\", \"\", \"\", \"\"]  \n   # Ici, on utilise"}
{"step": 502, "token": "Ġliste", "logprob": 0.0, "top_k": [{"token": "liste", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list = [\"\", \"\", \"\", \"\", \"\"]  \n   # Ici, on utilise la"}
{"step": 503, "token": "Ġde", "logprob": -0.5828105211257935, "top_k": [{"token": "de", "logprob": -0.5828105211257935}, {"token": "des", "logprob": -0.8171855211257935}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list = [\"\", \"\", \"\", \"\", \"\"]  \n   # Ici, on utilise la liste"}
{"step": 504, "token": "Ġphrases", "logprob": -0.7168593406677246, "top_k": [{"token": "fich", "logprob": -0.6699843406677246}, {"token": "phrases", "logprob": -0.7168593406677246}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list = [\"\", \"\", \"\", \"\", \"\"]  \n   # Ici, on utilise la liste de"}
{"step": 505, "token": "ĠÃł", "logprob": -0.9562069177627563, "top_k": [{"token": "pour", "logprob": -0.9093319177627563}, {"token": "Ãł", "logprob": -0.9562069177627563}, {"token": "qui", "logprob": -2.315581798553467}, {"token": "(", "logprob": -2.596831798553467}, {"token": "que", "logprob": -3.917144298553467}, {"token": "cont", "logprob": -3.924956798553467}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list = [\"\", \"\", \"\", \"\", \"\"]  \n   # Ici, on utilise la liste de phrases"}
{"step": 506, "token": "Ġutiliser", "logprob": -0.8171855211257935, "top_k": [{"token": "gÃ©n", "logprob": -0.5828105211257935}, {"token": "utiliser", "logprob": -0.8171855211257935}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list = [\"\", \"\", \"\", \"\", \"\"]  \n   # Ici, on utilise la liste de phrases à"}
{"step": 507, "token": "Ġpour", "logprob": 0.0, "top_k": [{"token": "pour", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list = [\"\", \"\", \"\", \"\", \"\"]  \n   # Ici, on utilise la liste de phrases à utiliser"}
{"step": 508, "token": "Ġla", "logprob": -0.04712718725204468, "top_k": [{"token": "la", "logprob": -0.04712718725204468}, {"token": "gÃ©n", "logprob": -3.0783772468566895}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list = [\"\", \"\", \"\", \"\", \"\"]  \n   # Ici, on utilise la liste de phrases à utiliser pour"}
{"step": 509, "token": "ĠgÃ©nÃ©", "logprob": 0.0, "top_k": [{"token": "gÃ©nÃ©", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list = [\"\", \"\", \"\", \"\", \"\"]  \n   # Ici, on utilise la liste de phrases à utiliser pour la"}
{"step": 510, "token": "ration", "logprob": 0.0, "top_k": [{"token": "ration", "logprob": 0.0}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list = [\"\", \"\", \"\", \"\", \"\"]  \n   # Ici, on utilise la liste de phrases à utiliser pour la géné"}
{"step": 511, "token": "Ġde", "logprob": -0.10322872549295425, "top_k": [{"token": "de", "logprob": -0.10322872549295425}, {"token": "des", "logprob": -2.321978807449341}], "context": " ___\n\n   # J'utilise la fonction generate_text avec les arguments suivants :  \n   ```python  \n   text_model = \"XLNET\"  \n   # Ici, on utilise le modèle de langage XLNET pour créer des phrases.  \n   text_generate_type = \"fill-in-the-blank\"  \n   # Ici, on utilise le type de génération de textes (fill-in-the-blank) pour créer des phrases.  \n   text_generate_length = 500  \n   # Ici, on utilise la longueur des phrases à générer (500 par défaut)  \n   num_samples = 5  \n   # Ici, on utilise le nombre d'échantillons à générer (5 par défaut)  \n   num_beams = 2  \n   # Ici, on utilise le nombre de bâtons à utiliser pour la génération des phrases (2 par défaut)  \n   max_seq_len = 500  \n   # Ici, on utilise la longueur maximale des phrases à générer (500 par défaut)  \n   min_seq_len = 300  \n   # Ici, on utilise la longueur minimale des phrases à générer (300 par défaut)  \n   n_top_n = 20  \n   # Ici, on utilise le nombre de phrases à générer (20 par défaut)  \n   batch_size = 10  \n   # Ici, on utilise le nombre d'échantillons par génération (10 par défaut)  \n   temperature = 0.3  \n   # Ici, on utilise la température (0.3 par défaut)  \n   num_return_sequences = 5  \n   # Ici, on utilise le nombre de phrases à générer pour chaque échantillon (5 par défaut)  \n   model_name_or_path = \"bert-base-uncased\"  \n   # Ici, on utilise le modèle Bert-Base (en utilisant un encodage de caractère non accentué) pour créer des phrases.  \n   ## La liste de fichiers est à renseigner par le développeur :  \n   ## Ici, on utilise les fichiers qui contiennent les phrases pour la génération de textes (cette liste sera à renseigner par le développeur).  \n   text_list = [\"\", \"\", \"\", \"\", \"\"]  \n   # Ici, on utilise la liste de phrases à utiliser pour la génération"}
